{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27923a2e-5509-46f4-9ce5-c20e4e565bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa5e6d4e-b501-4873-b3be-a2db3a28fcb8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import vision_transformer_v1 as vt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c96a5489-4614-4c5c-9a28-73758a77410c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Epoch 0 train loss:  3.650615059811136\n",
      ">>> Epoch 0 test loss:  3.6258246484010117\n",
      ">>> Epoch 1 train loss:  3.6251894417016404\n",
      ">>> Epoch 1 test loss:  3.6329609414805537\n",
      ">>> Epoch 2 train loss:  3.625865850759589\n",
      ">>> Epoch 2 test loss:  3.620877856793611\n",
      ">>> Epoch 3 train loss:  3.623349500739056\n",
      ">>> Epoch 3 test loss:  3.6274838343910547\n",
      ">>> Epoch 4 train loss:  3.6246335895165154\n",
      ">>> Epoch 4 test loss:  3.6263225182243017\n",
      ">>> Epoch 5 train loss:  3.624673773413119\n",
      ">>> Epoch 5 test loss:  3.621675543163134\n",
      ">>> Epoch 6 train loss:  3.6255375665167104\n",
      ">>> Epoch 6 test loss:  3.6279102304707402\n",
      ">>> Epoch 7 train loss:  3.624069929122925\n",
      ">>> Epoch 7 test loss:  3.627965885659923\n",
      ">>> Epoch 8 train loss:  3.622087914010753\n",
      ">>> Epoch 8 test loss:  3.6220062090002973\n",
      ">>> Epoch 9 train loss:  3.622345006984213\n",
      ">>> Epoch 9 test loss:  3.6196738118710727\n"
     ]
    }
   ],
   "source": [
    "vt.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb22ec7b-715a-4cf4-9bed-aec835532468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Epoch 0 train loss: 3.6593\n",
      ">>> Epoch 0 test loss: 3.6249\n",
      ">>> Epoch 1 train loss: 3.6268\n",
      ">>> Epoch 1 test loss: 3.6258\n",
      ">>> Epoch 2 train loss: 3.6250\n",
      ">>> Epoch 2 test loss: 3.6305\n",
      ">>> Epoch 3 train loss: 3.6243\n",
      ">>> Epoch 3 test loss: 3.6296\n",
      ">>> Epoch 4 train loss: 3.6231\n",
      ">>> Epoch 4 test loss: 3.6233\n",
      ">>> Epoch 5 train loss: 3.6211\n",
      ">>> Epoch 5 test loss: 3.6317\n",
      ">>> Epoch 6 train loss: 3.6215\n",
      ">>> Epoch 6 test loss: 3.6204\n",
      ">>> Epoch 7 train loss: 3.6231\n",
      ">>> Epoch 7 test loss: 3.6213\n",
      ">>> Epoch 8 train loss: 3.6215\n",
      ">>> Epoch 8 test loss: 3.6273\n",
      ">>> Epoch 9 train loss: 3.6197\n",
      ">>> Epoch 9 test loss: 3.6317\n"
     ]
    }
   ],
   "source": [
    "vt.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0cb46544-09fc-4ebc-a3cc-d4dcfec21bb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Epoch 0 train loss: 3.8365\n",
      ">>> Epoch 0 test loss: 3.7966\n",
      ">>> Epoch 1 train loss: 3.7101\n",
      ">>> Epoch 1 test loss: 3.7712\n",
      ">>> Epoch 2 train loss: 3.6750\n",
      ">>> Epoch 2 test loss: 3.7459\n",
      ">>> Epoch 3 train loss: 3.6398\n",
      ">>> Epoch 3 test loss: 3.7221\n",
      ">>> Epoch 4 train loss: 3.6063\n",
      ">>> Epoch 4 test loss: 3.6995\n",
      ">>> Epoch 5 train loss: 3.5437\n",
      ">>> Epoch 5 test loss: 3.6792\n",
      ">>> Epoch 6 train loss: 3.5176\n",
      ">>> Epoch 6 test loss: 3.6609\n",
      ">>> Epoch 7 train loss: 3.4620\n",
      ">>> Epoch 7 test loss: 3.6453\n",
      ">>> Epoch 8 train loss: 3.4246\n",
      ">>> Epoch 8 test loss: 3.6335\n",
      ">>> Epoch 9 train loss: 3.3759\n",
      ">>> Epoch 9 test loss: 3.6268\n"
     ]
    }
   ],
   "source": [
    "vt.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ebb486cf-14a9-4c6b-a32b-35c6a8ba7f41",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Epoch 0 train loss: 3.7755\n",
      ">>> Epoch 0 test loss: 3.3620\n",
      ">>> Epoch 1 train loss: 3.7275\n",
      ">>> Epoch 1 test loss: 3.3715\n",
      ">>> Epoch 2 train loss: 3.6470\n",
      ">>> Epoch 2 test loss: 3.3836\n",
      ">>> Epoch 3 train loss: 3.5205\n",
      ">>> Epoch 3 test loss: 3.4019\n",
      ">>> Epoch 4 train loss: 3.4938\n",
      ">>> Epoch 4 test loss: 3.4238\n",
      ">>> Epoch 5 train loss: 3.4389\n",
      ">>> Epoch 5 test loss: 3.4499\n",
      ">>> Epoch 6 train loss: 3.3357\n",
      ">>> Epoch 6 test loss: 3.4791\n",
      ">>> Epoch 7 train loss: 3.2711\n",
      ">>> Epoch 7 test loss: 3.5100\n",
      ">>> Epoch 8 train loss: 3.2730\n",
      ">>> Epoch 8 test loss: 3.5444\n",
      ">>> Epoch 9 train loss: 3.2285\n",
      ">>> Epoch 9 test loss: 3.5785\n",
      ">>> Epoch 10 train loss: 3.1984\n",
      ">>> Epoch 10 test loss: 3.6141\n",
      ">>> Epoch 11 train loss: 3.1737\n",
      ">>> Epoch 11 test loss: 3.6508\n",
      ">>> Epoch 12 train loss: 3.1295\n",
      ">>> Epoch 12 test loss: 3.6869\n",
      ">>> Epoch 13 train loss: 3.0987\n",
      ">>> Epoch 13 test loss: 3.7229\n",
      ">>> Epoch 14 train loss: 3.0606\n",
      ">>> Epoch 14 test loss: 3.7578\n",
      ">>> Epoch 15 train loss: 3.0136\n",
      ">>> Epoch 15 test loss: 3.7922\n",
      ">>> Epoch 16 train loss: 3.0274\n",
      ">>> Epoch 16 test loss: 3.8245\n",
      ">>> Epoch 17 train loss: 3.0278\n",
      ">>> Epoch 17 test loss: 3.8551\n",
      ">>> Epoch 18 train loss: 2.9648\n",
      ">>> Epoch 18 test loss: 3.8826\n",
      ">>> Epoch 19 train loss: 2.9740\n",
      ">>> Epoch 19 test loss: 3.9089\n",
      ">>> Epoch 20 train loss: 2.9079\n",
      ">>> Epoch 20 test loss: 3.9341\n",
      ">>> Epoch 21 train loss: 2.8726\n",
      ">>> Epoch 21 test loss: 3.9583\n",
      ">>> Epoch 22 train loss: 2.8944\n",
      ">>> Epoch 22 test loss: 3.9821\n",
      ">>> Epoch 23 train loss: 2.9002\n",
      ">>> Epoch 23 test loss: 4.0046\n",
      ">>> Epoch 24 train loss: 2.8732\n",
      ">>> Epoch 24 test loss: 4.0267\n",
      ">>> Epoch 25 train loss: 2.8010\n",
      ">>> Epoch 25 test loss: 4.0485\n",
      ">>> Epoch 26 train loss: 2.8441\n",
      ">>> Epoch 26 test loss: 4.0696\n",
      ">>> Epoch 27 train loss: 2.8532\n",
      ">>> Epoch 27 test loss: 4.0904\n",
      ">>> Epoch 28 train loss: 2.8487\n",
      ">>> Epoch 28 test loss: 4.1110\n",
      ">>> Epoch 29 train loss: 2.7931\n",
      ">>> Epoch 29 test loss: 4.1309\n",
      ">>> Epoch 30 train loss: 2.7912\n",
      ">>> Epoch 30 test loss: 4.1510\n",
      ">>> Epoch 31 train loss: 2.7751\n",
      ">>> Epoch 31 test loss: 4.1713\n",
      ">>> Epoch 32 train loss: 2.8263\n",
      ">>> Epoch 32 test loss: 4.1925\n",
      ">>> Epoch 33 train loss: 2.7503\n",
      ">>> Epoch 33 test loss: 4.2140\n",
      ">>> Epoch 34 train loss: 2.7624\n",
      ">>> Epoch 34 test loss: 4.2349\n",
      ">>> Epoch 35 train loss: 2.8111\n",
      ">>> Epoch 35 test loss: 4.2559\n",
      ">>> Epoch 36 train loss: 2.7836\n",
      ">>> Epoch 36 test loss: 4.2777\n",
      ">>> Epoch 37 train loss: 2.8100\n",
      ">>> Epoch 37 test loss: 4.2987\n",
      ">>> Epoch 38 train loss: 2.7493\n",
      ">>> Epoch 38 test loss: 4.3199\n",
      ">>> Epoch 39 train loss: 2.7421\n",
      ">>> Epoch 39 test loss: 4.3429\n",
      ">>> Epoch 40 train loss: 2.7233\n",
      ">>> Epoch 40 test loss: 4.3658\n",
      ">>> Epoch 41 train loss: 2.7873\n",
      ">>> Epoch 41 test loss: 4.3883\n",
      ">>> Epoch 42 train loss: 2.7272\n",
      ">>> Epoch 42 test loss: 4.4120\n",
      ">>> Epoch 43 train loss: 2.7437\n",
      ">>> Epoch 43 test loss: 4.4367\n",
      ">>> Epoch 44 train loss: 2.7196\n",
      ">>> Epoch 44 test loss: 4.4615\n",
      ">>> Epoch 45 train loss: 2.7007\n",
      ">>> Epoch 45 test loss: 4.4866\n",
      ">>> Epoch 46 train loss: 2.7087\n",
      ">>> Epoch 46 test loss: 4.5114\n",
      ">>> Epoch 47 train loss: 2.6813\n",
      ">>> Epoch 47 test loss: 4.5360\n",
      ">>> Epoch 48 train loss: 2.7149\n",
      ">>> Epoch 48 test loss: 4.5613\n",
      ">>> Epoch 49 train loss: 2.6831\n",
      ">>> Epoch 49 test loss: 4.5867\n"
     ]
    }
   ],
   "source": [
    "# reduce trainig set to size 16 test size 4 and test overfitting to learn dataset\n",
    "vt.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8cd2b7b4-d65f-45d5-9aa0-7599b845ca89",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Epoch 0 train loss: 3.5980\n",
      ">>> Epoch 0 test loss: 3.8934\n",
      ">>> Epoch 1 train loss: 3.5224\n",
      ">>> Epoch 1 test loss: 3.8602\n",
      ">>> Epoch 2 train loss: 3.4501\n",
      ">>> Epoch 2 test loss: 3.8291\n",
      ">>> Epoch 3 train loss: 3.3814\n",
      ">>> Epoch 3 test loss: 3.8007\n",
      ">>> Epoch 4 train loss: 3.3170\n",
      ">>> Epoch 4 test loss: 3.7753\n",
      ">>> Epoch 5 train loss: 3.2572\n",
      ">>> Epoch 5 test loss: 3.7528\n",
      ">>> Epoch 6 train loss: 3.2020\n",
      ">>> Epoch 6 test loss: 3.7335\n",
      ">>> Epoch 7 train loss: 3.1513\n",
      ">>> Epoch 7 test loss: 3.7176\n",
      ">>> Epoch 8 train loss: 3.1050\n",
      ">>> Epoch 8 test loss: 3.7055\n",
      ">>> Epoch 9 train loss: 3.0629\n",
      ">>> Epoch 9 test loss: 3.6971\n",
      ">>> Epoch 10 train loss: 3.0244\n",
      ">>> Epoch 10 test loss: 3.6922\n",
      ">>> Epoch 11 train loss: 2.9895\n",
      ">>> Epoch 11 test loss: 3.6906\n",
      ">>> Epoch 12 train loss: 2.9577\n",
      ">>> Epoch 12 test loss: 3.6920\n",
      ">>> Epoch 13 train loss: 2.9289\n",
      ">>> Epoch 13 test loss: 3.6962\n",
      ">>> Epoch 14 train loss: 2.9026\n",
      ">>> Epoch 14 test loss: 3.7031\n",
      ">>> Epoch 15 train loss: 2.8787\n",
      ">>> Epoch 15 test loss: 3.7126\n",
      ">>> Epoch 16 train loss: 2.8568\n",
      ">>> Epoch 16 test loss: 3.7244\n",
      ">>> Epoch 17 train loss: 2.8368\n",
      ">>> Epoch 17 test loss: 3.7385\n",
      ">>> Epoch 18 train loss: 2.8184\n",
      ">>> Epoch 18 test loss: 3.7548\n",
      ">>> Epoch 19 train loss: 2.8014\n",
      ">>> Epoch 19 test loss: 3.7729\n",
      ">>> Epoch 20 train loss: 2.7857\n",
      ">>> Epoch 20 test loss: 3.7929\n",
      ">>> Epoch 21 train loss: 2.7711\n",
      ">>> Epoch 21 test loss: 3.8145\n",
      ">>> Epoch 22 train loss: 2.7576\n",
      ">>> Epoch 22 test loss: 3.8374\n",
      ">>> Epoch 23 train loss: 2.7450\n",
      ">>> Epoch 23 test loss: 3.8615\n",
      ">>> Epoch 24 train loss: 2.7332\n",
      ">>> Epoch 24 test loss: 3.8866\n",
      ">>> Epoch 25 train loss: 2.7222\n",
      ">>> Epoch 25 test loss: 3.9123\n",
      ">>> Epoch 26 train loss: 2.7118\n",
      ">>> Epoch 26 test loss: 3.9385\n",
      ">>> Epoch 27 train loss: 2.7022\n",
      ">>> Epoch 27 test loss: 3.9650\n",
      ">>> Epoch 28 train loss: 2.6931\n",
      ">>> Epoch 28 test loss: 3.9917\n",
      ">>> Epoch 29 train loss: 2.6845\n",
      ">>> Epoch 29 test loss: 4.0183\n",
      ">>> Epoch 30 train loss: 2.6764\n",
      ">>> Epoch 30 test loss: 4.0448\n",
      ">>> Epoch 31 train loss: 2.6688\n",
      ">>> Epoch 31 test loss: 4.0711\n",
      ">>> Epoch 32 train loss: 2.6615\n",
      ">>> Epoch 32 test loss: 4.0971\n",
      ">>> Epoch 33 train loss: 2.6547\n",
      ">>> Epoch 33 test loss: 4.1227\n",
      ">>> Epoch 34 train loss: 2.6482\n",
      ">>> Epoch 34 test loss: 4.1480\n",
      ">>> Epoch 35 train loss: 2.6420\n",
      ">>> Epoch 35 test loss: 4.1730\n",
      ">>> Epoch 36 train loss: 2.6362\n",
      ">>> Epoch 36 test loss: 4.1976\n",
      ">>> Epoch 37 train loss: 2.6308\n",
      ">>> Epoch 37 test loss: 4.2219\n",
      ">>> Epoch 38 train loss: 2.6256\n",
      ">>> Epoch 38 test loss: 4.2457\n",
      ">>> Epoch 39 train loss: 2.6208\n",
      ">>> Epoch 39 test loss: 4.2691\n",
      ">>> Epoch 40 train loss: 2.6163\n",
      ">>> Epoch 40 test loss: 4.2922\n",
      ">>> Epoch 41 train loss: 2.6121\n",
      ">>> Epoch 41 test loss: 4.3148\n",
      ">>> Epoch 42 train loss: 2.6081\n",
      ">>> Epoch 42 test loss: 4.3371\n",
      ">>> Epoch 43 train loss: 2.6044\n",
      ">>> Epoch 43 test loss: 4.3590\n",
      ">>> Epoch 44 train loss: 2.6008\n",
      ">>> Epoch 44 test loss: 4.3805\n",
      ">>> Epoch 45 train loss: 2.5974\n",
      ">>> Epoch 45 test loss: 4.4015\n",
      ">>> Epoch 46 train loss: 2.5941\n",
      ">>> Epoch 46 test loss: 4.4219\n",
      ">>> Epoch 47 train loss: 2.5910\n",
      ">>> Epoch 47 test loss: 4.4417\n",
      ">>> Epoch 48 train loss: 2.5881\n",
      ">>> Epoch 48 test loss: 4.4608\n",
      ">>> Epoch 49 train loss: 2.5853\n",
      ">>> Epoch 49 test loss: 4.4791\n"
     ]
    }
   ],
   "source": [
    "# set p_dropout to 0\n",
    "vt.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0b76e94-207f-43e1-8171-41ebca889469",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Epoch 0 train loss: 3.7005\n",
      ">>> Epoch 0 test loss: 3.6886\n",
      ">>> Epoch 1 train loss: 3.6383\n",
      ">>> Epoch 1 test loss: 3.7175\n",
      ">>> Epoch 2 train loss: 3.5788\n",
      ">>> Epoch 2 test loss: 3.7461\n",
      ">>> Epoch 3 train loss: 3.5219\n",
      ">>> Epoch 3 test loss: 3.7739\n",
      ">>> Epoch 4 train loss: 3.4672\n",
      ">>> Epoch 4 test loss: 3.8010\n",
      ">>> Epoch 5 train loss: 3.4146\n",
      ">>> Epoch 5 test loss: 3.8282\n",
      ">>> Epoch 6 train loss: 3.3641\n",
      ">>> Epoch 6 test loss: 3.8560\n",
      ">>> Epoch 7 train loss: 3.3157\n",
      ">>> Epoch 7 test loss: 3.8852\n",
      ">>> Epoch 8 train loss: 3.2696\n",
      ">>> Epoch 8 test loss: 3.9164\n",
      ">>> Epoch 9 train loss: 3.2259\n",
      ">>> Epoch 9 test loss: 3.9502\n",
      ">>> Epoch 10 train loss: 3.1848\n",
      ">>> Epoch 10 test loss: 3.9868\n",
      ">>> Epoch 11 train loss: 3.1461\n",
      ">>> Epoch 11 test loss: 4.0265\n",
      ">>> Epoch 12 train loss: 3.1100\n",
      ">>> Epoch 12 test loss: 4.0692\n",
      ">>> Epoch 13 train loss: 3.0761\n",
      ">>> Epoch 13 test loss: 4.1144\n",
      ">>> Epoch 14 train loss: 3.0442\n",
      ">>> Epoch 14 test loss: 4.1617\n",
      ">>> Epoch 15 train loss: 3.0143\n",
      ">>> Epoch 15 test loss: 4.2106\n",
      ">>> Epoch 16 train loss: 2.9862\n",
      ">>> Epoch 16 test loss: 4.2607\n",
      ">>> Epoch 17 train loss: 2.9596\n",
      ">>> Epoch 17 test loss: 4.3115\n",
      ">>> Epoch 18 train loss: 2.9345\n",
      ">>> Epoch 18 test loss: 4.3628\n",
      ">>> Epoch 19 train loss: 2.9107\n",
      ">>> Epoch 19 test loss: 4.4143\n",
      ">>> Epoch 20 train loss: 2.8884\n",
      ">>> Epoch 20 test loss: 4.4659\n",
      ">>> Epoch 21 train loss: 2.8673\n",
      ">>> Epoch 21 test loss: 4.5173\n",
      ">>> Epoch 22 train loss: 2.8475\n",
      ">>> Epoch 22 test loss: 4.5684\n",
      ">>> Epoch 23 train loss: 2.8289\n",
      ">>> Epoch 23 test loss: 4.6190\n",
      ">>> Epoch 24 train loss: 2.8115\n",
      ">>> Epoch 24 test loss: 4.6689\n",
      ">>> Epoch 25 train loss: 2.7954\n",
      ">>> Epoch 25 test loss: 4.7177\n",
      ">>> Epoch 26 train loss: 2.7803\n",
      ">>> Epoch 26 test loss: 4.7654\n",
      ">>> Epoch 27 train loss: 2.7662\n",
      ">>> Epoch 27 test loss: 4.8117\n",
      ">>> Epoch 28 train loss: 2.7531\n",
      ">>> Epoch 28 test loss: 4.8567\n",
      ">>> Epoch 29 train loss: 2.7407\n",
      ">>> Epoch 29 test loss: 4.9002\n",
      ">>> Epoch 30 train loss: 2.7292\n",
      ">>> Epoch 30 test loss: 4.9424\n",
      ">>> Epoch 31 train loss: 2.7183\n",
      ">>> Epoch 31 test loss: 4.9835\n",
      ">>> Epoch 32 train loss: 2.7081\n",
      ">>> Epoch 32 test loss: 5.0237\n",
      ">>> Epoch 33 train loss: 2.6985\n",
      ">>> Epoch 33 test loss: 5.0632\n",
      ">>> Epoch 34 train loss: 2.6896\n",
      ">>> Epoch 34 test loss: 5.1023\n",
      ">>> Epoch 35 train loss: 2.6811\n",
      ">>> Epoch 35 test loss: 5.1412\n",
      ">>> Epoch 36 train loss: 2.6732\n",
      ">>> Epoch 36 test loss: 5.1802\n",
      ">>> Epoch 37 train loss: 2.6658\n",
      ">>> Epoch 37 test loss: 5.2193\n",
      ">>> Epoch 38 train loss: 2.6588\n",
      ">>> Epoch 38 test loss: 5.2587\n",
      ">>> Epoch 39 train loss: 2.6522\n",
      ">>> Epoch 39 test loss: 5.2982\n",
      ">>> Epoch 40 train loss: 2.6460\n",
      ">>> Epoch 40 test loss: 5.3380\n",
      ">>> Epoch 41 train loss: 2.6401\n",
      ">>> Epoch 41 test loss: 5.3779\n",
      ">>> Epoch 42 train loss: 2.6345\n",
      ">>> Epoch 42 test loss: 5.4177\n",
      ">>> Epoch 43 train loss: 2.6293\n",
      ">>> Epoch 43 test loss: 5.4573\n",
      ">>> Epoch 44 train loss: 2.6243\n",
      ">>> Epoch 44 test loss: 5.4967\n",
      ">>> Epoch 45 train loss: 2.6197\n",
      ">>> Epoch 45 test loss: 5.5355\n",
      ">>> Epoch 46 train loss: 2.6153\n",
      ">>> Epoch 46 test loss: 5.5739\n",
      ">>> Epoch 47 train loss: 2.6111\n",
      ">>> Epoch 47 test loss: 5.6118\n",
      ">>> Epoch 48 train loss: 2.6072\n",
      ">>> Epoch 48 test loss: 5.6490\n",
      ">>> Epoch 49 train loss: 2.6035\n",
      ">>> Epoch 49 test loss: 5.6858\n",
      ">>> Epoch 50 train loss: 2.6000\n",
      ">>> Epoch 50 test loss: 5.7222\n",
      ">>> Epoch 51 train loss: 2.5966\n",
      ">>> Epoch 51 test loss: 5.7582\n",
      ">>> Epoch 52 train loss: 2.5935\n",
      ">>> Epoch 52 test loss: 5.7938\n",
      ">>> Epoch 53 train loss: 2.5905\n",
      ">>> Epoch 53 test loss: 5.8292\n",
      ">>> Epoch 54 train loss: 2.5876\n",
      ">>> Epoch 54 test loss: 5.8643\n",
      ">>> Epoch 55 train loss: 2.5849\n",
      ">>> Epoch 55 test loss: 5.8990\n",
      ">>> Epoch 56 train loss: 2.5823\n",
      ">>> Epoch 56 test loss: 5.9335\n",
      ">>> Epoch 57 train loss: 2.5799\n",
      ">>> Epoch 57 test loss: 5.9675\n",
      ">>> Epoch 58 train loss: 2.5776\n",
      ">>> Epoch 58 test loss: 6.0011\n",
      ">>> Epoch 59 train loss: 2.5754\n",
      ">>> Epoch 59 test loss: 6.0343\n",
      ">>> Epoch 60 train loss: 2.5733\n",
      ">>> Epoch 60 test loss: 6.0669\n",
      ">>> Epoch 61 train loss: 2.5713\n",
      ">>> Epoch 61 test loss: 6.0990\n",
      ">>> Epoch 62 train loss: 2.5695\n",
      ">>> Epoch 62 test loss: 6.1306\n",
      ">>> Epoch 63 train loss: 2.5677\n",
      ">>> Epoch 63 test loss: 6.1617\n",
      ">>> Epoch 64 train loss: 2.5660\n",
      ">>> Epoch 64 test loss: 6.1922\n",
      ">>> Epoch 65 train loss: 2.5644\n",
      ">>> Epoch 65 test loss: 6.2223\n",
      ">>> Epoch 66 train loss: 2.5629\n",
      ">>> Epoch 66 test loss: 6.2519\n",
      ">>> Epoch 67 train loss: 2.5614\n",
      ">>> Epoch 67 test loss: 6.2810\n",
      ">>> Epoch 68 train loss: 2.5600\n",
      ">>> Epoch 68 test loss: 6.3097\n",
      ">>> Epoch 69 train loss: 2.5587\n",
      ">>> Epoch 69 test loss: 6.3378\n",
      ">>> Epoch 70 train loss: 2.5574\n",
      ">>> Epoch 70 test loss: 6.3655\n",
      ">>> Epoch 71 train loss: 2.5562\n",
      ">>> Epoch 71 test loss: 6.3926\n",
      ">>> Epoch 72 train loss: 2.5550\n",
      ">>> Epoch 72 test loss: 6.4192\n",
      ">>> Epoch 73 train loss: 2.5539\n",
      ">>> Epoch 73 test loss: 6.4451\n",
      ">>> Epoch 74 train loss: 2.5528\n",
      ">>> Epoch 74 test loss: 6.4706\n",
      ">>> Epoch 75 train loss: 2.5518\n",
      ">>> Epoch 75 test loss: 6.4955\n",
      ">>> Epoch 76 train loss: 2.5508\n",
      ">>> Epoch 76 test loss: 6.5199\n",
      ">>> Epoch 77 train loss: 2.5499\n",
      ">>> Epoch 77 test loss: 6.5439\n",
      ">>> Epoch 78 train loss: 2.5490\n",
      ">>> Epoch 78 test loss: 6.5675\n",
      ">>> Epoch 79 train loss: 2.5481\n",
      ">>> Epoch 79 test loss: 6.5908\n",
      ">>> Epoch 80 train loss: 2.5473\n",
      ">>> Epoch 80 test loss: 6.6137\n",
      ">>> Epoch 81 train loss: 2.5465\n",
      ">>> Epoch 81 test loss: 6.6364\n",
      ">>> Epoch 82 train loss: 2.5457\n",
      ">>> Epoch 82 test loss: 6.6589\n",
      ">>> Epoch 83 train loss: 2.5450\n",
      ">>> Epoch 83 test loss: 6.6811\n",
      ">>> Epoch 84 train loss: 2.5443\n",
      ">>> Epoch 84 test loss: 6.7030\n",
      ">>> Epoch 85 train loss: 2.5436\n",
      ">>> Epoch 85 test loss: 6.7246\n",
      ">>> Epoch 86 train loss: 2.5429\n",
      ">>> Epoch 86 test loss: 6.7460\n",
      ">>> Epoch 87 train loss: 2.5423\n",
      ">>> Epoch 87 test loss: 6.7670\n",
      ">>> Epoch 88 train loss: 2.5417\n",
      ">>> Epoch 88 test loss: 6.7877\n",
      ">>> Epoch 89 train loss: 2.5411\n",
      ">>> Epoch 89 test loss: 6.8081\n",
      ">>> Epoch 90 train loss: 2.5405\n",
      ">>> Epoch 90 test loss: 6.8282\n",
      ">>> Epoch 91 train loss: 2.5399\n",
      ">>> Epoch 91 test loss: 6.8479\n",
      ">>> Epoch 92 train loss: 2.5394\n",
      ">>> Epoch 92 test loss: 6.8674\n",
      ">>> Epoch 93 train loss: 2.5389\n",
      ">>> Epoch 93 test loss: 6.8867\n",
      ">>> Epoch 94 train loss: 2.5384\n",
      ">>> Epoch 94 test loss: 6.9057\n",
      ">>> Epoch 95 train loss: 2.5379\n",
      ">>> Epoch 95 test loss: 6.9245\n",
      ">>> Epoch 96 train loss: 2.5374\n",
      ">>> Epoch 96 test loss: 6.9431\n",
      ">>> Epoch 97 train loss: 2.5369\n",
      ">>> Epoch 97 test loss: 6.9614\n",
      ">>> Epoch 98 train loss: 2.5365\n",
      ">>> Epoch 98 test loss: 6.9796\n",
      ">>> Epoch 99 train loss: 2.5361\n",
      ">>> Epoch 99 test loss: 6.9975\n",
      ">>> Epoch 100 train loss: 2.5356\n",
      ">>> Epoch 100 test loss: 7.0152\n",
      ">>> Epoch 101 train loss: 2.5352\n",
      ">>> Epoch 101 test loss: 7.0327\n",
      ">>> Epoch 102 train loss: 2.5348\n",
      ">>> Epoch 102 test loss: 7.0501\n",
      ">>> Epoch 103 train loss: 2.5345\n",
      ">>> Epoch 103 test loss: 7.0672\n",
      ">>> Epoch 104 train loss: 2.5341\n",
      ">>> Epoch 104 test loss: 7.0841\n",
      ">>> Epoch 105 train loss: 2.5337\n",
      ">>> Epoch 105 test loss: 7.1008\n",
      ">>> Epoch 106 train loss: 2.5334\n",
      ">>> Epoch 106 test loss: 7.1173\n",
      ">>> Epoch 107 train loss: 2.5330\n",
      ">>> Epoch 107 test loss: 7.1337\n",
      ">>> Epoch 108 train loss: 2.5327\n",
      ">>> Epoch 108 test loss: 7.1499\n",
      ">>> Epoch 109 train loss: 2.5324\n",
      ">>> Epoch 109 test loss: 7.1659\n",
      ">>> Epoch 110 train loss: 2.5321\n",
      ">>> Epoch 110 test loss: 7.1817\n",
      ">>> Epoch 111 train loss: 2.5317\n",
      ">>> Epoch 111 test loss: 7.1973\n",
      ">>> Epoch 112 train loss: 2.5314\n",
      ">>> Epoch 112 test loss: 7.2128\n",
      ">>> Epoch 113 train loss: 2.5311\n",
      ">>> Epoch 113 test loss: 7.2281\n",
      ">>> Epoch 114 train loss: 2.5309\n",
      ">>> Epoch 114 test loss: 7.2433\n",
      ">>> Epoch 115 train loss: 2.5306\n",
      ">>> Epoch 115 test loss: 7.2584\n",
      ">>> Epoch 116 train loss: 2.5303\n",
      ">>> Epoch 116 test loss: 7.2734\n",
      ">>> Epoch 117 train loss: 2.5300\n",
      ">>> Epoch 117 test loss: 7.2882\n",
      ">>> Epoch 118 train loss: 2.5298\n",
      ">>> Epoch 118 test loss: 7.3029\n",
      ">>> Epoch 119 train loss: 2.5295\n",
      ">>> Epoch 119 test loss: 7.3175\n",
      ">>> Epoch 120 train loss: 2.5293\n",
      ">>> Epoch 120 test loss: 7.3319\n",
      ">>> Epoch 121 train loss: 2.5290\n",
      ">>> Epoch 121 test loss: 7.3462\n",
      ">>> Epoch 122 train loss: 2.5288\n",
      ">>> Epoch 122 test loss: 7.3604\n",
      ">>> Epoch 123 train loss: 2.5286\n",
      ">>> Epoch 123 test loss: 7.3745\n",
      ">>> Epoch 124 train loss: 2.5284\n",
      ">>> Epoch 124 test loss: 7.3884\n",
      ">>> Epoch 125 train loss: 2.5281\n",
      ">>> Epoch 125 test loss: 7.4022\n",
      ">>> Epoch 126 train loss: 2.5279\n",
      ">>> Epoch 126 test loss: 7.4159\n",
      ">>> Epoch 127 train loss: 2.5277\n",
      ">>> Epoch 127 test loss: 7.4295\n",
      ">>> Epoch 128 train loss: 2.5275\n",
      ">>> Epoch 128 test loss: 7.4430\n",
      ">>> Epoch 129 train loss: 2.5273\n",
      ">>> Epoch 129 test loss: 7.4564\n",
      ">>> Epoch 130 train loss: 2.5271\n",
      ">>> Epoch 130 test loss: 7.4696\n",
      ">>> Epoch 131 train loss: 2.5269\n",
      ">>> Epoch 131 test loss: 7.4828\n",
      ">>> Epoch 132 train loss: 2.5267\n",
      ">>> Epoch 132 test loss: 7.4959\n",
      ">>> Epoch 133 train loss: 2.5265\n",
      ">>> Epoch 133 test loss: 7.5088\n",
      ">>> Epoch 134 train loss: 2.5264\n",
      ">>> Epoch 134 test loss: 7.5217\n",
      ">>> Epoch 135 train loss: 2.5262\n",
      ">>> Epoch 135 test loss: 7.5345\n",
      ">>> Epoch 136 train loss: 2.5260\n",
      ">>> Epoch 136 test loss: 7.5471\n",
      ">>> Epoch 137 train loss: 2.5258\n",
      ">>> Epoch 137 test loss: 7.5597\n",
      ">>> Epoch 138 train loss: 2.5257\n",
      ">>> Epoch 138 test loss: 7.5722\n",
      ">>> Epoch 139 train loss: 2.5255\n",
      ">>> Epoch 139 test loss: 7.5847\n",
      ">>> Epoch 140 train loss: 2.5253\n",
      ">>> Epoch 140 test loss: 7.5970\n",
      ">>> Epoch 141 train loss: 2.5252\n",
      ">>> Epoch 141 test loss: 7.6092\n",
      ">>> Epoch 142 train loss: 2.5250\n",
      ">>> Epoch 142 test loss: 7.6214\n",
      ">>> Epoch 143 train loss: 2.5249\n",
      ">>> Epoch 143 test loss: 7.6335\n",
      ">>> Epoch 144 train loss: 2.5247\n",
      ">>> Epoch 144 test loss: 7.6455\n",
      ">>> Epoch 145 train loss: 2.5246\n",
      ">>> Epoch 145 test loss: 7.6574\n",
      ">>> Epoch 146 train loss: 2.5244\n",
      ">>> Epoch 146 test loss: 7.6692\n",
      ">>> Epoch 147 train loss: 2.5243\n",
      ">>> Epoch 147 test loss: 7.6810\n",
      ">>> Epoch 148 train loss: 2.5242\n",
      ">>> Epoch 148 test loss: 7.6926\n",
      ">>> Epoch 149 train loss: 2.5240\n",
      ">>> Epoch 149 test loss: 7.7042\n",
      ">>> Epoch 150 train loss: 2.5239\n",
      ">>> Epoch 150 test loss: 7.7157\n",
      ">>> Epoch 151 train loss: 2.5238\n",
      ">>> Epoch 151 test loss: 7.7272\n",
      ">>> Epoch 152 train loss: 2.5236\n",
      ">>> Epoch 152 test loss: 7.7385\n",
      ">>> Epoch 153 train loss: 2.5235\n",
      ">>> Epoch 153 test loss: 7.7498\n",
      ">>> Epoch 154 train loss: 2.5234\n",
      ">>> Epoch 154 test loss: 7.7610\n",
      ">>> Epoch 155 train loss: 2.5233\n",
      ">>> Epoch 155 test loss: 7.7722\n",
      ">>> Epoch 156 train loss: 2.5231\n",
      ">>> Epoch 156 test loss: 7.7833\n",
      ">>> Epoch 157 train loss: 2.5230\n",
      ">>> Epoch 157 test loss: 7.7943\n",
      ">>> Epoch 158 train loss: 2.5229\n",
      ">>> Epoch 158 test loss: 7.8052\n",
      ">>> Epoch 159 train loss: 2.5228\n",
      ">>> Epoch 159 test loss: 7.8161\n",
      ">>> Epoch 160 train loss: 2.5227\n",
      ">>> Epoch 160 test loss: 7.8269\n",
      ">>> Epoch 161 train loss: 2.5226\n",
      ">>> Epoch 161 test loss: 7.8377\n",
      ">>> Epoch 162 train loss: 2.5225\n",
      ">>> Epoch 162 test loss: 7.8484\n",
      ">>> Epoch 163 train loss: 2.5224\n",
      ">>> Epoch 163 test loss: 7.8590\n",
      ">>> Epoch 164 train loss: 2.5223\n",
      ">>> Epoch 164 test loss: 7.8696\n",
      ">>> Epoch 165 train loss: 2.5222\n",
      ">>> Epoch 165 test loss: 7.8801\n",
      ">>> Epoch 166 train loss: 2.5221\n",
      ">>> Epoch 166 test loss: 7.8905\n",
      ">>> Epoch 167 train loss: 2.5220\n",
      ">>> Epoch 167 test loss: 7.9009\n",
      ">>> Epoch 168 train loss: 2.5219\n",
      ">>> Epoch 168 test loss: 7.9112\n",
      ">>> Epoch 169 train loss: 2.5218\n",
      ">>> Epoch 169 test loss: 7.9215\n",
      ">>> Epoch 170 train loss: 2.5217\n",
      ">>> Epoch 170 test loss: 7.9317\n",
      ">>> Epoch 171 train loss: 2.5216\n",
      ">>> Epoch 171 test loss: 7.9418\n",
      ">>> Epoch 172 train loss: 2.5215\n",
      ">>> Epoch 172 test loss: 7.9519\n",
      ">>> Epoch 173 train loss: 2.5214\n",
      ">>> Epoch 173 test loss: 7.9619\n",
      ">>> Epoch 174 train loss: 2.5213\n",
      ">>> Epoch 174 test loss: 7.9719\n",
      ">>> Epoch 175 train loss: 2.5212\n",
      ">>> Epoch 175 test loss: 7.9818\n",
      ">>> Epoch 176 train loss: 2.5211\n",
      ">>> Epoch 176 test loss: 7.9917\n",
      ">>> Epoch 177 train loss: 2.5210\n",
      ">>> Epoch 177 test loss: 8.0015\n",
      ">>> Epoch 178 train loss: 2.5210\n",
      ">>> Epoch 178 test loss: 8.0113\n",
      ">>> Epoch 179 train loss: 2.5209\n",
      ">>> Epoch 179 test loss: 8.0210\n",
      ">>> Epoch 180 train loss: 2.5208\n",
      ">>> Epoch 180 test loss: 8.0306\n",
      ">>> Epoch 181 train loss: 2.5207\n",
      ">>> Epoch 181 test loss: 8.0403\n",
      ">>> Epoch 182 train loss: 2.5206\n",
      ">>> Epoch 182 test loss: 8.0498\n",
      ">>> Epoch 183 train loss: 2.5206\n",
      ">>> Epoch 183 test loss: 8.0593\n",
      ">>> Epoch 184 train loss: 2.5205\n",
      ">>> Epoch 184 test loss: 8.0688\n",
      ">>> Epoch 185 train loss: 2.5204\n",
      ">>> Epoch 185 test loss: 8.0782\n",
      ">>> Epoch 186 train loss: 2.5203\n",
      ">>> Epoch 186 test loss: 8.0876\n",
      ">>> Epoch 187 train loss: 2.5203\n",
      ">>> Epoch 187 test loss: 8.0969\n",
      ">>> Epoch 188 train loss: 2.5202\n",
      ">>> Epoch 188 test loss: 8.1061\n",
      ">>> Epoch 189 train loss: 2.5201\n",
      ">>> Epoch 189 test loss: 8.1154\n",
      ">>> Epoch 190 train loss: 2.5201\n",
      ">>> Epoch 190 test loss: 8.1245\n",
      ">>> Epoch 191 train loss: 2.5200\n",
      ">>> Epoch 191 test loss: 8.1337\n",
      ">>> Epoch 192 train loss: 2.5199\n",
      ">>> Epoch 192 test loss: 8.1427\n",
      ">>> Epoch 193 train loss: 2.5199\n",
      ">>> Epoch 193 test loss: 8.1518\n",
      ">>> Epoch 194 train loss: 2.5198\n",
      ">>> Epoch 194 test loss: 8.1608\n",
      ">>> Epoch 195 train loss: 2.5197\n",
      ">>> Epoch 195 test loss: 8.1697\n",
      ">>> Epoch 196 train loss: 2.5197\n",
      ">>> Epoch 196 test loss: 8.1786\n",
      ">>> Epoch 197 train loss: 2.5196\n",
      ">>> Epoch 197 test loss: 8.1875\n",
      ">>> Epoch 198 train loss: 2.5195\n",
      ">>> Epoch 198 test loss: 8.1963\n",
      ">>> Epoch 199 train loss: 2.5195\n",
      ">>> Epoch 199 test loss: 8.2051\n",
      ">>> Epoch 200 train loss: 2.5194\n",
      ">>> Epoch 200 test loss: 8.2138\n",
      ">>> Epoch 201 train loss: 2.5194\n",
      ">>> Epoch 201 test loss: 8.2225\n",
      ">>> Epoch 202 train loss: 2.5193\n",
      ">>> Epoch 202 test loss: 8.2312\n",
      ">>> Epoch 203 train loss: 2.5192\n",
      ">>> Epoch 203 test loss: 8.2398\n",
      ">>> Epoch 204 train loss: 2.5192\n",
      ">>> Epoch 204 test loss: 8.2484\n",
      ">>> Epoch 205 train loss: 2.5191\n",
      ">>> Epoch 205 test loss: 8.2569\n",
      ">>> Epoch 206 train loss: 2.5191\n",
      ">>> Epoch 206 test loss: 8.2654\n",
      ">>> Epoch 207 train loss: 2.5190\n",
      ">>> Epoch 207 test loss: 8.2739\n",
      ">>> Epoch 208 train loss: 2.5190\n",
      ">>> Epoch 208 test loss: 8.2823\n",
      ">>> Epoch 209 train loss: 2.5189\n",
      ">>> Epoch 209 test loss: 8.2907\n",
      ">>> Epoch 210 train loss: 2.5189\n",
      ">>> Epoch 210 test loss: 8.2990\n",
      ">>> Epoch 211 train loss: 2.5188\n",
      ">>> Epoch 211 test loss: 8.3073\n",
      ">>> Epoch 212 train loss: 2.5188\n",
      ">>> Epoch 212 test loss: 8.3156\n",
      ">>> Epoch 213 train loss: 2.5187\n",
      ">>> Epoch 213 test loss: 8.3238\n",
      ">>> Epoch 214 train loss: 2.5187\n",
      ">>> Epoch 214 test loss: 8.3320\n",
      ">>> Epoch 215 train loss: 2.5186\n",
      ">>> Epoch 215 test loss: 8.3402\n",
      ">>> Epoch 216 train loss: 2.5186\n",
      ">>> Epoch 216 test loss: 8.3483\n",
      ">>> Epoch 217 train loss: 2.5185\n",
      ">>> Epoch 217 test loss: 8.3564\n",
      ">>> Epoch 218 train loss: 2.5185\n",
      ">>> Epoch 218 test loss: 8.3644\n",
      ">>> Epoch 219 train loss: 2.5184\n",
      ">>> Epoch 219 test loss: 8.3725\n",
      ">>> Epoch 220 train loss: 2.5184\n",
      ">>> Epoch 220 test loss: 8.3804\n",
      ">>> Epoch 221 train loss: 2.5183\n",
      ">>> Epoch 221 test loss: 8.3884\n",
      ">>> Epoch 222 train loss: 2.5183\n",
      ">>> Epoch 222 test loss: 8.3963\n",
      ">>> Epoch 223 train loss: 2.5182\n",
      ">>> Epoch 223 test loss: 8.4042\n",
      ">>> Epoch 224 train loss: 2.5182\n",
      ">>> Epoch 224 test loss: 8.4120\n",
      ">>> Epoch 225 train loss: 2.5181\n",
      ">>> Epoch 225 test loss: 8.4198\n",
      ">>> Epoch 226 train loss: 2.5181\n",
      ">>> Epoch 226 test loss: 8.4276\n",
      ">>> Epoch 227 train loss: 2.5181\n",
      ">>> Epoch 227 test loss: 8.4354\n",
      ">>> Epoch 228 train loss: 2.5180\n",
      ">>> Epoch 228 test loss: 8.4431\n",
      ">>> Epoch 229 train loss: 2.5180\n",
      ">>> Epoch 229 test loss: 8.4508\n",
      ">>> Epoch 230 train loss: 2.5179\n",
      ">>> Epoch 230 test loss: 8.4584\n",
      ">>> Epoch 231 train loss: 2.5179\n",
      ">>> Epoch 231 test loss: 8.4661\n",
      ">>> Epoch 232 train loss: 2.5179\n",
      ">>> Epoch 232 test loss: 8.4736\n",
      ">>> Epoch 233 train loss: 2.5178\n",
      ">>> Epoch 233 test loss: 8.4812\n",
      ">>> Epoch 234 train loss: 2.5178\n",
      ">>> Epoch 234 test loss: 8.4887\n",
      ">>> Epoch 235 train loss: 2.5177\n",
      ">>> Epoch 235 test loss: 8.4962\n",
      ">>> Epoch 236 train loss: 2.5177\n",
      ">>> Epoch 236 test loss: 8.5037\n",
      ">>> Epoch 237 train loss: 2.5177\n",
      ">>> Epoch 237 test loss: 8.5111\n",
      ">>> Epoch 238 train loss: 2.5176\n",
      ">>> Epoch 238 test loss: 8.5185\n",
      ">>> Epoch 239 train loss: 2.5176\n",
      ">>> Epoch 239 test loss: 8.5259\n",
      ">>> Epoch 240 train loss: 2.5175\n",
      ">>> Epoch 240 test loss: 8.5333\n",
      ">>> Epoch 241 train loss: 2.5175\n",
      ">>> Epoch 241 test loss: 8.5406\n",
      ">>> Epoch 242 train loss: 2.5175\n",
      ">>> Epoch 242 test loss: 8.5479\n",
      ">>> Epoch 243 train loss: 2.5174\n",
      ">>> Epoch 243 test loss: 8.5552\n",
      ">>> Epoch 244 train loss: 2.5174\n",
      ">>> Epoch 244 test loss: 8.5624\n",
      ">>> Epoch 245 train loss: 2.5174\n",
      ">>> Epoch 245 test loss: 8.5696\n",
      ">>> Epoch 246 train loss: 2.5173\n",
      ">>> Epoch 246 test loss: 8.5768\n",
      ">>> Epoch 247 train loss: 2.5173\n",
      ">>> Epoch 247 test loss: 8.5839\n",
      ">>> Epoch 248 train loss: 2.5173\n",
      ">>> Epoch 248 test loss: 8.5911\n",
      ">>> Epoch 249 train loss: 2.5172\n",
      ">>> Epoch 249 test loss: 8.5982\n",
      ">>> Epoch 250 train loss: 2.5172\n",
      ">>> Epoch 250 test loss: 8.6052\n",
      ">>> Epoch 251 train loss: 2.5172\n",
      ">>> Epoch 251 test loss: 8.6123\n",
      ">>> Epoch 252 train loss: 2.5171\n",
      ">>> Epoch 252 test loss: 8.6193\n",
      ">>> Epoch 253 train loss: 2.5171\n",
      ">>> Epoch 253 test loss: 8.6263\n",
      ">>> Epoch 254 train loss: 2.5171\n",
      ">>> Epoch 254 test loss: 8.6332\n",
      ">>> Epoch 255 train loss: 2.5170\n",
      ">>> Epoch 255 test loss: 8.6402\n",
      ">>> Epoch 256 train loss: 2.5170\n",
      ">>> Epoch 256 test loss: 8.6471\n",
      ">>> Epoch 257 train loss: 2.5170\n",
      ">>> Epoch 257 test loss: 8.6540\n",
      ">>> Epoch 258 train loss: 2.5170\n",
      ">>> Epoch 258 test loss: 8.6608\n",
      ">>> Epoch 259 train loss: 2.5169\n",
      ">>> Epoch 259 test loss: 8.6677\n",
      ">>> Epoch 260 train loss: 2.5169\n",
      ">>> Epoch 260 test loss: 8.6745\n",
      ">>> Epoch 261 train loss: 2.5169\n",
      ">>> Epoch 261 test loss: 8.6813\n",
      ">>> Epoch 262 train loss: 2.5168\n",
      ">>> Epoch 262 test loss: 8.6880\n",
      ">>> Epoch 263 train loss: 2.5168\n",
      ">>> Epoch 263 test loss: 8.6948\n",
      ">>> Epoch 264 train loss: 2.5168\n",
      ">>> Epoch 264 test loss: 8.7015\n",
      ">>> Epoch 265 train loss: 2.5168\n",
      ">>> Epoch 265 test loss: 8.7082\n",
      ">>> Epoch 266 train loss: 2.5167\n",
      ">>> Epoch 266 test loss: 8.7149\n",
      ">>> Epoch 267 train loss: 2.5167\n",
      ">>> Epoch 267 test loss: 8.7215\n",
      ">>> Epoch 268 train loss: 2.5167\n",
      ">>> Epoch 268 test loss: 8.7281\n",
      ">>> Epoch 269 train loss: 2.5166\n",
      ">>> Epoch 269 test loss: 8.7347\n",
      ">>> Epoch 270 train loss: 2.5166\n",
      ">>> Epoch 270 test loss: 8.7413\n",
      ">>> Epoch 271 train loss: 2.5166\n",
      ">>> Epoch 271 test loss: 8.7478\n",
      ">>> Epoch 272 train loss: 2.5166\n",
      ">>> Epoch 272 test loss: 8.7544\n",
      ">>> Epoch 273 train loss: 2.5165\n",
      ">>> Epoch 273 test loss: 8.7609\n",
      ">>> Epoch 274 train loss: 2.5165\n",
      ">>> Epoch 274 test loss: 8.7674\n",
      ">>> Epoch 275 train loss: 2.5165\n",
      ">>> Epoch 275 test loss: 8.7738\n",
      ">>> Epoch 276 train loss: 2.5165\n",
      ">>> Epoch 276 test loss: 8.7803\n",
      ">>> Epoch 277 train loss: 2.5164\n",
      ">>> Epoch 277 test loss: 8.7867\n",
      ">>> Epoch 278 train loss: 2.5164\n",
      ">>> Epoch 278 test loss: 8.7931\n",
      ">>> Epoch 279 train loss: 2.5164\n",
      ">>> Epoch 279 test loss: 8.7994\n",
      ">>> Epoch 280 train loss: 2.5164\n",
      ">>> Epoch 280 test loss: 8.8058\n",
      ">>> Epoch 281 train loss: 2.5163\n",
      ">>> Epoch 281 test loss: 8.8121\n",
      ">>> Epoch 282 train loss: 2.5163\n",
      ">>> Epoch 282 test loss: 8.8184\n",
      ">>> Epoch 283 train loss: 2.5163\n",
      ">>> Epoch 283 test loss: 8.8247\n",
      ">>> Epoch 284 train loss: 2.5163\n",
      ">>> Epoch 284 test loss: 8.8310\n",
      ">>> Epoch 285 train loss: 2.5162\n",
      ">>> Epoch 285 test loss: 8.8372\n",
      ">>> Epoch 286 train loss: 2.5162\n",
      ">>> Epoch 286 test loss: 8.8434\n",
      ">>> Epoch 287 train loss: 2.5162\n",
      ">>> Epoch 287 test loss: 8.8497\n",
      ">>> Epoch 288 train loss: 2.5162\n",
      ">>> Epoch 288 test loss: 8.8558\n",
      ">>> Epoch 289 train loss: 2.5162\n",
      ">>> Epoch 289 test loss: 8.8620\n",
      ">>> Epoch 290 train loss: 2.5161\n",
      ">>> Epoch 290 test loss: 8.8681\n",
      ">>> Epoch 291 train loss: 2.5161\n",
      ">>> Epoch 291 test loss: 8.8743\n",
      ">>> Epoch 292 train loss: 2.5161\n",
      ">>> Epoch 292 test loss: 8.8804\n",
      ">>> Epoch 293 train loss: 2.5161\n",
      ">>> Epoch 293 test loss: 8.8864\n",
      ">>> Epoch 294 train loss: 2.5161\n",
      ">>> Epoch 294 test loss: 8.8925\n",
      ">>> Epoch 295 train loss: 2.5160\n",
      ">>> Epoch 295 test loss: 8.8986\n",
      ">>> Epoch 296 train loss: 2.5160\n",
      ">>> Epoch 296 test loss: 8.9046\n",
      ">>> Epoch 297 train loss: 2.5160\n",
      ">>> Epoch 297 test loss: 8.9106\n",
      ">>> Epoch 298 train loss: 2.5160\n",
      ">>> Epoch 298 test loss: 8.9166\n",
      ">>> Epoch 299 train loss: 2.5160\n",
      ">>> Epoch 299 test loss: 8.9225\n",
      ">>> Epoch 300 train loss: 2.5159\n",
      ">>> Epoch 300 test loss: 8.9285\n",
      ">>> Epoch 301 train loss: 2.5159\n",
      ">>> Epoch 301 test loss: 8.9344\n",
      ">>> Epoch 302 train loss: 2.5159\n",
      ">>> Epoch 302 test loss: 8.9403\n",
      ">>> Epoch 303 train loss: 2.5159\n",
      ">>> Epoch 303 test loss: 8.9462\n",
      ">>> Epoch 304 train loss: 2.5159\n",
      ">>> Epoch 304 test loss: 8.9521\n",
      ">>> Epoch 305 train loss: 2.5158\n",
      ">>> Epoch 305 test loss: 8.9579\n",
      ">>> Epoch 306 train loss: 2.5158\n",
      ">>> Epoch 306 test loss: 8.9638\n",
      ">>> Epoch 307 train loss: 2.5158\n",
      ">>> Epoch 307 test loss: 8.9696\n",
      ">>> Epoch 308 train loss: 2.5158\n",
      ">>> Epoch 308 test loss: 8.9754\n",
      ">>> Epoch 309 train loss: 2.5158\n",
      ">>> Epoch 309 test loss: 8.9812\n",
      ">>> Epoch 310 train loss: 2.5157\n",
      ">>> Epoch 310 test loss: 8.9870\n",
      ">>> Epoch 311 train loss: 2.5157\n",
      ">>> Epoch 311 test loss: 8.9927\n",
      ">>> Epoch 312 train loss: 2.5157\n",
      ">>> Epoch 312 test loss: 8.9984\n",
      ">>> Epoch 313 train loss: 2.5157\n",
      ">>> Epoch 313 test loss: 9.0042\n",
      ">>> Epoch 314 train loss: 2.5157\n",
      ">>> Epoch 314 test loss: 9.0098\n",
      ">>> Epoch 315 train loss: 2.5157\n",
      ">>> Epoch 315 test loss: 9.0155\n",
      ">>> Epoch 316 train loss: 2.5156\n",
      ">>> Epoch 316 test loss: 9.0212\n",
      ">>> Epoch 317 train loss: 2.5156\n",
      ">>> Epoch 317 test loss: 9.0268\n",
      ">>> Epoch 318 train loss: 2.5156\n",
      ">>> Epoch 318 test loss: 9.0325\n",
      ">>> Epoch 319 train loss: 2.5156\n",
      ">>> Epoch 319 test loss: 9.0381\n",
      ">>> Epoch 320 train loss: 2.5156\n",
      ">>> Epoch 320 test loss: 9.0437\n",
      ">>> Epoch 321 train loss: 2.5156\n",
      ">>> Epoch 321 test loss: 9.0493\n",
      ">>> Epoch 322 train loss: 2.5155\n",
      ">>> Epoch 322 test loss: 9.0548\n",
      ">>> Epoch 323 train loss: 2.5155\n",
      ">>> Epoch 323 test loss: 9.0604\n",
      ">>> Epoch 324 train loss: 2.5155\n",
      ">>> Epoch 324 test loss: 9.0659\n",
      ">>> Epoch 325 train loss: 2.5155\n",
      ">>> Epoch 325 test loss: 9.0714\n",
      ">>> Epoch 326 train loss: 2.5155\n",
      ">>> Epoch 326 test loss: 9.0769\n",
      ">>> Epoch 327 train loss: 2.5155\n",
      ">>> Epoch 327 test loss: 9.0824\n",
      ">>> Epoch 328 train loss: 2.5154\n",
      ">>> Epoch 328 test loss: 9.0878\n",
      ">>> Epoch 329 train loss: 2.5154\n",
      ">>> Epoch 329 test loss: 9.0933\n",
      ">>> Epoch 330 train loss: 2.5154\n",
      ">>> Epoch 330 test loss: 9.0987\n",
      ">>> Epoch 331 train loss: 2.5154\n",
      ">>> Epoch 331 test loss: 9.1042\n",
      ">>> Epoch 332 train loss: 2.5154\n",
      ">>> Epoch 332 test loss: 9.1096\n",
      ">>> Epoch 333 train loss: 2.5154\n",
      ">>> Epoch 333 test loss: 9.1149\n",
      ">>> Epoch 334 train loss: 2.5154\n",
      ">>> Epoch 334 test loss: 9.1203\n",
      ">>> Epoch 335 train loss: 2.5153\n",
      ">>> Epoch 335 test loss: 9.1257\n",
      ">>> Epoch 336 train loss: 2.5153\n",
      ">>> Epoch 336 test loss: 9.1310\n",
      ">>> Epoch 337 train loss: 2.5153\n",
      ">>> Epoch 337 test loss: 9.1363\n",
      ">>> Epoch 338 train loss: 2.5153\n",
      ">>> Epoch 338 test loss: 9.1417\n",
      ">>> Epoch 339 train loss: 2.5153\n",
      ">>> Epoch 339 test loss: 9.1470\n",
      ">>> Epoch 340 train loss: 2.5153\n",
      ">>> Epoch 340 test loss: 9.1522\n",
      ">>> Epoch 341 train loss: 2.5153\n",
      ">>> Epoch 341 test loss: 9.1575\n",
      ">>> Epoch 342 train loss: 2.5152\n",
      ">>> Epoch 342 test loss: 9.1628\n",
      ">>> Epoch 343 train loss: 2.5152\n",
      ">>> Epoch 343 test loss: 9.1680\n",
      ">>> Epoch 344 train loss: 2.5152\n",
      ">>> Epoch 344 test loss: 9.1732\n",
      ">>> Epoch 345 train loss: 2.5152\n",
      ">>> Epoch 345 test loss: 9.1784\n",
      ">>> Epoch 346 train loss: 2.5152\n",
      ">>> Epoch 346 test loss: 9.1836\n",
      ">>> Epoch 347 train loss: 2.5152\n",
      ">>> Epoch 347 test loss: 9.1888\n",
      ">>> Epoch 348 train loss: 2.5152\n",
      ">>> Epoch 348 test loss: 9.1940\n",
      ">>> Epoch 349 train loss: 2.5151\n",
      ">>> Epoch 349 test loss: 9.1991\n",
      ">>> Epoch 350 train loss: 2.5151\n",
      ">>> Epoch 350 test loss: 9.2043\n",
      ">>> Epoch 351 train loss: 2.5151\n",
      ">>> Epoch 351 test loss: 9.2094\n",
      ">>> Epoch 352 train loss: 2.5151\n",
      ">>> Epoch 352 test loss: 9.2145\n",
      ">>> Epoch 353 train loss: 2.5151\n",
      ">>> Epoch 353 test loss: 9.2196\n",
      ">>> Epoch 354 train loss: 2.5151\n",
      ">>> Epoch 354 test loss: 9.2247\n",
      ">>> Epoch 355 train loss: 2.5151\n",
      ">>> Epoch 355 test loss: 9.2298\n",
      ">>> Epoch 356 train loss: 2.5151\n",
      ">>> Epoch 356 test loss: 9.2348\n",
      ">>> Epoch 357 train loss: 2.5150\n",
      ">>> Epoch 357 test loss: 9.2399\n",
      ">>> Epoch 358 train loss: 2.5150\n",
      ">>> Epoch 358 test loss: 9.2449\n",
      ">>> Epoch 359 train loss: 2.5150\n",
      ">>> Epoch 359 test loss: 9.2499\n",
      ">>> Epoch 360 train loss: 2.5150\n",
      ">>> Epoch 360 test loss: 9.2549\n",
      ">>> Epoch 361 train loss: 2.5150\n",
      ">>> Epoch 361 test loss: 9.2599\n",
      ">>> Epoch 362 train loss: 2.5150\n",
      ">>> Epoch 362 test loss: 9.2649\n",
      ">>> Epoch 363 train loss: 2.5150\n",
      ">>> Epoch 363 test loss: 9.2698\n",
      ">>> Epoch 364 train loss: 2.5150\n",
      ">>> Epoch 364 test loss: 9.2748\n",
      ">>> Epoch 365 train loss: 2.5150\n",
      ">>> Epoch 365 test loss: 9.2797\n",
      ">>> Epoch 366 train loss: 2.5149\n",
      ">>> Epoch 366 test loss: 9.2847\n",
      ">>> Epoch 367 train loss: 2.5149\n",
      ">>> Epoch 367 test loss: 9.2896\n",
      ">>> Epoch 368 train loss: 2.5149\n",
      ">>> Epoch 368 test loss: 9.2945\n",
      ">>> Epoch 369 train loss: 2.5149\n",
      ">>> Epoch 369 test loss: 9.2994\n",
      ">>> Epoch 370 train loss: 2.5149\n",
      ">>> Epoch 370 test loss: 9.3042\n",
      ">>> Epoch 371 train loss: 2.5149\n",
      ">>> Epoch 371 test loss: 9.3091\n",
      ">>> Epoch 372 train loss: 2.5149\n",
      ">>> Epoch 372 test loss: 9.3140\n",
      ">>> Epoch 373 train loss: 2.5149\n",
      ">>> Epoch 373 test loss: 9.3188\n",
      ">>> Epoch 374 train loss: 2.5149\n",
      ">>> Epoch 374 test loss: 9.3236\n",
      ">>> Epoch 375 train loss: 2.5148\n",
      ">>> Epoch 375 test loss: 9.3284\n",
      ">>> Epoch 376 train loss: 2.5148\n",
      ">>> Epoch 376 test loss: 9.3332\n",
      ">>> Epoch 377 train loss: 2.5148\n",
      ">>> Epoch 377 test loss: 9.3380\n",
      ">>> Epoch 378 train loss: 2.5148\n",
      ">>> Epoch 378 test loss: 9.3428\n",
      ">>> Epoch 379 train loss: 2.5148\n",
      ">>> Epoch 379 test loss: 9.3476\n",
      ">>> Epoch 380 train loss: 2.5148\n",
      ">>> Epoch 380 test loss: 9.3523\n",
      ">>> Epoch 381 train loss: 2.5148\n",
      ">>> Epoch 381 test loss: 9.3571\n",
      ">>> Epoch 382 train loss: 2.5148\n",
      ">>> Epoch 382 test loss: 9.3618\n",
      ">>> Epoch 383 train loss: 2.5148\n",
      ">>> Epoch 383 test loss: 9.3665\n",
      ">>> Epoch 384 train loss: 2.5147\n",
      ">>> Epoch 384 test loss: 9.3712\n",
      ">>> Epoch 385 train loss: 2.5147\n",
      ">>> Epoch 385 test loss: 9.3759\n",
      ">>> Epoch 386 train loss: 2.5147\n",
      ">>> Epoch 386 test loss: 9.3806\n",
      ">>> Epoch 387 train loss: 2.5147\n",
      ">>> Epoch 387 test loss: 9.3853\n",
      ">>> Epoch 388 train loss: 2.5147\n",
      ">>> Epoch 388 test loss: 9.3899\n",
      ">>> Epoch 389 train loss: 2.5147\n",
      ">>> Epoch 389 test loss: 9.3946\n",
      ">>> Epoch 390 train loss: 2.5147\n",
      ">>> Epoch 390 test loss: 9.3992\n",
      ">>> Epoch 391 train loss: 2.5147\n",
      ">>> Epoch 391 test loss: 9.4039\n",
      ">>> Epoch 392 train loss: 2.5147\n",
      ">>> Epoch 392 test loss: 9.4085\n",
      ">>> Epoch 393 train loss: 2.5147\n",
      ">>> Epoch 393 test loss: 9.4131\n",
      ">>> Epoch 394 train loss: 2.5147\n",
      ">>> Epoch 394 test loss: 9.4177\n",
      ">>> Epoch 395 train loss: 2.5146\n",
      ">>> Epoch 395 test loss: 9.4223\n",
      ">>> Epoch 396 train loss: 2.5146\n",
      ">>> Epoch 396 test loss: 9.4268\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# set n_epochs to 5000\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mvt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/vision-transformer/vision_transformer_v1.py:209\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, (inputs, labels) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_dataloader):\n\u001b[1;32m    208\u001b[0m     inputs, labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m--> 209\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    210\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[1;32m    211\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/vit/lib/python3.10/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/vit/lib/python3.10/site-packages/torch/utils/data/dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/vit/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:50\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[0;32m---> 50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/vit/lib/python3.10/site-packages/torch/utils/data/dataset.py:418\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitems__\u001b[39m(\u001b[38;5;28mself\u001b[39m, indices: List[\u001b[38;5;28mint\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[_T_co]:\n\u001b[1;32m    415\u001b[0m     \u001b[38;5;66;03m# add batched sampling support when parent dataset supports it.\u001b[39;00m\n\u001b[1;32m    416\u001b[0m     \u001b[38;5;66;03m# see torch.utils.data._utils.fetch._MapDatasetFetcher\u001b[39;00m\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)):\n\u001b[0;32m--> 418\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    419\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    420\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx]] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/vit/lib/python3.10/site-packages/torch/utils/data/dataset.py:420\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx]] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/vit/lib/python3.10/site-packages/torch/utils/data/dataset.py:420\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/vit/lib/python3.10/site-packages/torchvision/datasets/oxford_iiit_pet.py:97\u001b[0m, in \u001b[0;36mOxfordIIITPet.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Any, Any]:\n\u001b[0;32m---> 97\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_images\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRGB\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m     target: Any \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m target_type \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_target_types:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/vit/lib/python3.10/site-packages/PIL/Image.py:984\u001b[0m, in \u001b[0;36mImage.convert\u001b[0;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[1;32m    981\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBGR;15\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBGR;16\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBGR;24\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    982\u001b[0m     deprecate(mode, \u001b[38;5;241m12\u001b[39m)\n\u001b[0;32m--> 984\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    986\u001b[0m has_transparency \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransparency\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\n\u001b[1;32m    987\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mP\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    988\u001b[0m     \u001b[38;5;66;03m# determine default mode\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/vit/lib/python3.10/site-packages/PIL/ImageFile.py:300\u001b[0m, in \u001b[0;36mImageFile.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(msg)\n\u001b[1;32m    299\u001b[0m b \u001b[38;5;241m=\u001b[39m b \u001b[38;5;241m+\u001b[39m s\n\u001b[0;32m--> 300\u001b[0m n, err_code \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# set n_epochs to 5000\n",
    "vt.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c564b6b3-f7b7-4912-8e5b-b5678161a89e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Epoch 0 train loss: 3.8278\n",
      ">>> Epoch 0 test loss: 3.5687\n",
      ">>> Epoch 1 train loss: 3.7472\n",
      ">>> Epoch 1 test loss: 3.5501\n",
      ">>> Epoch 2 train loss: 3.6688\n",
      ">>> Epoch 2 test loss: 3.5346\n",
      ">>> Epoch 3 train loss: 3.5929\n",
      ">>> Epoch 3 test loss: 3.5227\n",
      ">>> Epoch 4 train loss: 3.5196\n",
      ">>> Epoch 4 test loss: 3.5146\n",
      ">>> Epoch 5 train loss: 3.4491\n",
      ">>> Epoch 5 test loss: 3.5102\n",
      ">>> Epoch 6 train loss: 3.3815\n",
      ">>> Epoch 6 test loss: 3.5097\n",
      ">>> Epoch 7 train loss: 3.3170\n",
      ">>> Epoch 7 test loss: 3.5131\n",
      ">>> Epoch 8 train loss: 3.2559\n",
      ">>> Epoch 8 test loss: 3.5203\n",
      ">>> Epoch 9 train loss: 3.1982\n",
      ">>> Epoch 9 test loss: 3.5312\n",
      ">>> Epoch 10 train loss: 3.1441\n",
      ">>> Epoch 10 test loss: 3.5452\n",
      ">>> Epoch 11 train loss: 3.0937\n",
      ">>> Epoch 11 test loss: 3.5619\n",
      ">>> Epoch 12 train loss: 3.0471\n",
      ">>> Epoch 12 test loss: 3.5806\n",
      ">>> Epoch 13 train loss: 3.0040\n",
      ">>> Epoch 13 test loss: 3.6002\n",
      ">>> Epoch 14 train loss: 2.9643\n",
      ">>> Epoch 14 test loss: 3.6201\n",
      ">>> Epoch 15 train loss: 2.9276\n",
      ">>> Epoch 15 test loss: 3.6395\n",
      ">>> Epoch 16 train loss: 2.8937\n",
      ">>> Epoch 16 test loss: 3.6581\n",
      ">>> Epoch 17 train loss: 2.8621\n",
      ">>> Epoch 17 test loss: 3.6755\n",
      ">>> Epoch 18 train loss: 2.8326\n",
      ">>> Epoch 18 test loss: 3.6915\n",
      ">>> Epoch 19 train loss: 2.8050\n",
      ">>> Epoch 19 test loss: 3.7063\n",
      ">>> Epoch 20 train loss: 2.7790\n",
      ">>> Epoch 20 test loss: 3.7198\n",
      ">>> Epoch 21 train loss: 2.7546\n",
      ">>> Epoch 21 test loss: 3.7321\n",
      ">>> Epoch 22 train loss: 2.7316\n",
      ">>> Epoch 22 test loss: 3.7435\n",
      ">>> Epoch 23 train loss: 2.7100\n",
      ">>> Epoch 23 test loss: 3.7543\n",
      ">>> Epoch 24 train loss: 2.6898\n",
      ">>> Epoch 24 test loss: 3.7646\n",
      ">>> Epoch 25 train loss: 2.6708\n",
      ">>> Epoch 25 test loss: 3.7747\n",
      ">>> Epoch 26 train loss: 2.6531\n",
      ">>> Epoch 26 test loss: 3.7848\n",
      ">>> Epoch 27 train loss: 2.6366\n",
      ">>> Epoch 27 test loss: 3.7951\n",
      ">>> Epoch 28 train loss: 2.6211\n",
      ">>> Epoch 28 test loss: 3.8058\n",
      ">>> Epoch 29 train loss: 2.6065\n",
      ">>> Epoch 29 test loss: 3.8170\n",
      ">>> Epoch 30 train loss: 2.5928\n",
      ">>> Epoch 30 test loss: 3.8288\n",
      ">>> Epoch 31 train loss: 2.5798\n",
      ">>> Epoch 31 test loss: 3.8412\n",
      ">>> Epoch 32 train loss: 2.5675\n",
      ">>> Epoch 32 test loss: 3.8545\n",
      ">>> Epoch 33 train loss: 2.5558\n",
      ">>> Epoch 33 test loss: 3.8686\n",
      ">>> Epoch 34 train loss: 2.5448\n",
      ">>> Epoch 34 test loss: 3.8835\n",
      ">>> Epoch 35 train loss: 2.5343\n",
      ">>> Epoch 35 test loss: 3.8991\n",
      ">>> Epoch 36 train loss: 2.5243\n",
      ">>> Epoch 36 test loss: 3.9155\n",
      ">>> Epoch 37 train loss: 2.5150\n",
      ">>> Epoch 37 test loss: 3.9325\n",
      ">>> Epoch 38 train loss: 2.5061\n",
      ">>> Epoch 38 test loss: 3.9500\n",
      ">>> Epoch 39 train loss: 2.4979\n",
      ">>> Epoch 39 test loss: 3.9679\n",
      ">>> Epoch 40 train loss: 2.4901\n",
      ">>> Epoch 40 test loss: 3.9859\n",
      ">>> Epoch 41 train loss: 2.4827\n",
      ">>> Epoch 41 test loss: 4.0039\n",
      ">>> Epoch 42 train loss: 2.4757\n",
      ">>> Epoch 42 test loss: 4.0219\n",
      ">>> Epoch 43 train loss: 2.4691\n",
      ">>> Epoch 43 test loss: 4.0397\n",
      ">>> Epoch 44 train loss: 2.4629\n",
      ">>> Epoch 44 test loss: 4.0572\n",
      ">>> Epoch 45 train loss: 2.4569\n",
      ">>> Epoch 45 test loss: 4.0745\n",
      ">>> Epoch 46 train loss: 2.4511\n",
      ">>> Epoch 46 test loss: 4.0915\n",
      ">>> Epoch 47 train loss: 2.4457\n",
      ">>> Epoch 47 test loss: 4.1083\n",
      ">>> Epoch 48 train loss: 2.4404\n",
      ">>> Epoch 48 test loss: 4.1249\n",
      ">>> Epoch 49 train loss: 2.4354\n",
      ">>> Epoch 49 test loss: 4.1413\n",
      ">>> Epoch 50 train loss: 2.4307\n",
      ">>> Epoch 50 test loss: 4.1575\n",
      ">>> Epoch 51 train loss: 2.4261\n",
      ">>> Epoch 51 test loss: 4.1737\n",
      ">>> Epoch 52 train loss: 2.4218\n",
      ">>> Epoch 52 test loss: 4.1898\n",
      ">>> Epoch 53 train loss: 2.4177\n",
      ">>> Epoch 53 test loss: 4.2060\n",
      ">>> Epoch 54 train loss: 2.4138\n",
      ">>> Epoch 54 test loss: 4.2221\n",
      ">>> Epoch 55 train loss: 2.4101\n",
      ">>> Epoch 55 test loss: 4.2384\n",
      ">>> Epoch 56 train loss: 2.4065\n",
      ">>> Epoch 56 test loss: 4.2546\n",
      ">>> Epoch 57 train loss: 2.4031\n",
      ">>> Epoch 57 test loss: 4.2709\n",
      ">>> Epoch 58 train loss: 2.3999\n",
      ">>> Epoch 58 test loss: 4.2873\n",
      ">>> Epoch 59 train loss: 2.3969\n",
      ">>> Epoch 59 test loss: 4.3037\n",
      ">>> Epoch 60 train loss: 2.3939\n",
      ">>> Epoch 60 test loss: 4.3200\n",
      ">>> Epoch 61 train loss: 2.3911\n",
      ">>> Epoch 61 test loss: 4.3362\n",
      ">>> Epoch 62 train loss: 2.3885\n",
      ">>> Epoch 62 test loss: 4.3523\n",
      ">>> Epoch 63 train loss: 2.3859\n",
      ">>> Epoch 63 test loss: 4.3681\n",
      ">>> Epoch 64 train loss: 2.3835\n",
      ">>> Epoch 64 test loss: 4.3837\n",
      ">>> Epoch 65 train loss: 2.3812\n",
      ">>> Epoch 65 test loss: 4.3990\n",
      ">>> Epoch 66 train loss: 2.3789\n",
      ">>> Epoch 66 test loss: 4.4139\n",
      ">>> Epoch 67 train loss: 2.3768\n",
      ">>> Epoch 67 test loss: 4.4286\n",
      ">>> Epoch 68 train loss: 2.3748\n",
      ">>> Epoch 68 test loss: 4.4428\n",
      ">>> Epoch 69 train loss: 2.3728\n",
      ">>> Epoch 69 test loss: 4.4568\n",
      ">>> Epoch 70 train loss: 2.3710\n",
      ">>> Epoch 70 test loss: 4.4704\n",
      ">>> Epoch 71 train loss: 2.3692\n",
      ">>> Epoch 71 test loss: 4.4837\n",
      ">>> Epoch 72 train loss: 2.3675\n",
      ">>> Epoch 72 test loss: 4.4968\n",
      ">>> Epoch 73 train loss: 2.3659\n",
      ">>> Epoch 73 test loss: 4.5096\n",
      ">>> Epoch 74 train loss: 2.3643\n",
      ">>> Epoch 74 test loss: 4.5223\n",
      ">>> Epoch 75 train loss: 2.3628\n",
      ">>> Epoch 75 test loss: 4.5349\n",
      ">>> Epoch 76 train loss: 2.3613\n",
      ">>> Epoch 76 test loss: 4.5473\n",
      ">>> Epoch 77 train loss: 2.3600\n",
      ">>> Epoch 77 test loss: 4.5597\n",
      ">>> Epoch 78 train loss: 2.3586\n",
      ">>> Epoch 78 test loss: 4.5720\n",
      ">>> Epoch 79 train loss: 2.3573\n",
      ">>> Epoch 79 test loss: 4.5843\n",
      ">>> Epoch 80 train loss: 2.3561\n",
      ">>> Epoch 80 test loss: 4.5965\n",
      ">>> Epoch 81 train loss: 2.3549\n",
      ">>> Epoch 81 test loss: 4.6087\n",
      ">>> Epoch 82 train loss: 2.3538\n",
      ">>> Epoch 82 test loss: 4.6207\n",
      ">>> Epoch 83 train loss: 2.3527\n",
      ">>> Epoch 83 test loss: 4.6327\n",
      ">>> Epoch 84 train loss: 2.3516\n",
      ">>> Epoch 84 test loss: 4.6445\n",
      ">>> Epoch 85 train loss: 2.3506\n",
      ">>> Epoch 85 test loss: 4.6561\n",
      ">>> Epoch 86 train loss: 2.3496\n",
      ">>> Epoch 86 test loss: 4.6675\n",
      ">>> Epoch 87 train loss: 2.3486\n",
      ">>> Epoch 87 test loss: 4.6787\n",
      ">>> Epoch 88 train loss: 2.3477\n",
      ">>> Epoch 88 test loss: 4.6896\n",
      ">>> Epoch 89 train loss: 2.3468\n",
      ">>> Epoch 89 test loss: 4.7003\n",
      ">>> Epoch 90 train loss: 2.3460\n",
      ">>> Epoch 90 test loss: 4.7108\n",
      ">>> Epoch 91 train loss: 2.3451\n",
      ">>> Epoch 91 test loss: 4.7210\n",
      ">>> Epoch 92 train loss: 2.3443\n",
      ">>> Epoch 92 test loss: 4.7310\n",
      ">>> Epoch 93 train loss: 2.3436\n",
      ">>> Epoch 93 test loss: 4.7408\n",
      ">>> Epoch 94 train loss: 2.3428\n",
      ">>> Epoch 94 test loss: 4.7504\n",
      ">>> Epoch 95 train loss: 2.3421\n",
      ">>> Epoch 95 test loss: 4.7600\n",
      ">>> Epoch 96 train loss: 2.3414\n",
      ">>> Epoch 96 test loss: 4.7694\n",
      ">>> Epoch 97 train loss: 2.3407\n",
      ">>> Epoch 97 test loss: 4.7788\n",
      ">>> Epoch 98 train loss: 2.3400\n",
      ">>> Epoch 98 test loss: 4.7881\n",
      ">>> Epoch 99 train loss: 2.3394\n",
      ">>> Epoch 99 test loss: 4.7974\n",
      ">>> Epoch 100 train loss: 2.3388\n",
      ">>> Epoch 100 test loss: 4.8067\n",
      ">>> Epoch 101 train loss: 2.3382\n",
      ">>> Epoch 101 test loss: 4.8160\n",
      ">>> Epoch 102 train loss: 2.3376\n",
      ">>> Epoch 102 test loss: 4.8252\n",
      ">>> Epoch 103 train loss: 2.3370\n",
      ">>> Epoch 103 test loss: 4.8344\n",
      ">>> Epoch 104 train loss: 2.3365\n",
      ">>> Epoch 104 test loss: 4.8435\n",
      ">>> Epoch 105 train loss: 2.3359\n",
      ">>> Epoch 105 test loss: 4.8525\n",
      ">>> Epoch 106 train loss: 2.3354\n",
      ">>> Epoch 106 test loss: 4.8613\n",
      ">>> Epoch 107 train loss: 2.3349\n",
      ">>> Epoch 107 test loss: 4.8701\n",
      ">>> Epoch 108 train loss: 2.3344\n",
      ">>> Epoch 108 test loss: 4.8787\n",
      ">>> Epoch 109 train loss: 2.3339\n",
      ">>> Epoch 109 test loss: 4.8872\n",
      ">>> Epoch 110 train loss: 2.3334\n",
      ">>> Epoch 110 test loss: 4.8955\n",
      ">>> Epoch 111 train loss: 2.3330\n",
      ">>> Epoch 111 test loss: 4.9038\n",
      ">>> Epoch 112 train loss: 2.3325\n",
      ">>> Epoch 112 test loss: 4.9120\n",
      ">>> Epoch 113 train loss: 2.3321\n",
      ">>> Epoch 113 test loss: 4.9200\n",
      ">>> Epoch 114 train loss: 2.3317\n",
      ">>> Epoch 114 test loss: 4.9280\n",
      ">>> Epoch 115 train loss: 2.3313\n",
      ">>> Epoch 115 test loss: 4.9360\n",
      ">>> Epoch 116 train loss: 2.3309\n",
      ">>> Epoch 116 test loss: 4.9440\n",
      ">>> Epoch 117 train loss: 2.3305\n",
      ">>> Epoch 117 test loss: 4.9519\n",
      ">>> Epoch 118 train loss: 2.3301\n",
      ">>> Epoch 118 test loss: 4.9597\n",
      ">>> Epoch 119 train loss: 2.3297\n",
      ">>> Epoch 119 test loss: 4.9675\n",
      ">>> Epoch 120 train loss: 2.3294\n",
      ">>> Epoch 120 test loss: 4.9753\n",
      ">>> Epoch 121 train loss: 2.3290\n",
      ">>> Epoch 121 test loss: 4.9831\n",
      ">>> Epoch 122 train loss: 2.3287\n",
      ">>> Epoch 122 test loss: 4.9907\n",
      ">>> Epoch 123 train loss: 2.3283\n",
      ">>> Epoch 123 test loss: 4.9983\n",
      ">>> Epoch 124 train loss: 2.3280\n",
      ">>> Epoch 124 test loss: 5.0058\n",
      ">>> Epoch 125 train loss: 2.3277\n",
      ">>> Epoch 125 test loss: 5.0132\n",
      ">>> Epoch 126 train loss: 2.3274\n",
      ">>> Epoch 126 test loss: 5.0205\n",
      ">>> Epoch 127 train loss: 2.3271\n",
      ">>> Epoch 127 test loss: 5.0277\n",
      ">>> Epoch 128 train loss: 2.3268\n",
      ">>> Epoch 128 test loss: 5.0349\n",
      ">>> Epoch 129 train loss: 2.3265\n",
      ">>> Epoch 129 test loss: 5.0420\n",
      ">>> Epoch 130 train loss: 2.3262\n",
      ">>> Epoch 130 test loss: 5.0491\n",
      ">>> Epoch 131 train loss: 2.3259\n",
      ">>> Epoch 131 test loss: 5.0561\n",
      ">>> Epoch 132 train loss: 2.3256\n",
      ">>> Epoch 132 test loss: 5.0631\n",
      ">>> Epoch 133 train loss: 2.3254\n",
      ">>> Epoch 133 test loss: 5.0701\n",
      ">>> Epoch 134 train loss: 2.3251\n",
      ">>> Epoch 134 test loss: 5.0770\n",
      ">>> Epoch 135 train loss: 2.3248\n",
      ">>> Epoch 135 test loss: 5.0839\n",
      ">>> Epoch 136 train loss: 2.3246\n",
      ">>> Epoch 136 test loss: 5.0908\n",
      ">>> Epoch 137 train loss: 2.3243\n",
      ">>> Epoch 137 test loss: 5.0976\n",
      ">>> Epoch 138 train loss: 2.3241\n",
      ">>> Epoch 138 test loss: 5.1044\n",
      ">>> Epoch 139 train loss: 2.3239\n",
      ">>> Epoch 139 test loss: 5.1111\n",
      ">>> Epoch 140 train loss: 2.3236\n",
      ">>> Epoch 140 test loss: 5.1177\n",
      ">>> Epoch 141 train loss: 2.3234\n",
      ">>> Epoch 141 test loss: 5.1243\n",
      ">>> Epoch 142 train loss: 2.3232\n",
      ">>> Epoch 142 test loss: 5.1308\n",
      ">>> Epoch 143 train loss: 2.3230\n",
      ">>> Epoch 143 test loss: 5.1373\n",
      ">>> Epoch 144 train loss: 2.3228\n",
      ">>> Epoch 144 test loss: 5.1437\n",
      ">>> Epoch 145 train loss: 2.3225\n",
      ">>> Epoch 145 test loss: 5.1501\n",
      ">>> Epoch 146 train loss: 2.3223\n",
      ">>> Epoch 146 test loss: 5.1564\n",
      ">>> Epoch 147 train loss: 2.3221\n",
      ">>> Epoch 147 test loss: 5.1627\n",
      ">>> Epoch 148 train loss: 2.3219\n",
      ">>> Epoch 148 test loss: 5.1690\n",
      ">>> Epoch 149 train loss: 2.3218\n",
      ">>> Epoch 149 test loss: 5.1752\n",
      ">>> Epoch 150 train loss: 2.3216\n",
      ">>> Epoch 150 test loss: 5.1815\n",
      ">>> Epoch 151 train loss: 2.3214\n",
      ">>> Epoch 151 test loss: 5.1877\n",
      ">>> Epoch 152 train loss: 2.3212\n",
      ">>> Epoch 152 test loss: 5.1938\n",
      ">>> Epoch 153 train loss: 2.3210\n",
      ">>> Epoch 153 test loss: 5.1999\n",
      ">>> Epoch 154 train loss: 2.3208\n",
      ">>> Epoch 154 test loss: 5.2060\n",
      ">>> Epoch 155 train loss: 2.3207\n",
      ">>> Epoch 155 test loss: 5.2120\n",
      ">>> Epoch 156 train loss: 2.3205\n",
      ">>> Epoch 156 test loss: 5.2180\n",
      ">>> Epoch 157 train loss: 2.3203\n",
      ">>> Epoch 157 test loss: 5.2239\n",
      ">>> Epoch 158 train loss: 2.3202\n",
      ">>> Epoch 158 test loss: 5.2298\n",
      ">>> Epoch 159 train loss: 2.3200\n",
      ">>> Epoch 159 test loss: 5.2357\n",
      ">>> Epoch 160 train loss: 2.3198\n",
      ">>> Epoch 160 test loss: 5.2415\n",
      ">>> Epoch 161 train loss: 2.3197\n",
      ">>> Epoch 161 test loss: 5.2473\n",
      ">>> Epoch 162 train loss: 2.3195\n",
      ">>> Epoch 162 test loss: 5.2530\n",
      ">>> Epoch 163 train loss: 2.3194\n",
      ">>> Epoch 163 test loss: 5.2587\n",
      ">>> Epoch 164 train loss: 2.3192\n",
      ">>> Epoch 164 test loss: 5.2644\n",
      ">>> Epoch 165 train loss: 2.3191\n",
      ">>> Epoch 165 test loss: 5.2701\n",
      ">>> Epoch 166 train loss: 2.3190\n",
      ">>> Epoch 166 test loss: 5.2757\n",
      ">>> Epoch 167 train loss: 2.3188\n",
      ">>> Epoch 167 test loss: 5.2814\n",
      ">>> Epoch 168 train loss: 2.3187\n",
      ">>> Epoch 168 test loss: 5.2869\n",
      ">>> Epoch 169 train loss: 2.3185\n",
      ">>> Epoch 169 test loss: 5.2925\n",
      ">>> Epoch 170 train loss: 2.3184\n",
      ">>> Epoch 170 test loss: 5.2980\n",
      ">>> Epoch 171 train loss: 2.3183\n",
      ">>> Epoch 171 test loss: 5.3035\n",
      ">>> Epoch 172 train loss: 2.3182\n",
      ">>> Epoch 172 test loss: 5.3089\n",
      ">>> Epoch 173 train loss: 2.3180\n",
      ">>> Epoch 173 test loss: 5.3143\n",
      ">>> Epoch 174 train loss: 2.3179\n",
      ">>> Epoch 174 test loss: 5.3197\n",
      ">>> Epoch 175 train loss: 2.3178\n",
      ">>> Epoch 175 test loss: 5.3251\n",
      ">>> Epoch 176 train loss: 2.3177\n",
      ">>> Epoch 176 test loss: 5.3304\n",
      ">>> Epoch 177 train loss: 2.3175\n",
      ">>> Epoch 177 test loss: 5.3357\n",
      ">>> Epoch 178 train loss: 2.3174\n",
      ">>> Epoch 178 test loss: 5.3410\n",
      ">>> Epoch 179 train loss: 2.3173\n",
      ">>> Epoch 179 test loss: 5.3462\n",
      ">>> Epoch 180 train loss: 2.3172\n",
      ">>> Epoch 180 test loss: 5.3514\n",
      ">>> Epoch 181 train loss: 2.3171\n",
      ">>> Epoch 181 test loss: 5.3566\n",
      ">>> Epoch 182 train loss: 2.3170\n",
      ">>> Epoch 182 test loss: 5.3618\n",
      ">>> Epoch 183 train loss: 2.3169\n",
      ">>> Epoch 183 test loss: 5.3669\n",
      ">>> Epoch 184 train loss: 2.3168\n",
      ">>> Epoch 184 test loss: 5.3720\n",
      ">>> Epoch 185 train loss: 2.3167\n",
      ">>> Epoch 185 test loss: 5.3771\n",
      ">>> Epoch 186 train loss: 2.3166\n",
      ">>> Epoch 186 test loss: 5.3822\n",
      ">>> Epoch 187 train loss: 2.3165\n",
      ">>> Epoch 187 test loss: 5.3872\n",
      ">>> Epoch 188 train loss: 2.3164\n",
      ">>> Epoch 188 test loss: 5.3922\n",
      ">>> Epoch 189 train loss: 2.3163\n",
      ">>> Epoch 189 test loss: 5.3972\n",
      ">>> Epoch 190 train loss: 2.3162\n",
      ">>> Epoch 190 test loss: 5.4021\n",
      ">>> Epoch 191 train loss: 2.3161\n",
      ">>> Epoch 191 test loss: 5.4071\n",
      ">>> Epoch 192 train loss: 2.3160\n",
      ">>> Epoch 192 test loss: 5.4120\n",
      ">>> Epoch 193 train loss: 2.3159\n",
      ">>> Epoch 193 test loss: 5.4169\n",
      ">>> Epoch 194 train loss: 2.3158\n",
      ">>> Epoch 194 test loss: 5.4217\n",
      ">>> Epoch 195 train loss: 2.3157\n",
      ">>> Epoch 195 test loss: 5.4266\n",
      ">>> Epoch 196 train loss: 2.3156\n",
      ">>> Epoch 196 test loss: 5.4314\n",
      ">>> Epoch 197 train loss: 2.3155\n",
      ">>> Epoch 197 test loss: 5.4362\n",
      ">>> Epoch 198 train loss: 2.3155\n",
      ">>> Epoch 198 test loss: 5.4410\n",
      ">>> Epoch 199 train loss: 2.3154\n",
      ">>> Epoch 199 test loss: 5.4457\n",
      ">>> Epoch 200 train loss: 2.3153\n",
      ">>> Epoch 200 test loss: 5.4504\n",
      ">>> Epoch 201 train loss: 2.3152\n",
      ">>> Epoch 201 test loss: 5.4551\n",
      ">>> Epoch 202 train loss: 2.3151\n",
      ">>> Epoch 202 test loss: 5.4598\n",
      ">>> Epoch 203 train loss: 2.3150\n",
      ">>> Epoch 203 test loss: 5.4645\n",
      ">>> Epoch 204 train loss: 2.3150\n",
      ">>> Epoch 204 test loss: 5.4691\n",
      ">>> Epoch 205 train loss: 2.3149\n",
      ">>> Epoch 205 test loss: 5.4737\n",
      ">>> Epoch 206 train loss: 2.3148\n",
      ">>> Epoch 206 test loss: 5.4783\n",
      ">>> Epoch 207 train loss: 2.3147\n",
      ">>> Epoch 207 test loss: 5.4829\n",
      ">>> Epoch 208 train loss: 2.3147\n",
      ">>> Epoch 208 test loss: 5.4874\n",
      ">>> Epoch 209 train loss: 2.3146\n",
      ">>> Epoch 209 test loss: 5.4920\n",
      ">>> Epoch 210 train loss: 2.3145\n",
      ">>> Epoch 210 test loss: 5.4965\n",
      ">>> Epoch 211 train loss: 2.3144\n",
      ">>> Epoch 211 test loss: 5.5010\n",
      ">>> Epoch 212 train loss: 2.3144\n",
      ">>> Epoch 212 test loss: 5.5054\n",
      ">>> Epoch 213 train loss: 2.3143\n",
      ">>> Epoch 213 test loss: 5.5099\n",
      ">>> Epoch 214 train loss: 2.3142\n",
      ">>> Epoch 214 test loss: 5.5143\n",
      ">>> Epoch 215 train loss: 2.3142\n",
      ">>> Epoch 215 test loss: 5.5187\n",
      ">>> Epoch 216 train loss: 2.3141\n",
      ">>> Epoch 216 test loss: 5.5231\n",
      ">>> Epoch 217 train loss: 2.3140\n",
      ">>> Epoch 217 test loss: 5.5275\n",
      ">>> Epoch 218 train loss: 2.3140\n",
      ">>> Epoch 218 test loss: 5.5319\n",
      ">>> Epoch 219 train loss: 2.3139\n",
      ">>> Epoch 219 test loss: 5.5362\n",
      ">>> Epoch 220 train loss: 2.3138\n",
      ">>> Epoch 220 test loss: 5.5405\n",
      ">>> Epoch 221 train loss: 2.3138\n",
      ">>> Epoch 221 test loss: 5.5448\n",
      ">>> Epoch 222 train loss: 2.3137\n",
      ">>> Epoch 222 test loss: 5.5491\n",
      ">>> Epoch 223 train loss: 2.3137\n",
      ">>> Epoch 223 test loss: 5.5534\n",
      ">>> Epoch 224 train loss: 2.3136\n",
      ">>> Epoch 224 test loss: 5.5576\n",
      ">>> Epoch 225 train loss: 2.3135\n",
      ">>> Epoch 225 test loss: 5.5618\n",
      ">>> Epoch 226 train loss: 2.3135\n",
      ">>> Epoch 226 test loss: 5.5660\n",
      ">>> Epoch 227 train loss: 2.3134\n",
      ">>> Epoch 227 test loss: 5.5702\n",
      ">>> Epoch 228 train loss: 2.3134\n",
      ">>> Epoch 228 test loss: 5.5744\n",
      ">>> Epoch 229 train loss: 2.3133\n",
      ">>> Epoch 229 test loss: 5.5786\n",
      ">>> Epoch 230 train loss: 2.3133\n",
      ">>> Epoch 230 test loss: 5.5827\n",
      ">>> Epoch 231 train loss: 2.3132\n",
      ">>> Epoch 231 test loss: 5.5868\n",
      ">>> Epoch 232 train loss: 2.3131\n",
      ">>> Epoch 232 test loss: 5.5909\n",
      ">>> Epoch 233 train loss: 2.3131\n",
      ">>> Epoch 233 test loss: 5.5950\n",
      ">>> Epoch 234 train loss: 2.3130\n",
      ">>> Epoch 234 test loss: 5.5991\n",
      ">>> Epoch 235 train loss: 2.3130\n",
      ">>> Epoch 235 test loss: 5.6032\n",
      ">>> Epoch 236 train loss: 2.3129\n",
      ">>> Epoch 236 test loss: 5.6072\n",
      ">>> Epoch 237 train loss: 2.3129\n",
      ">>> Epoch 237 test loss: 5.6112\n",
      ">>> Epoch 238 train loss: 2.3128\n",
      ">>> Epoch 238 test loss: 5.6152\n",
      ">>> Epoch 239 train loss: 2.3128\n",
      ">>> Epoch 239 test loss: 5.6192\n",
      ">>> Epoch 240 train loss: 2.3127\n",
      ">>> Epoch 240 test loss: 5.6232\n",
      ">>> Epoch 241 train loss: 2.3127\n",
      ">>> Epoch 241 test loss: 5.6272\n",
      ">>> Epoch 242 train loss: 2.3126\n",
      ">>> Epoch 242 test loss: 5.6311\n",
      ">>> Epoch 243 train loss: 2.3126\n",
      ">>> Epoch 243 test loss: 5.6351\n",
      ">>> Epoch 244 train loss: 2.3125\n",
      ">>> Epoch 244 test loss: 5.6390\n",
      ">>> Epoch 245 train loss: 2.3125\n",
      ">>> Epoch 245 test loss: 5.6429\n",
      ">>> Epoch 246 train loss: 2.3124\n",
      ">>> Epoch 246 test loss: 5.6468\n",
      ">>> Epoch 247 train loss: 2.3124\n",
      ">>> Epoch 247 test loss: 5.6506\n",
      ">>> Epoch 248 train loss: 2.3124\n",
      ">>> Epoch 248 test loss: 5.6545\n",
      ">>> Epoch 249 train loss: 2.3123\n",
      ">>> Epoch 249 test loss: 5.6583\n",
      ">>> Epoch 250 train loss: 2.3123\n",
      ">>> Epoch 250 test loss: 5.6622\n",
      ">>> Epoch 251 train loss: 2.3122\n",
      ">>> Epoch 251 test loss: 5.6660\n",
      ">>> Epoch 252 train loss: 2.3122\n",
      ">>> Epoch 252 test loss: 5.6698\n",
      ">>> Epoch 253 train loss: 2.3121\n",
      ">>> Epoch 253 test loss: 5.6736\n",
      ">>> Epoch 254 train loss: 2.3121\n",
      ">>> Epoch 254 test loss: 5.6773\n",
      ">>> Epoch 255 train loss: 2.3121\n",
      ">>> Epoch 255 test loss: 5.6811\n",
      ">>> Epoch 256 train loss: 2.3120\n",
      ">>> Epoch 256 test loss: 5.6848\n",
      ">>> Epoch 257 train loss: 2.3120\n",
      ">>> Epoch 257 test loss: 5.6886\n",
      ">>> Epoch 258 train loss: 2.3119\n",
      ">>> Epoch 258 test loss: 5.6923\n",
      ">>> Epoch 259 train loss: 2.3119\n",
      ">>> Epoch 259 test loss: 5.6960\n",
      ">>> Epoch 260 train loss: 2.3119\n",
      ">>> Epoch 260 test loss: 5.6997\n",
      ">>> Epoch 261 train loss: 2.3118\n",
      ">>> Epoch 261 test loss: 5.7034\n",
      ">>> Epoch 262 train loss: 2.3118\n",
      ">>> Epoch 262 test loss: 5.7070\n",
      ">>> Epoch 263 train loss: 2.3117\n",
      ">>> Epoch 263 test loss: 5.7107\n",
      ">>> Epoch 264 train loss: 2.3117\n",
      ">>> Epoch 264 test loss: 5.7143\n",
      ">>> Epoch 265 train loss: 2.3117\n",
      ">>> Epoch 265 test loss: 5.7179\n",
      ">>> Epoch 266 train loss: 2.3116\n",
      ">>> Epoch 266 test loss: 5.7215\n",
      ">>> Epoch 267 train loss: 2.3116\n",
      ">>> Epoch 267 test loss: 5.7251\n",
      ">>> Epoch 268 train loss: 2.3116\n",
      ">>> Epoch 268 test loss: 5.7287\n",
      ">>> Epoch 269 train loss: 2.3115\n",
      ">>> Epoch 269 test loss: 5.7323\n",
      ">>> Epoch 270 train loss: 2.3115\n",
      ">>> Epoch 270 test loss: 5.7359\n",
      ">>> Epoch 271 train loss: 2.3115\n",
      ">>> Epoch 271 test loss: 5.7394\n",
      ">>> Epoch 272 train loss: 2.3114\n",
      ">>> Epoch 272 test loss: 5.7430\n",
      ">>> Epoch 273 train loss: 2.3114\n",
      ">>> Epoch 273 test loss: 5.7465\n",
      ">>> Epoch 274 train loss: 2.3114\n",
      ">>> Epoch 274 test loss: 5.7500\n",
      ">>> Epoch 275 train loss: 2.3113\n",
      ">>> Epoch 275 test loss: 5.7535\n",
      ">>> Epoch 276 train loss: 2.3113\n",
      ">>> Epoch 276 test loss: 5.7570\n",
      ">>> Epoch 277 train loss: 2.3113\n",
      ">>> Epoch 277 test loss: 5.7605\n",
      ">>> Epoch 278 train loss: 2.3112\n",
      ">>> Epoch 278 test loss: 5.7639\n",
      ">>> Epoch 279 train loss: 2.3112\n",
      ">>> Epoch 279 test loss: 5.7674\n",
      ">>> Epoch 280 train loss: 2.3112\n",
      ">>> Epoch 280 test loss: 5.7708\n",
      ">>> Epoch 281 train loss: 2.3111\n",
      ">>> Epoch 281 test loss: 5.7742\n",
      ">>> Epoch 282 train loss: 2.3111\n",
      ">>> Epoch 282 test loss: 5.7777\n",
      ">>> Epoch 283 train loss: 2.3111\n",
      ">>> Epoch 283 test loss: 5.7811\n",
      ">>> Epoch 284 train loss: 2.3110\n",
      ">>> Epoch 284 test loss: 5.7845\n",
      ">>> Epoch 285 train loss: 2.3110\n",
      ">>> Epoch 285 test loss: 5.7878\n",
      ">>> Epoch 286 train loss: 2.3110\n",
      ">>> Epoch 286 test loss: 5.7912\n",
      ">>> Epoch 287 train loss: 2.3109\n",
      ">>> Epoch 287 test loss: 5.7946\n",
      ">>> Epoch 288 train loss: 2.3109\n",
      ">>> Epoch 288 test loss: 5.7979\n",
      ">>> Epoch 289 train loss: 2.3109\n",
      ">>> Epoch 289 test loss: 5.8013\n",
      ">>> Epoch 290 train loss: 2.3109\n",
      ">>> Epoch 290 test loss: 5.8046\n",
      ">>> Epoch 291 train loss: 2.3108\n",
      ">>> Epoch 291 test loss: 5.8079\n",
      ">>> Epoch 292 train loss: 2.3108\n",
      ">>> Epoch 292 test loss: 5.8112\n",
      ">>> Epoch 293 train loss: 2.3108\n",
      ">>> Epoch 293 test loss: 5.8145\n",
      ">>> Epoch 294 train loss: 2.3107\n",
      ">>> Epoch 294 test loss: 5.8178\n",
      ">>> Epoch 295 train loss: 2.3107\n",
      ">>> Epoch 295 test loss: 5.8211\n",
      ">>> Epoch 296 train loss: 2.3107\n",
      ">>> Epoch 296 test loss: 5.8243\n",
      ">>> Epoch 297 train loss: 2.3107\n",
      ">>> Epoch 297 test loss: 5.8276\n",
      ">>> Epoch 298 train loss: 2.3106\n",
      ">>> Epoch 298 test loss: 5.8308\n",
      ">>> Epoch 299 train loss: 2.3106\n",
      ">>> Epoch 299 test loss: 5.8341\n",
      ">>> Epoch 300 train loss: 2.3106\n",
      ">>> Epoch 300 test loss: 5.8373\n",
      ">>> Epoch 301 train loss: 2.3106\n",
      ">>> Epoch 301 test loss: 5.8405\n",
      ">>> Epoch 302 train loss: 2.3105\n",
      ">>> Epoch 302 test loss: 5.8437\n",
      ">>> Epoch 303 train loss: 2.3105\n",
      ">>> Epoch 303 test loss: 5.8469\n",
      ">>> Epoch 304 train loss: 2.3105\n",
      ">>> Epoch 304 test loss: 5.8501\n",
      ">>> Epoch 305 train loss: 2.3105\n",
      ">>> Epoch 305 test loss: 5.8533\n",
      ">>> Epoch 306 train loss: 2.3104\n",
      ">>> Epoch 306 test loss: 5.8564\n",
      ">>> Epoch 307 train loss: 2.3104\n",
      ">>> Epoch 307 test loss: 5.8596\n",
      ">>> Epoch 308 train loss: 2.3104\n",
      ">>> Epoch 308 test loss: 5.8627\n",
      ">>> Epoch 309 train loss: 2.3104\n",
      ">>> Epoch 309 test loss: 5.8659\n",
      ">>> Epoch 310 train loss: 2.3103\n",
      ">>> Epoch 310 test loss: 5.8690\n",
      ">>> Epoch 311 train loss: 2.3103\n",
      ">>> Epoch 311 test loss: 5.8721\n",
      ">>> Epoch 312 train loss: 2.3103\n",
      ">>> Epoch 312 test loss: 5.8752\n",
      ">>> Epoch 313 train loss: 2.3103\n",
      ">>> Epoch 313 test loss: 5.8783\n",
      ">>> Epoch 314 train loss: 2.3103\n",
      ">>> Epoch 314 test loss: 5.8814\n",
      ">>> Epoch 315 train loss: 2.3102\n",
      ">>> Epoch 315 test loss: 5.8845\n",
      ">>> Epoch 316 train loss: 2.3102\n",
      ">>> Epoch 316 test loss: 5.8876\n",
      ">>> Epoch 317 train loss: 2.3102\n",
      ">>> Epoch 317 test loss: 5.8906\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# increase size of image to 64\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mvt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/vision-transformer/vision_transformer_v1.py:209\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, (inputs, labels) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_dataloader):\n\u001b[1;32m    208\u001b[0m     inputs, labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m--> 209\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    210\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[1;32m    211\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/vit/lib/python3.10/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/vit/lib/python3.10/site-packages/torch/utils/data/dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/vit/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:50\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[0;32m---> 50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/vit/lib/python3.10/site-packages/torch/utils/data/dataset.py:418\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitems__\u001b[39m(\u001b[38;5;28mself\u001b[39m, indices: List[\u001b[38;5;28mint\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[_T_co]:\n\u001b[1;32m    415\u001b[0m     \u001b[38;5;66;03m# add batched sampling support when parent dataset supports it.\u001b[39;00m\n\u001b[1;32m    416\u001b[0m     \u001b[38;5;66;03m# see torch.utils.data._utils.fetch._MapDatasetFetcher\u001b[39;00m\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)):\n\u001b[0;32m--> 418\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    419\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    420\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx]] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/vit/lib/python3.10/site-packages/torch/utils/data/dataset.py:420\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx]] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/vit/lib/python3.10/site-packages/torch/utils/data/dataset.py:420\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/vit/lib/python3.10/site-packages/torchvision/datasets/oxford_iiit_pet.py:116\u001b[0m, in \u001b[0;36mOxfordIIITPet.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    113\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(target)\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m--> 116\u001b[0m     image, target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransforms\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m image, target\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/vit/lib/python3.10/site-packages/torchvision/datasets/vision.py:95\u001b[0m, in \u001b[0;36mStandardTransform.__call__\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Any, target: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Any, Any]:\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 95\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     97\u001b[0m         target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/vit/lib/python3.10/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/vit/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/vit/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/vit/lib/python3.10/site-packages/torchvision/transforms/transforms.py:354\u001b[0m, in \u001b[0;36mResize.forward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m    347\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    349\u001b[0m \u001b[38;5;124;03m        img (PIL Image or Tensor): Image to be scaled.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;124;03m        PIL Image or Tensor: Rescaled image.\u001b[39;00m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 354\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mantialias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/vit/lib/python3.10/site-packages/torchvision/transforms/functional.py:477\u001b[0m, in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation, max_size, antialias)\u001b[0m\n\u001b[1;32m    475\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnti-alias option is always applied for PIL Image input. Argument antialias is ignored.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    476\u001b[0m     pil_interpolation \u001b[38;5;241m=\u001b[39m pil_modes_mapping[interpolation]\n\u001b[0;32m--> 477\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_pil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpil_interpolation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F_t\u001b[38;5;241m.\u001b[39mresize(img, size\u001b[38;5;241m=\u001b[39moutput_size, interpolation\u001b[38;5;241m=\u001b[39minterpolation\u001b[38;5;241m.\u001b[39mvalue, antialias\u001b[38;5;241m=\u001b[39mantialias)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/vit/lib/python3.10/site-packages/torchvision/transforms/_functional_pil.py:250\u001b[0m, in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(size, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(size) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m):\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot inappropriate size arg: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 250\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/vit/lib/python3.10/site-packages/PIL/Image.py:2356\u001b[0m, in \u001b[0;36mImage.resize\u001b[0;34m(self, size, resample, box, reducing_gap)\u001b[0m\n\u001b[1;32m   2344\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2345\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduce(factor, box\u001b[38;5;241m=\u001b[39mreduce_box)\n\u001b[1;32m   2346\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduce)\n\u001b[1;32m   2347\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m Image\u001b[38;5;241m.\u001b[39mreduce(\u001b[38;5;28mself\u001b[39m, factor, box\u001b[38;5;241m=\u001b[39mreduce_box)\n\u001b[1;32m   2348\u001b[0m         )\n\u001b[1;32m   2349\u001b[0m         box \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2350\u001b[0m             (box[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_x,\n\u001b[1;32m   2351\u001b[0m             (box[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_y,\n\u001b[1;32m   2352\u001b[0m             (box[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_x,\n\u001b[1;32m   2353\u001b[0m             (box[\u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_y,\n\u001b[1;32m   2354\u001b[0m         )\n\u001b[0;32m-> 2356\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbox\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# increase size of image to 64\n",
    "# train loss goes from 2.3 -> 2.5\n",
    "vt.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e82d06d7-42cd-4451-8b1a-4cd6f5007f9b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Epoch 0 train loss: 3.8658\n",
      ">>> Epoch 0 test loss: 2.8687\n",
      ">>> Epoch 1 train loss: 3.7164\n",
      ">>> Epoch 1 test loss: 2.8992\n",
      ">>> Epoch 2 train loss: 3.5805\n",
      ">>> Epoch 2 test loss: 2.9448\n",
      ">>> Epoch 3 train loss: 3.4591\n",
      ">>> Epoch 3 test loss: 3.0034\n",
      ">>> Epoch 4 train loss: 3.3519\n",
      ">>> Epoch 4 test loss: 3.0735\n",
      ">>> Epoch 5 train loss: 3.2580\n",
      ">>> Epoch 5 test loss: 3.1538\n",
      ">>> Epoch 6 train loss: 3.1760\n",
      ">>> Epoch 6 test loss: 3.2433\n",
      ">>> Epoch 7 train loss: 3.1048\n",
      ">>> Epoch 7 test loss: 3.3399\n",
      ">>> Epoch 8 train loss: 3.0429\n",
      ">>> Epoch 8 test loss: 3.4412\n",
      ">>> Epoch 9 train loss: 2.9894\n",
      ">>> Epoch 9 test loss: 3.5448\n",
      ">>> Epoch 10 train loss: 2.9430\n",
      ">>> Epoch 10 test loss: 3.6484\n",
      ">>> Epoch 11 train loss: 2.9030\n",
      ">>> Epoch 11 test loss: 3.7498\n",
      ">>> Epoch 12 train loss: 2.8685\n",
      ">>> Epoch 12 test loss: 3.8475\n",
      ">>> Epoch 13 train loss: 2.8387\n",
      ">>> Epoch 13 test loss: 3.9405\n",
      ">>> Epoch 14 train loss: 2.8130\n",
      ">>> Epoch 14 test loss: 4.0282\n",
      ">>> Epoch 15 train loss: 2.7910\n",
      ">>> Epoch 15 test loss: 4.1103\n",
      ">>> Epoch 16 train loss: 2.7720\n",
      ">>> Epoch 16 test loss: 4.1869\n",
      ">>> Epoch 17 train loss: 2.7555\n",
      ">>> Epoch 17 test loss: 4.2583\n",
      ">>> Epoch 18 train loss: 2.7412\n",
      ">>> Epoch 18 test loss: 4.3251\n",
      ">>> Epoch 19 train loss: 2.7285\n",
      ">>> Epoch 19 test loss: 4.3880\n",
      ">>> Epoch 20 train loss: 2.7171\n",
      ">>> Epoch 20 test loss: 4.4477\n",
      ">>> Epoch 21 train loss: 2.7068\n",
      ">>> Epoch 21 test loss: 4.5048\n",
      ">>> Epoch 22 train loss: 2.6976\n",
      ">>> Epoch 22 test loss: 4.5599\n",
      ">>> Epoch 23 train loss: 2.6893\n",
      ">>> Epoch 23 test loss: 4.6133\n",
      ">>> Epoch 24 train loss: 2.6820\n",
      ">>> Epoch 24 test loss: 4.6652\n",
      ">>> Epoch 25 train loss: 2.6756\n",
      ">>> Epoch 25 test loss: 4.7157\n",
      ">>> Epoch 26 train loss: 2.6699\n",
      ">>> Epoch 26 test loss: 4.7649\n",
      ">>> Epoch 27 train loss: 2.6649\n",
      ">>> Epoch 27 test loss: 4.8126\n",
      ">>> Epoch 28 train loss: 2.6605\n",
      ">>> Epoch 28 test loss: 4.8587\n",
      ">>> Epoch 29 train loss: 2.6565\n",
      ">>> Epoch 29 test loss: 4.9031\n",
      ">>> Epoch 30 train loss: 2.6528\n",
      ">>> Epoch 30 test loss: 4.9458\n",
      ">>> Epoch 31 train loss: 2.6493\n",
      ">>> Epoch 31 test loss: 4.9867\n",
      ">>> Epoch 32 train loss: 2.6461\n",
      ">>> Epoch 32 test loss: 5.0259\n",
      ">>> Epoch 33 train loss: 2.6432\n",
      ">>> Epoch 33 test loss: 5.0635\n",
      ">>> Epoch 34 train loss: 2.6404\n",
      ">>> Epoch 34 test loss: 5.0997\n",
      ">>> Epoch 35 train loss: 2.6379\n",
      ">>> Epoch 35 test loss: 5.1348\n",
      ">>> Epoch 36 train loss: 2.6356\n",
      ">>> Epoch 36 test loss: 5.1690\n",
      ">>> Epoch 37 train loss: 2.6336\n",
      ">>> Epoch 37 test loss: 5.2026\n",
      ">>> Epoch 38 train loss: 2.6318\n",
      ">>> Epoch 38 test loss: 5.2356\n",
      ">>> Epoch 39 train loss: 2.6302\n",
      ">>> Epoch 39 test loss: 5.2680\n",
      ">>> Epoch 40 train loss: 2.6287\n",
      ">>> Epoch 40 test loss: 5.2998\n",
      ">>> Epoch 41 train loss: 2.6273\n",
      ">>> Epoch 41 test loss: 5.3309\n",
      ">>> Epoch 42 train loss: 2.6260\n",
      ">>> Epoch 42 test loss: 5.3611\n",
      ">>> Epoch 43 train loss: 2.6247\n",
      ">>> Epoch 43 test loss: 5.3903\n",
      ">>> Epoch 44 train loss: 2.6236\n",
      ">>> Epoch 44 test loss: 5.4184\n",
      ">>> Epoch 45 train loss: 2.6225\n",
      ">>> Epoch 45 test loss: 5.4453\n",
      ">>> Epoch 46 train loss: 2.6216\n",
      ">>> Epoch 46 test loss: 5.4712\n",
      ">>> Epoch 47 train loss: 2.6207\n",
      ">>> Epoch 47 test loss: 5.4960\n",
      ">>> Epoch 48 train loss: 2.6198\n",
      ">>> Epoch 48 test loss: 5.5199\n",
      ">>> Epoch 49 train loss: 2.6191\n",
      ">>> Epoch 49 test loss: 5.5432\n",
      ">>> Epoch 50 train loss: 2.6184\n",
      ">>> Epoch 50 test loss: 5.5659\n",
      ">>> Epoch 51 train loss: 2.6177\n",
      ">>> Epoch 51 test loss: 5.5882\n",
      ">>> Epoch 52 train loss: 2.6170\n",
      ">>> Epoch 52 test loss: 5.6101\n",
      ">>> Epoch 53 train loss: 2.6164\n",
      ">>> Epoch 53 test loss: 5.6316\n",
      ">>> Epoch 54 train loss: 2.6158\n",
      ">>> Epoch 54 test loss: 5.6526\n",
      ">>> Epoch 55 train loss: 2.6153\n",
      ">>> Epoch 55 test loss: 5.6730\n",
      ">>> Epoch 56 train loss: 2.6148\n",
      ">>> Epoch 56 test loss: 5.6928\n",
      ">>> Epoch 57 train loss: 2.6144\n",
      ">>> Epoch 57 test loss: 5.7119\n",
      ">>> Epoch 58 train loss: 2.6139\n",
      ">>> Epoch 58 test loss: 5.7304\n",
      ">>> Epoch 59 train loss: 2.6135\n",
      ">>> Epoch 59 test loss: 5.7483\n",
      ">>> Epoch 60 train loss: 2.6131\n",
      ">>> Epoch 60 test loss: 5.7657\n",
      ">>> Epoch 61 train loss: 2.6127\n",
      ">>> Epoch 61 test loss: 5.7829\n",
      ">>> Epoch 62 train loss: 2.6124\n",
      ">>> Epoch 62 test loss: 5.7999\n",
      ">>> Epoch 63 train loss: 2.6120\n",
      ">>> Epoch 63 test loss: 5.8167\n",
      ">>> Epoch 64 train loss: 2.6117\n",
      ">>> Epoch 64 test loss: 5.8336\n",
      ">>> Epoch 65 train loss: 2.6114\n",
      ">>> Epoch 65 test loss: 5.8502\n",
      ">>> Epoch 66 train loss: 2.6111\n",
      ">>> Epoch 66 test loss: 5.8668\n",
      ">>> Epoch 67 train loss: 2.6108\n",
      ">>> Epoch 67 test loss: 5.8830\n",
      ">>> Epoch 68 train loss: 2.6106\n",
      ">>> Epoch 68 test loss: 5.8988\n",
      ">>> Epoch 69 train loss: 2.6103\n",
      ">>> Epoch 69 test loss: 5.9141\n",
      ">>> Epoch 70 train loss: 2.6100\n",
      ">>> Epoch 70 test loss: 5.9290\n",
      ">>> Epoch 71 train loss: 2.6098\n",
      ">>> Epoch 71 test loss: 5.9435\n",
      ">>> Epoch 72 train loss: 2.6096\n",
      ">>> Epoch 72 test loss: 5.9576\n",
      ">>> Epoch 73 train loss: 2.6094\n",
      ">>> Epoch 73 test loss: 5.9715\n",
      ">>> Epoch 74 train loss: 2.6092\n",
      ">>> Epoch 74 test loss: 5.9852\n",
      ">>> Epoch 75 train loss: 2.6090\n",
      ">>> Epoch 75 test loss: 5.9988\n",
      ">>> Epoch 76 train loss: 2.6088\n",
      ">>> Epoch 76 test loss: 6.0123\n",
      ">>> Epoch 77 train loss: 2.6086\n",
      ">>> Epoch 77 test loss: 6.0257\n",
      ">>> Epoch 78 train loss: 2.6084\n",
      ">>> Epoch 78 test loss: 6.0389\n",
      ">>> Epoch 79 train loss: 2.6082\n",
      ">>> Epoch 79 test loss: 6.0520\n",
      ">>> Epoch 80 train loss: 2.6081\n",
      ">>> Epoch 80 test loss: 6.0649\n",
      ">>> Epoch 81 train loss: 2.6079\n",
      ">>> Epoch 81 test loss: 6.0775\n",
      ">>> Epoch 82 train loss: 2.6077\n",
      ">>> Epoch 82 test loss: 6.0899\n",
      ">>> Epoch 83 train loss: 2.6076\n",
      ">>> Epoch 83 test loss: 6.1022\n",
      ">>> Epoch 84 train loss: 2.6074\n",
      ">>> Epoch 84 test loss: 6.1143\n",
      ">>> Epoch 85 train loss: 2.6073\n",
      ">>> Epoch 85 test loss: 6.1264\n",
      ">>> Epoch 86 train loss: 2.6072\n",
      ">>> Epoch 86 test loss: 6.1385\n",
      ">>> Epoch 87 train loss: 2.6070\n",
      ">>> Epoch 87 test loss: 6.1505\n",
      ">>> Epoch 88 train loss: 2.6069\n",
      ">>> Epoch 88 test loss: 6.1623\n",
      ">>> Epoch 89 train loss: 2.6068\n",
      ">>> Epoch 89 test loss: 6.1741\n",
      ">>> Epoch 90 train loss: 2.6066\n",
      ">>> Epoch 90 test loss: 6.1857\n",
      ">>> Epoch 91 train loss: 2.6065\n",
      ">>> Epoch 91 test loss: 6.1971\n",
      ">>> Epoch 92 train loss: 2.6064\n",
      ">>> Epoch 92 test loss: 6.2083\n",
      ">>> Epoch 93 train loss: 2.6063\n",
      ">>> Epoch 93 test loss: 6.2194\n",
      ">>> Epoch 94 train loss: 2.6062\n",
      ">>> Epoch 94 test loss: 6.2303\n",
      ">>> Epoch 95 train loss: 2.6061\n",
      ">>> Epoch 95 test loss: 6.2412\n",
      ">>> Epoch 96 train loss: 2.6060\n",
      ">>> Epoch 96 test loss: 6.2520\n",
      ">>> Epoch 97 train loss: 2.6059\n",
      ">>> Epoch 97 test loss: 6.2627\n",
      ">>> Epoch 98 train loss: 2.6058\n",
      ">>> Epoch 98 test loss: 6.2733\n",
      ">>> Epoch 99 train loss: 2.6057\n",
      ">>> Epoch 99 test loss: 6.2838\n",
      ">>> Epoch 100 train loss: 2.6056\n",
      ">>> Epoch 100 test loss: 6.2942\n",
      ">>> Epoch 101 train loss: 2.6055\n",
      ">>> Epoch 101 test loss: 6.3045\n",
      ">>> Epoch 102 train loss: 2.6054\n",
      ">>> Epoch 102 test loss: 6.3147\n",
      ">>> Epoch 103 train loss: 2.6053\n",
      ">>> Epoch 103 test loss: 6.3248\n",
      ">>> Epoch 104 train loss: 2.6052\n",
      ">>> Epoch 104 test loss: 6.3348\n",
      ">>> Epoch 105 train loss: 2.6051\n",
      ">>> Epoch 105 test loss: 6.3448\n",
      ">>> Epoch 106 train loss: 2.6050\n",
      ">>> Epoch 106 test loss: 6.3548\n",
      ">>> Epoch 107 train loss: 2.6050\n",
      ">>> Epoch 107 test loss: 6.3647\n",
      ">>> Epoch 108 train loss: 2.6049\n",
      ">>> Epoch 108 test loss: 6.3746\n",
      ">>> Epoch 109 train loss: 2.6048\n",
      ">>> Epoch 109 test loss: 6.3843\n",
      ">>> Epoch 110 train loss: 2.6047\n",
      ">>> Epoch 110 test loss: 6.3940\n",
      ">>> Epoch 111 train loss: 2.6047\n",
      ">>> Epoch 111 test loss: 6.4036\n",
      ">>> Epoch 112 train loss: 2.6046\n",
      ">>> Epoch 112 test loss: 6.4131\n",
      ">>> Epoch 113 train loss: 2.6045\n",
      ">>> Epoch 113 test loss: 6.4225\n",
      ">>> Epoch 114 train loss: 2.6044\n",
      ">>> Epoch 114 test loss: 6.4318\n",
      ">>> Epoch 115 train loss: 2.6044\n",
      ">>> Epoch 115 test loss: 6.4410\n",
      ">>> Epoch 116 train loss: 2.6043\n",
      ">>> Epoch 116 test loss: 6.4502\n",
      ">>> Epoch 117 train loss: 2.6042\n",
      ">>> Epoch 117 test loss: 6.4593\n",
      ">>> Epoch 118 train loss: 2.6042\n",
      ">>> Epoch 118 test loss: 6.4684\n",
      ">>> Epoch 119 train loss: 2.6041\n",
      ">>> Epoch 119 test loss: 6.4774\n",
      ">>> Epoch 120 train loss: 2.6041\n",
      ">>> Epoch 120 test loss: 6.4863\n",
      ">>> Epoch 121 train loss: 2.6040\n",
      ">>> Epoch 121 test loss: 6.4952\n",
      ">>> Epoch 122 train loss: 2.6039\n",
      ">>> Epoch 122 test loss: 6.5040\n",
      ">>> Epoch 123 train loss: 2.6039\n",
      ">>> Epoch 123 test loss: 6.5128\n",
      ">>> Epoch 124 train loss: 2.6038\n",
      ">>> Epoch 124 test loss: 6.5216\n",
      ">>> Epoch 125 train loss: 2.6038\n",
      ">>> Epoch 125 test loss: 6.5303\n",
      ">>> Epoch 126 train loss: 2.6037\n",
      ">>> Epoch 126 test loss: 6.5390\n",
      ">>> Epoch 127 train loss: 2.6037\n",
      ">>> Epoch 127 test loss: 6.5476\n",
      ">>> Epoch 128 train loss: 2.6036\n",
      ">>> Epoch 128 test loss: 6.5561\n",
      ">>> Epoch 129 train loss: 2.6036\n",
      ">>> Epoch 129 test loss: 6.5646\n",
      ">>> Epoch 130 train loss: 2.6035\n",
      ">>> Epoch 130 test loss: 6.5730\n",
      ">>> Epoch 131 train loss: 2.6035\n",
      ">>> Epoch 131 test loss: 6.5813\n",
      ">>> Epoch 132 train loss: 2.6034\n",
      ">>> Epoch 132 test loss: 6.5896\n",
      ">>> Epoch 133 train loss: 2.6034\n",
      ">>> Epoch 133 test loss: 6.5979\n",
      ">>> Epoch 134 train loss: 2.6033\n",
      ">>> Epoch 134 test loss: 6.6061\n",
      ">>> Epoch 135 train loss: 2.6033\n",
      ">>> Epoch 135 test loss: 6.6142\n",
      ">>> Epoch 136 train loss: 2.6032\n",
      ">>> Epoch 136 test loss: 6.6224\n",
      ">>> Epoch 137 train loss: 2.6032\n",
      ">>> Epoch 137 test loss: 6.6304\n",
      ">>> Epoch 138 train loss: 2.6031\n",
      ">>> Epoch 138 test loss: 6.6385\n",
      ">>> Epoch 139 train loss: 2.6031\n",
      ">>> Epoch 139 test loss: 6.6465\n",
      ">>> Epoch 140 train loss: 2.6030\n",
      ">>> Epoch 140 test loss: 6.6544\n",
      ">>> Epoch 141 train loss: 2.6030\n",
      ">>> Epoch 141 test loss: 6.6623\n",
      ">>> Epoch 142 train loss: 2.6030\n",
      ">>> Epoch 142 test loss: 6.6702\n",
      ">>> Epoch 143 train loss: 2.6029\n",
      ">>> Epoch 143 test loss: 6.6780\n",
      ">>> Epoch 144 train loss: 2.6029\n",
      ">>> Epoch 144 test loss: 6.6858\n",
      ">>> Epoch 145 train loss: 2.6028\n",
      ">>> Epoch 145 test loss: 6.6935\n",
      ">>> Epoch 146 train loss: 2.6028\n",
      ">>> Epoch 146 test loss: 6.7012\n",
      ">>> Epoch 147 train loss: 2.6028\n",
      ">>> Epoch 147 test loss: 6.7089\n",
      ">>> Epoch 148 train loss: 2.6027\n",
      ">>> Epoch 148 test loss: 6.7165\n",
      ">>> Epoch 149 train loss: 2.6027\n",
      ">>> Epoch 149 test loss: 6.7241\n",
      ">>> Epoch 150 train loss: 2.6027\n",
      ">>> Epoch 150 test loss: 6.7316\n",
      ">>> Epoch 151 train loss: 2.6026\n",
      ">>> Epoch 151 test loss: 6.7391\n",
      ">>> Epoch 152 train loss: 2.6026\n",
      ">>> Epoch 152 test loss: 6.7466\n",
      ">>> Epoch 153 train loss: 2.6026\n",
      ">>> Epoch 153 test loss: 6.7540\n",
      ">>> Epoch 154 train loss: 2.6025\n",
      ">>> Epoch 154 test loss: 6.7614\n",
      ">>> Epoch 155 train loss: 2.6025\n",
      ">>> Epoch 155 test loss: 6.7687\n",
      ">>> Epoch 156 train loss: 2.6025\n",
      ">>> Epoch 156 test loss: 6.7760\n",
      ">>> Epoch 157 train loss: 2.6024\n",
      ">>> Epoch 157 test loss: 6.7833\n",
      ">>> Epoch 158 train loss: 2.6024\n",
      ">>> Epoch 158 test loss: 6.7906\n",
      ">>> Epoch 159 train loss: 2.6024\n",
      ">>> Epoch 159 test loss: 6.7978\n",
      ">>> Epoch 160 train loss: 2.6023\n",
      ">>> Epoch 160 test loss: 6.8049\n",
      ">>> Epoch 161 train loss: 2.6023\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# emb_dim from 32 -> 64\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mvt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/vision-transformer/vision_transformer_v1.py:223\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, (inputs, labels) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(test_dataloader):\n\u001b[1;32m    222\u001b[0m     inputs, labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m--> 223\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[1;32m    224\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m    225\u001b[0m     epoch_losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/vit/lib/python3.10/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/vit/lib/python3.10/site-packages/torch/utils/data/dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/vit/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:50\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[0;32m---> 50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/vit/lib/python3.10/site-packages/torch/utils/data/dataset.py:418\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitems__\u001b[39m(\u001b[38;5;28mself\u001b[39m, indices: List[\u001b[38;5;28mint\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[_T_co]:\n\u001b[1;32m    415\u001b[0m     \u001b[38;5;66;03m# add batched sampling support when parent dataset supports it.\u001b[39;00m\n\u001b[1;32m    416\u001b[0m     \u001b[38;5;66;03m# see torch.utils.data._utils.fetch._MapDatasetFetcher\u001b[39;00m\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)):\n\u001b[0;32m--> 418\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    419\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    420\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx]] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/vit/lib/python3.10/site-packages/torch/utils/data/dataset.py:420\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx]] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/vit/lib/python3.10/site-packages/torch/utils/data/dataset.py:420\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/vit/lib/python3.10/site-packages/torchvision/datasets/oxford_iiit_pet.py:116\u001b[0m, in \u001b[0;36mOxfordIIITPet.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    113\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(target)\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m--> 116\u001b[0m     image, target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransforms\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m image, target\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/vit/lib/python3.10/site-packages/torchvision/datasets/vision.py:95\u001b[0m, in \u001b[0;36mStandardTransform.__call__\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Any, target: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Any, Any]:\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 95\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     97\u001b[0m         target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/vit/lib/python3.10/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/vit/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/vit/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/vit/lib/python3.10/site-packages/torchvision/transforms/transforms.py:354\u001b[0m, in \u001b[0;36mResize.forward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m    347\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    349\u001b[0m \u001b[38;5;124;03m        img (PIL Image or Tensor): Image to be scaled.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;124;03m        PIL Image or Tensor: Rescaled image.\u001b[39;00m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 354\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mantialias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/vit/lib/python3.10/site-packages/torchvision/transforms/functional.py:477\u001b[0m, in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation, max_size, antialias)\u001b[0m\n\u001b[1;32m    475\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnti-alias option is always applied for PIL Image input. Argument antialias is ignored.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    476\u001b[0m     pil_interpolation \u001b[38;5;241m=\u001b[39m pil_modes_mapping[interpolation]\n\u001b[0;32m--> 477\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_pil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpil_interpolation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F_t\u001b[38;5;241m.\u001b[39mresize(img, size\u001b[38;5;241m=\u001b[39moutput_size, interpolation\u001b[38;5;241m=\u001b[39minterpolation\u001b[38;5;241m.\u001b[39mvalue, antialias\u001b[38;5;241m=\u001b[39mantialias)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/vit/lib/python3.10/site-packages/torchvision/transforms/_functional_pil.py:250\u001b[0m, in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(size, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(size) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m):\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot inappropriate size arg: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 250\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/vit/lib/python3.10/site-packages/PIL/Image.py:2356\u001b[0m, in \u001b[0;36mImage.resize\u001b[0;34m(self, size, resample, box, reducing_gap)\u001b[0m\n\u001b[1;32m   2344\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2345\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduce(factor, box\u001b[38;5;241m=\u001b[39mreduce_box)\n\u001b[1;32m   2346\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduce)\n\u001b[1;32m   2347\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m Image\u001b[38;5;241m.\u001b[39mreduce(\u001b[38;5;28mself\u001b[39m, factor, box\u001b[38;5;241m=\u001b[39mreduce_box)\n\u001b[1;32m   2348\u001b[0m         )\n\u001b[1;32m   2349\u001b[0m         box \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2350\u001b[0m             (box[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_x,\n\u001b[1;32m   2351\u001b[0m             (box[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_y,\n\u001b[1;32m   2352\u001b[0m             (box[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_x,\n\u001b[1;32m   2353\u001b[0m             (box[\u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_y,\n\u001b[1;32m   2354\u001b[0m         )\n\u001b[0;32m-> 2356\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbox\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# emb_dim from 32 -> 64\n",
    "# train loss stabilizes around 2.6, up from 2.3\n",
    "vt.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24291fa5-8ac9-4fef-8ccb-0f90babf1de0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Epoch 0 train loss: 3.8522\n",
      ">>> Epoch 0 test loss: 3.7583\n",
      ">>> Epoch 1 train loss: 3.3214\n",
      ">>> Epoch 1 test loss: 4.2924\n",
      ">>> Epoch 2 train loss: 3.0272\n",
      ">>> Epoch 2 test loss: 4.6930\n",
      ">>> Epoch 3 train loss: 2.8738\n",
      ">>> Epoch 3 test loss: 5.0023\n",
      ">>> Epoch 4 train loss: 2.7878\n",
      ">>> Epoch 4 test loss: 5.2786\n",
      ">>> Epoch 5 train loss: 2.7291\n",
      ">>> Epoch 5 test loss: 5.5277\n",
      ">>> Epoch 6 train loss: 2.6863\n",
      ">>> Epoch 6 test loss: 5.7484\n",
      ">>> Epoch 7 train loss: 2.6564\n",
      ">>> Epoch 7 test loss: 5.9519\n",
      ">>> Epoch 8 train loss: 2.6367\n",
      ">>> Epoch 8 test loss: 6.1578\n",
      ">>> Epoch 9 train loss: 2.6277\n",
      ">>> Epoch 9 test loss: 6.3787\n",
      ">>> Epoch 10 train loss: 2.6256\n",
      ">>> Epoch 10 test loss: 6.6100\n",
      ">>> Epoch 11 train loss: 2.6222\n",
      ">>> Epoch 11 test loss: 6.8390\n",
      ">>> Epoch 12 train loss: 2.6153\n",
      ">>> Epoch 12 test loss: 7.0561\n",
      ">>> Epoch 13 train loss: 2.6086\n",
      ">>> Epoch 13 test loss: 7.2550\n",
      ">>> Epoch 14 train loss: 2.6055\n",
      ">>> Epoch 14 test loss: 7.4310\n",
      ">>> Epoch 15 train loss: 2.6059\n",
      ">>> Epoch 15 test loss: 7.5831\n",
      ">>> Epoch 16 train loss: 2.6073\n",
      ">>> Epoch 16 test loss: 7.7156\n",
      ">>> Epoch 17 train loss: 2.6076\n",
      ">>> Epoch 17 test loss: 7.8375\n",
      ">>> Epoch 18 train loss: 2.6060\n",
      ">>> Epoch 18 test loss: 7.9563\n",
      ">>> Epoch 19 train loss: 2.6038\n",
      ">>> Epoch 19 test loss: 8.0741\n",
      ">>> Epoch 20 train loss: 2.6024\n",
      ">>> Epoch 20 test loss: 8.1875\n",
      ">>> Epoch 21 train loss: 2.6021\n",
      ">>> Epoch 21 test loss: 8.2905\n",
      ">>> Epoch 22 train loss: 2.6022\n",
      ">>> Epoch 22 test loss: 8.3796\n",
      ">>> Epoch 23 train loss: 2.6023\n",
      ">>> Epoch 23 test loss: 8.4548\n",
      ">>> Epoch 24 train loss: 2.6022\n",
      ">>> Epoch 24 test loss: 8.5197\n",
      ">>> Epoch 25 train loss: 2.6021\n",
      ">>> Epoch 25 test loss: 8.5793\n",
      ">>> Epoch 26 train loss: 2.6018\n",
      ">>> Epoch 26 test loss: 8.6385\n",
      ">>> Epoch 27 train loss: 2.6013\n",
      ">>> Epoch 27 test loss: 8.7001\n",
      ">>> Epoch 28 train loss: 2.6007\n",
      ">>> Epoch 28 test loss: 8.7632\n",
      ">>> Epoch 29 train loss: 2.6003\n",
      ">>> Epoch 29 test loss: 8.8243\n",
      ">>> Epoch 30 train loss: 2.6005\n",
      ">>> Epoch 30 test loss: 8.8789\n",
      ">>> Epoch 31 train loss: 2.6009\n",
      ">>> Epoch 31 test loss: 8.9236\n",
      ">>> Epoch 32 train loss: 2.6011\n",
      ">>> Epoch 32 test loss: 8.9580\n",
      ">>> Epoch 33 train loss: 2.6007\n",
      ">>> Epoch 33 test loss: 8.9853\n",
      ">>> Epoch 34 train loss: 2.6002\n",
      ">>> Epoch 34 test loss: 9.0098\n",
      ">>> Epoch 35 train loss: 2.6000\n",
      ">>> Epoch 35 test loss: 9.0356\n",
      ">>> Epoch 36 train loss: 2.6000\n",
      ">>> Epoch 36 test loss: 9.0638\n",
      ">>> Epoch 37 train loss: 2.6001\n",
      ">>> Epoch 37 test loss: 9.0933\n",
      ">>> Epoch 38 train loss: 2.6002\n",
      ">>> Epoch 38 test loss: 9.1215\n",
      ">>> Epoch 39 train loss: 2.6002\n",
      ">>> Epoch 39 test loss: 9.1461\n",
      ">>> Epoch 40 train loss: 2.6001\n",
      ">>> Epoch 40 test loss: 9.1661\n",
      ">>> Epoch 41 train loss: 2.5999\n",
      ">>> Epoch 41 test loss: 9.1826\n",
      ">>> Epoch 42 train loss: 2.5998\n",
      ">>> Epoch 42 test loss: 9.1980\n",
      ">>> Epoch 43 train loss: 2.5997\n",
      ">>> Epoch 43 test loss: 9.2144\n",
      ">>> Epoch 44 train loss: 2.5998\n",
      ">>> Epoch 44 test loss: 9.2323\n",
      ">>> Epoch 45 train loss: 2.5999\n",
      ">>> Epoch 45 test loss: 9.2503\n",
      ">>> Epoch 46 train loss: 2.5999\n",
      ">>> Epoch 46 test loss: 9.2660\n",
      ">>> Epoch 47 train loss: 2.5998\n",
      ">>> Epoch 47 test loss: 9.2778\n",
      ">>> Epoch 48 train loss: 2.5996\n",
      ">>> Epoch 48 test loss: 9.2856\n",
      ">>> Epoch 49 train loss: 2.5996\n",
      ">>> Epoch 49 test loss: 9.2913\n",
      ">>> Epoch 50 train loss: 2.5997\n",
      ">>> Epoch 50 test loss: 9.2972\n",
      ">>> Epoch 51 train loss: 2.5997\n",
      ">>> Epoch 51 test loss: 9.3054\n",
      ">>> Epoch 52 train loss: 2.5997\n",
      ">>> Epoch 52 test loss: 9.3164\n",
      ">>> Epoch 53 train loss: 2.5996\n",
      ">>> Epoch 53 test loss: 9.3289\n",
      ">>> Epoch 54 train loss: 2.5996\n",
      ">>> Epoch 54 test loss: 9.3408\n",
      ">>> Epoch 55 train loss: 2.5996\n",
      ">>> Epoch 55 test loss: 9.3502\n",
      ">>> Epoch 56 train loss: 2.5996\n",
      ">>> Epoch 56 test loss: 9.3567\n",
      ">>> Epoch 57 train loss: 2.5996\n",
      ">>> Epoch 57 test loss: 9.3612\n",
      ">>> Epoch 58 train loss: 2.5996\n",
      ">>> Epoch 58 test loss: 9.3652\n",
      ">>> Epoch 59 train loss: 2.5996\n",
      ">>> Epoch 59 test loss: 9.3699\n",
      ">>> Epoch 60 train loss: 2.5996\n",
      ">>> Epoch 60 test loss: 9.3757\n",
      ">>> Epoch 61 train loss: 2.5996\n",
      ">>> Epoch 61 test loss: 9.3821\n",
      ">>> Epoch 62 train loss: 2.5996\n",
      ">>> Epoch 62 test loss: 9.3882\n",
      ">>> Epoch 63 train loss: 2.5996\n",
      ">>> Epoch 63 test loss: 9.3936\n",
      ">>> Epoch 64 train loss: 2.5996\n",
      ">>> Epoch 64 test loss: 9.3983\n",
      ">>> Epoch 65 train loss: 2.5996\n",
      ">>> Epoch 65 test loss: 9.4029\n",
      ">>> Epoch 66 train loss: 2.5995\n",
      ">>> Epoch 66 test loss: 9.4078\n",
      ">>> Epoch 67 train loss: 2.5995\n",
      ">>> Epoch 67 test loss: 9.4130\n",
      ">>> Epoch 68 train loss: 2.5995\n",
      ">>> Epoch 68 test loss: 9.4183\n",
      ">>> Epoch 69 train loss: 2.5995\n",
      ">>> Epoch 69 test loss: 9.4230\n",
      ">>> Epoch 70 train loss: 2.5995\n",
      ">>> Epoch 70 test loss: 9.4270\n",
      ">>> Epoch 71 train loss: 2.5995\n",
      ">>> Epoch 71 test loss: 9.4305\n",
      ">>> Epoch 72 train loss: 2.5995\n",
      ">>> Epoch 72 test loss: 9.4339\n",
      ">>> Epoch 73 train loss: 2.5995\n",
      ">>> Epoch 73 test loss: 9.4376\n",
      ">>> Epoch 74 train loss: 2.5995\n",
      ">>> Epoch 74 test loss: 9.4417\n",
      ">>> Epoch 75 train loss: 2.5995\n",
      ">>> Epoch 75 test loss: 9.4459\n",
      ">>> Epoch 76 train loss: 2.5995\n",
      ">>> Epoch 76 test loss: 9.4501\n",
      ">>> Epoch 77 train loss: 2.5995\n",
      ">>> Epoch 77 test loss: 9.4541\n",
      ">>> Epoch 78 train loss: 2.5995\n",
      ">>> Epoch 78 test loss: 9.4578\n",
      ">>> Epoch 79 train loss: 2.5995\n",
      ">>> Epoch 79 test loss: 9.4615\n",
      ">>> Epoch 80 train loss: 2.5995\n",
      ">>> Epoch 80 test loss: 9.4653\n",
      ">>> Epoch 81 train loss: 2.5995\n",
      ">>> Epoch 81 test loss: 9.4692\n",
      ">>> Epoch 82 train loss: 2.5995\n",
      ">>> Epoch 82 test loss: 9.4730\n",
      ">>> Epoch 83 train loss: 2.5995\n",
      ">>> Epoch 83 test loss: 9.4766\n",
      ">>> Epoch 84 train loss: 2.5995\n",
      ">>> Epoch 84 test loss: 9.4799\n",
      ">>> Epoch 85 train loss: 2.5995\n",
      ">>> Epoch 85 test loss: 9.4831\n",
      ">>> Epoch 86 train loss: 2.5995\n",
      ">>> Epoch 86 test loss: 9.4863\n",
      ">>> Epoch 87 train loss: 2.5995\n",
      ">>> Epoch 87 test loss: 9.4898\n",
      ">>> Epoch 88 train loss: 2.5995\n",
      ">>> Epoch 88 test loss: 9.4935\n",
      ">>> Epoch 89 train loss: 2.5995\n",
      ">>> Epoch 89 test loss: 9.4975\n",
      ">>> Epoch 90 train loss: 2.5995\n",
      ">>> Epoch 90 test loss: 9.5014\n",
      ">>> Epoch 91 train loss: 2.5995\n",
      ">>> Epoch 91 test loss: 9.5051\n",
      ">>> Epoch 92 train loss: 2.5995\n",
      ">>> Epoch 92 test loss: 9.5085\n",
      ">>> Epoch 93 train loss: 2.5995\n",
      ">>> Epoch 93 test loss: 9.5117\n",
      ">>> Epoch 94 train loss: 2.5995\n",
      ">>> Epoch 94 test loss: 9.5148\n",
      ">>> Epoch 95 train loss: 2.5995\n",
      ">>> Epoch 95 test loss: 9.5181\n",
      ">>> Epoch 96 train loss: 2.5995\n",
      ">>> Epoch 96 test loss: 9.5216\n",
      ">>> Epoch 97 train loss: 2.5995\n",
      ">>> Epoch 97 test loss: 9.5253\n",
      ">>> Epoch 98 train loss: 2.5995\n",
      ">>> Epoch 98 test loss: 9.5289\n",
      ">>> Epoch 99 train loss: 2.5995\n",
      ">>> Epoch 99 test loss: 9.5324\n",
      ">>> Epoch 100 train loss: 2.5995\n",
      ">>> Epoch 100 test loss: 9.5357\n",
      ">>> Epoch 101 train loss: 2.5995\n",
      ">>> Epoch 101 test loss: 9.5391\n",
      ">>> Epoch 102 train loss: 2.5995\n",
      ">>> Epoch 102 test loss: 9.5426\n",
      ">>> Epoch 103 train loss: 2.5995\n",
      ">>> Epoch 103 test loss: 9.5462\n",
      ">>> Epoch 104 train loss: 2.5995\n",
      ">>> Epoch 104 test loss: 9.5498\n",
      ">>> Epoch 105 train loss: 2.5995\n",
      ">>> Epoch 105 test loss: 9.5531\n",
      ">>> Epoch 106 train loss: 2.5995\n",
      ">>> Epoch 106 test loss: 9.5563\n",
      ">>> Epoch 107 train loss: 2.5995\n",
      ">>> Epoch 107 test loss: 9.5595\n",
      ">>> Epoch 108 train loss: 2.5995\n",
      ">>> Epoch 108 test loss: 9.5628\n",
      ">>> Epoch 109 train loss: 2.5995\n",
      ">>> Epoch 109 test loss: 9.5664\n",
      ">>> Epoch 110 train loss: 2.5995\n",
      ">>> Epoch 110 test loss: 9.5701\n",
      ">>> Epoch 111 train loss: 2.5995\n",
      ">>> Epoch 111 test loss: 9.5738\n",
      ">>> Epoch 112 train loss: 2.5995\n",
      ">>> Epoch 112 test loss: 9.5772\n",
      ">>> Epoch 113 train loss: 2.5995\n",
      ">>> Epoch 113 test loss: 9.5805\n",
      ">>> Epoch 114 train loss: 2.5995\n",
      ">>> Epoch 114 test loss: 9.5837\n",
      ">>> Epoch 115 train loss: 2.5995\n",
      ">>> Epoch 115 test loss: 9.5870\n",
      ">>> Epoch 116 train loss: 2.5995\n",
      ">>> Epoch 116 test loss: 9.5904\n",
      ">>> Epoch 117 train loss: 2.5995\n",
      ">>> Epoch 117 test loss: 9.5939\n",
      ">>> Epoch 118 train loss: 2.5995\n",
      ">>> Epoch 118 test loss: 9.5974\n",
      ">>> Epoch 119 train loss: 2.5995\n",
      ">>> Epoch 119 test loss: 9.6008\n",
      ">>> Epoch 120 train loss: 2.5995\n",
      ">>> Epoch 120 test loss: 9.6042\n",
      ">>> Epoch 121 train loss: 2.5995\n",
      ">>> Epoch 121 test loss: 9.6076\n",
      ">>> Epoch 122 train loss: 2.5995\n",
      ">>> Epoch 122 test loss: 9.6110\n",
      ">>> Epoch 123 train loss: 2.5995\n",
      ">>> Epoch 123 test loss: 9.6144\n",
      ">>> Epoch 124 train loss: 2.5995\n",
      ">>> Epoch 124 test loss: 9.6178\n",
      ">>> Epoch 125 train loss: 2.5995\n",
      ">>> Epoch 125 test loss: 9.6211\n",
      ">>> Epoch 126 train loss: 2.5995\n",
      ">>> Epoch 126 test loss: 9.6245\n",
      ">>> Epoch 127 train loss: 2.5995\n",
      ">>> Epoch 127 test loss: 9.6278\n",
      ">>> Epoch 128 train loss: 2.5995\n",
      ">>> Epoch 128 test loss: 9.6313\n",
      ">>> Epoch 129 train loss: 2.5995\n",
      ">>> Epoch 129 test loss: 9.6347\n",
      ">>> Epoch 130 train loss: 2.5995\n",
      ">>> Epoch 130 test loss: 9.6380\n",
      ">>> Epoch 131 train loss: 2.5995\n",
      ">>> Epoch 131 test loss: 9.6414\n",
      ">>> Epoch 132 train loss: 2.5995\n",
      ">>> Epoch 132 test loss: 9.6447\n",
      ">>> Epoch 133 train loss: 2.5995\n",
      ">>> Epoch 133 test loss: 9.6481\n",
      ">>> Epoch 134 train loss: 2.5995\n",
      ">>> Epoch 134 test loss: 9.6515\n",
      ">>> Epoch 135 train loss: 2.5995\n",
      ">>> Epoch 135 test loss: 9.6549\n",
      ">>> Epoch 136 train loss: 2.5995\n",
      ">>> Epoch 136 test loss: 9.6582\n",
      ">>> Epoch 137 train loss: 2.5995\n",
      ">>> Epoch 137 test loss: 9.6615\n",
      ">>> Epoch 138 train loss: 2.5995\n",
      ">>> Epoch 138 test loss: 9.6648\n",
      ">>> Epoch 139 train loss: 2.5995\n",
      ">>> Epoch 139 test loss: 9.6682\n",
      ">>> Epoch 140 train loss: 2.5995\n",
      ">>> Epoch 140 test loss: 9.6716\n",
      ">>> Epoch 141 train loss: 2.5995\n",
      ">>> Epoch 141 test loss: 9.6750\n",
      ">>> Epoch 142 train loss: 2.5995\n",
      ">>> Epoch 142 test loss: 9.6783\n",
      ">>> Epoch 143 train loss: 2.5995\n",
      ">>> Epoch 143 test loss: 9.6815\n",
      ">>> Epoch 144 train loss: 2.5995\n",
      ">>> Epoch 144 test loss: 9.6848\n",
      ">>> Epoch 145 train loss: 2.5995\n",
      ">>> Epoch 145 test loss: 9.6882\n",
      ">>> Epoch 146 train loss: 2.5995\n",
      ">>> Epoch 146 test loss: 9.6915\n",
      ">>> Epoch 147 train loss: 2.5995\n",
      ">>> Epoch 147 test loss: 9.6948\n",
      ">>> Epoch 148 train loss: 2.5995\n",
      ">>> Epoch 148 test loss: 9.6981\n",
      ">>> Epoch 149 train loss: 2.5994\n",
      ">>> Epoch 149 test loss: 9.7014\n",
      ">>> Epoch 150 train loss: 2.5994\n",
      ">>> Epoch 150 test loss: 9.7047\n",
      ">>> Epoch 151 train loss: 2.5994\n",
      ">>> Epoch 151 test loss: 9.7080\n",
      ">>> Epoch 152 train loss: 2.5994\n",
      ">>> Epoch 152 test loss: 9.7113\n",
      ">>> Epoch 153 train loss: 2.5994\n",
      ">>> Epoch 153 test loss: 9.7146\n",
      ">>> Epoch 154 train loss: 2.5994\n",
      ">>> Epoch 154 test loss: 9.7179\n",
      ">>> Epoch 155 train loss: 2.5994\n",
      ">>> Epoch 155 test loss: 9.7211\n",
      ">>> Epoch 156 train loss: 2.5994\n",
      ">>> Epoch 156 test loss: 9.7244\n",
      ">>> Epoch 157 train loss: 2.5994\n",
      ">>> Epoch 157 test loss: 9.7276\n",
      ">>> Epoch 158 train loss: 2.5994\n",
      ">>> Epoch 158 test loss: 9.7309\n",
      ">>> Epoch 159 train loss: 2.5994\n",
      ">>> Epoch 159 test loss: 9.7342\n",
      ">>> Epoch 160 train loss: 2.5994\n",
      ">>> Epoch 160 test loss: 9.7374\n",
      ">>> Epoch 161 train loss: 2.5994\n",
      ">>> Epoch 161 test loss: 9.7407\n",
      ">>> Epoch 162 train loss: 2.5994\n",
      ">>> Epoch 162 test loss: 9.7439\n",
      ">>> Epoch 163 train loss: 2.5994\n",
      ">>> Epoch 163 test loss: 9.7471\n",
      ">>> Epoch 164 train loss: 2.5994\n",
      ">>> Epoch 164 test loss: 9.7503\n",
      ">>> Epoch 165 train loss: 2.5994\n",
      ">>> Epoch 165 test loss: 9.7536\n",
      ">>> Epoch 166 train loss: 2.5994\n",
      ">>> Epoch 166 test loss: 9.7568\n",
      ">>> Epoch 167 train loss: 2.5994\n",
      ">>> Epoch 167 test loss: 9.7600\n",
      ">>> Epoch 168 train loss: 2.5994\n",
      ">>> Epoch 168 test loss: 9.7632\n",
      ">>> Epoch 169 train loss: 2.5994\n",
      ">>> Epoch 169 test loss: 9.7664\n",
      ">>> Epoch 170 train loss: 2.5994\n",
      ">>> Epoch 170 test loss: 9.7696\n",
      ">>> Epoch 171 train loss: 2.5994\n",
      ">>> Epoch 171 test loss: 9.7728\n",
      ">>> Epoch 172 train loss: 2.5994\n",
      ">>> Epoch 172 test loss: 9.7760\n",
      ">>> Epoch 173 train loss: 2.5994\n",
      ">>> Epoch 173 test loss: 9.7791\n",
      ">>> Epoch 174 train loss: 2.5994\n",
      ">>> Epoch 174 test loss: 9.7823\n",
      ">>> Epoch 175 train loss: 2.5994\n",
      ">>> Epoch 175 test loss: 9.7855\n",
      ">>> Epoch 176 train loss: 2.5994\n",
      ">>> Epoch 176 test loss: 9.7886\n",
      ">>> Epoch 177 train loss: 2.5994\n",
      ">>> Epoch 177 test loss: 9.7918\n",
      ">>> Epoch 178 train loss: 2.5994\n",
      ">>> Epoch 178 test loss: 9.7949\n",
      ">>> Epoch 179 train loss: 2.5994\n",
      ">>> Epoch 179 test loss: 9.7981\n",
      ">>> Epoch 180 train loss: 2.5994\n",
      ">>> Epoch 180 test loss: 9.8012\n",
      ">>> Epoch 181 train loss: 2.5994\n",
      ">>> Epoch 181 test loss: 9.8044\n",
      ">>> Epoch 182 train loss: 2.5994\n",
      ">>> Epoch 182 test loss: 9.8075\n",
      ">>> Epoch 183 train loss: 2.5994\n",
      ">>> Epoch 183 test loss: 9.8106\n",
      ">>> Epoch 184 train loss: 2.5994\n",
      ">>> Epoch 184 test loss: 9.8137\n",
      ">>> Epoch 185 train loss: 2.5994\n",
      ">>> Epoch 185 test loss: 9.8168\n",
      ">>> Epoch 186 train loss: 2.5994\n",
      ">>> Epoch 186 test loss: 9.8200\n",
      ">>> Epoch 187 train loss: 2.5994\n",
      ">>> Epoch 187 test loss: 9.8231\n",
      ">>> Epoch 188 train loss: 2.5994\n",
      ">>> Epoch 188 test loss: 9.8261\n",
      ">>> Epoch 189 train loss: 2.5994\n",
      ">>> Epoch 189 test loss: 9.8292\n",
      ">>> Epoch 190 train loss: 2.5994\n",
      ">>> Epoch 190 test loss: 9.8323\n",
      ">>> Epoch 191 train loss: 2.5994\n",
      ">>> Epoch 191 test loss: 9.8354\n",
      ">>> Epoch 192 train loss: 2.5994\n",
      ">>> Epoch 192 test loss: 9.8384\n",
      ">>> Epoch 193 train loss: 2.5994\n",
      ">>> Epoch 193 test loss: 9.8415\n",
      ">>> Epoch 194 train loss: 2.5994\n",
      ">>> Epoch 194 test loss: 9.8446\n",
      ">>> Epoch 195 train loss: 2.5994\n",
      ">>> Epoch 195 test loss: 9.8476\n",
      ">>> Epoch 196 train loss: 2.5994\n",
      ">>> Epoch 196 test loss: 9.8507\n",
      ">>> Epoch 197 train loss: 2.5994\n",
      ">>> Epoch 197 test loss: 9.8537\n",
      ">>> Epoch 198 train loss: 2.5994\n",
      ">>> Epoch 198 test loss: 9.8567\n",
      ">>> Epoch 199 train loss: 2.5994\n",
      ">>> Epoch 199 test loss: 9.8598\n",
      ">>> Epoch 200 train loss: 2.5994\n",
      ">>> Epoch 200 test loss: 9.8628\n",
      ">>> Epoch 201 train loss: 2.5994\n",
      ">>> Epoch 201 test loss: 9.8658\n",
      ">>> Epoch 202 train loss: 2.5994\n",
      ">>> Epoch 202 test loss: 9.8688\n",
      ">>> Epoch 203 train loss: 2.5994\n",
      ">>> Epoch 203 test loss: 9.8718\n",
      ">>> Epoch 204 train loss: 2.5994\n",
      ">>> Epoch 204 test loss: 9.8748\n",
      ">>> Epoch 205 train loss: 2.5994\n",
      ">>> Epoch 205 test loss: 9.8778\n",
      ">>> Epoch 206 train loss: 2.5994\n",
      ">>> Epoch 206 test loss: 9.8808\n",
      ">>> Epoch 207 train loss: 2.5994\n",
      ">>> Epoch 207 test loss: 9.8838\n",
      ">>> Epoch 208 train loss: 2.5994\n",
      ">>> Epoch 208 test loss: 9.8868\n",
      ">>> Epoch 209 train loss: 2.5994\n",
      ">>> Epoch 209 test loss: 9.8897\n",
      ">>> Epoch 210 train loss: 2.5994\n",
      ">>> Epoch 210 test loss: 9.8927\n",
      ">>> Epoch 211 train loss: 2.5994\n",
      ">>> Epoch 211 test loss: 9.8956\n",
      ">>> Epoch 212 train loss: 2.5994\n",
      ">>> Epoch 212 test loss: 9.8986\n",
      ">>> Epoch 213 train loss: 2.5994\n",
      ">>> Epoch 213 test loss: 9.9015\n",
      ">>> Epoch 214 train loss: 2.5994\n",
      ">>> Epoch 214 test loss: 9.9045\n",
      ">>> Epoch 215 train loss: 2.5994\n",
      ">>> Epoch 215 test loss: 9.9074\n",
      ">>> Epoch 216 train loss: 2.5994\n",
      ">>> Epoch 216 test loss: 9.9103\n",
      ">>> Epoch 217 train loss: 2.5994\n",
      ">>> Epoch 217 test loss: 9.9133\n",
      ">>> Epoch 218 train loss: 2.5994\n",
      ">>> Epoch 218 test loss: 9.9162\n",
      ">>> Epoch 219 train loss: 2.5994\n",
      ">>> Epoch 219 test loss: 9.9191\n",
      ">>> Epoch 220 train loss: 2.5994\n",
      ">>> Epoch 220 test loss: 9.9220\n",
      ">>> Epoch 221 train loss: 2.5994\n",
      ">>> Epoch 221 test loss: 9.9249\n",
      ">>> Epoch 222 train loss: 2.5994\n",
      ">>> Epoch 222 test loss: 9.9278\n",
      ">>> Epoch 223 train loss: 2.5994\n",
      ">>> Epoch 223 test loss: 9.9307\n",
      ">>> Epoch 224 train loss: 2.5994\n",
      ">>> Epoch 224 test loss: 9.9335\n",
      ">>> Epoch 225 train loss: 2.5994\n",
      ">>> Epoch 225 test loss: 9.9364\n",
      ">>> Epoch 226 train loss: 2.5994\n",
      ">>> Epoch 226 test loss: 9.9393\n",
      ">>> Epoch 227 train loss: 2.5994\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# emb_dim from 64 -> 32\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# lr 1e-3 -> 1e-2\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# train loss stabilizes around 2.6, up from 2.3\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[43mvt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/vision-transformer/vision_transformer_v1.py:223\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, (inputs, labels) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(test_dataloader):\n\u001b[1;32m    222\u001b[0m     inputs, labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m--> 223\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[1;32m    224\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m    225\u001b[0m     epoch_losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/vit/lib/python3.10/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/vit/lib/python3.10/site-packages/torch/utils/data/dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/vit/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:50\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[0;32m---> 50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/vit/lib/python3.10/site-packages/torch/utils/data/dataset.py:418\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitems__\u001b[39m(\u001b[38;5;28mself\u001b[39m, indices: List[\u001b[38;5;28mint\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[_T_co]:\n\u001b[1;32m    415\u001b[0m     \u001b[38;5;66;03m# add batched sampling support when parent dataset supports it.\u001b[39;00m\n\u001b[1;32m    416\u001b[0m     \u001b[38;5;66;03m# see torch.utils.data._utils.fetch._MapDatasetFetcher\u001b[39;00m\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)):\n\u001b[0;32m--> 418\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    419\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    420\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx]] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/vit/lib/python3.10/site-packages/torch/utils/data/dataset.py:420\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx]] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/vit/lib/python3.10/site-packages/torch/utils/data/dataset.py:420\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/vit/lib/python3.10/site-packages/torchvision/datasets/oxford_iiit_pet.py:97\u001b[0m, in \u001b[0;36mOxfordIIITPet.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Any, Any]:\n\u001b[0;32m---> 97\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_images\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRGB\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m     target: Any \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m target_type \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_target_types:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/vit/lib/python3.10/site-packages/PIL/Image.py:984\u001b[0m, in \u001b[0;36mImage.convert\u001b[0;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[1;32m    981\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBGR;15\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBGR;16\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBGR;24\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    982\u001b[0m     deprecate(mode, \u001b[38;5;241m12\u001b[39m)\n\u001b[0;32m--> 984\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    986\u001b[0m has_transparency \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransparency\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\n\u001b[1;32m    987\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mP\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    988\u001b[0m     \u001b[38;5;66;03m# determine default mode\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/vit/lib/python3.10/site-packages/PIL/ImageFile.py:300\u001b[0m, in \u001b[0;36mImageFile.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(msg)\n\u001b[1;32m    299\u001b[0m b \u001b[38;5;241m=\u001b[39m b \u001b[38;5;241m+\u001b[39m s\n\u001b[0;32m--> 300\u001b[0m n, err_code \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# emb_dim from 64 -> 32\n",
    "# lr 1e-3 -> 1e-2\n",
    "# train loss stabilizes around 2.6, up from 2.3\n",
    "vt.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea9ee380-b574-4762-a836-b9d304d574be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Epoch 0 train loss: 3.7857\n",
      ">>> Epoch 0 test loss: 3.3686\n",
      ">>> Epoch 1 train loss: 3.7163\n",
      ">>> Epoch 1 test loss: 3.3706\n",
      ">>> Epoch 2 train loss: 3.6500\n",
      ">>> Epoch 2 test loss: 3.3755\n",
      ">>> Epoch 3 train loss: 3.5867\n",
      ">>> Epoch 3 test loss: 3.3831\n",
      ">>> Epoch 4 train loss: 3.5265\n",
      ">>> Epoch 4 test loss: 3.3931\n",
      ">>> Epoch 5 train loss: 3.4695\n",
      ">>> Epoch 5 test loss: 3.4054\n",
      ">>> Epoch 6 train loss: 3.4156\n",
      ">>> Epoch 6 test loss: 3.4198\n",
      ">>> Epoch 7 train loss: 3.3649\n",
      ">>> Epoch 7 test loss: 3.4359\n",
      ">>> Epoch 8 train loss: 3.3171\n",
      ">>> Epoch 8 test loss: 3.4535\n",
      ">>> Epoch 9 train loss: 3.2722\n",
      ">>> Epoch 9 test loss: 3.4723\n",
      ">>> Epoch 10 train loss: 3.2300\n",
      ">>> Epoch 10 test loss: 3.4922\n",
      ">>> Epoch 11 train loss: 3.1903\n",
      ">>> Epoch 11 test loss: 3.5129\n",
      ">>> Epoch 12 train loss: 3.1531\n",
      ">>> Epoch 12 test loss: 3.5344\n",
      ">>> Epoch 13 train loss: 3.1182\n",
      ">>> Epoch 13 test loss: 3.5565\n",
      ">>> Epoch 14 train loss: 3.0855\n",
      ">>> Epoch 14 test loss: 3.5790\n",
      ">>> Epoch 15 train loss: 3.0549\n",
      ">>> Epoch 15 test loss: 3.6018\n",
      ">>> Epoch 16 train loss: 3.0262\n",
      ">>> Epoch 16 test loss: 3.6246\n",
      ">>> Epoch 17 train loss: 2.9995\n",
      ">>> Epoch 17 test loss: 3.6472\n",
      ">>> Epoch 18 train loss: 2.9745\n",
      ">>> Epoch 18 test loss: 3.6695\n",
      ">>> Epoch 19 train loss: 2.9511\n",
      ">>> Epoch 19 test loss: 3.6913\n",
      ">>> Epoch 20 train loss: 2.9291\n",
      ">>> Epoch 20 test loss: 3.7126\n",
      ">>> Epoch 21 train loss: 2.9085\n",
      ">>> Epoch 21 test loss: 3.7333\n",
      ">>> Epoch 22 train loss: 2.8890\n",
      ">>> Epoch 22 test loss: 3.7533\n",
      ">>> Epoch 23 train loss: 2.8707\n",
      ">>> Epoch 23 test loss: 3.7727\n",
      ">>> Epoch 24 train loss: 2.8534\n",
      ">>> Epoch 24 test loss: 3.7916\n",
      ">>> Epoch 25 train loss: 2.8370\n",
      ">>> Epoch 25 test loss: 3.8100\n",
      ">>> Epoch 26 train loss: 2.8216\n",
      ">>> Epoch 26 test loss: 3.8281\n",
      ">>> Epoch 27 train loss: 2.8070\n",
      ">>> Epoch 27 test loss: 3.8461\n",
      ">>> Epoch 28 train loss: 2.7934\n",
      ">>> Epoch 28 test loss: 3.8639\n",
      ">>> Epoch 29 train loss: 2.7805\n",
      ">>> Epoch 29 test loss: 3.8818\n",
      ">>> Epoch 30 train loss: 2.7683\n",
      ">>> Epoch 30 test loss: 3.8998\n",
      ">>> Epoch 31 train loss: 2.7569\n",
      ">>> Epoch 31 test loss: 3.9179\n",
      ">>> Epoch 32 train loss: 2.7461\n",
      ">>> Epoch 32 test loss: 3.9361\n",
      ">>> Epoch 33 train loss: 2.7359\n",
      ">>> Epoch 33 test loss: 3.9543\n",
      ">>> Epoch 34 train loss: 2.7263\n",
      ">>> Epoch 34 test loss: 3.9724\n",
      ">>> Epoch 35 train loss: 2.7171\n",
      ">>> Epoch 35 test loss: 3.9902\n",
      ">>> Epoch 36 train loss: 2.7085\n",
      ">>> Epoch 36 test loss: 4.0077\n",
      ">>> Epoch 37 train loss: 2.7002\n",
      ">>> Epoch 37 test loss: 4.0246\n",
      ">>> Epoch 38 train loss: 2.6923\n",
      ">>> Epoch 38 test loss: 4.0407\n",
      ">>> Epoch 39 train loss: 2.6849\n",
      ">>> Epoch 39 test loss: 4.0560\n",
      ">>> Epoch 40 train loss: 2.6777\n",
      ">>> Epoch 40 test loss: 4.0705\n",
      ">>> Epoch 41 train loss: 2.6710\n",
      ">>> Epoch 41 test loss: 4.0841\n",
      ">>> Epoch 42 train loss: 2.6646\n",
      ">>> Epoch 42 test loss: 4.0969\n",
      ">>> Epoch 43 train loss: 2.6584\n",
      ">>> Epoch 43 test loss: 4.1090\n",
      ">>> Epoch 44 train loss: 2.6526\n",
      ">>> Epoch 44 test loss: 4.1205\n",
      ">>> Epoch 45 train loss: 2.6471\n",
      ">>> Epoch 45 test loss: 4.1316\n",
      ">>> Epoch 46 train loss: 2.6418\n",
      ">>> Epoch 46 test loss: 4.1425\n",
      ">>> Epoch 47 train loss: 2.6368\n",
      ">>> Epoch 47 test loss: 4.1532\n",
      ">>> Epoch 48 train loss: 2.6320\n",
      ">>> Epoch 48 test loss: 4.1640\n",
      ">>> Epoch 49 train loss: 2.6275\n",
      ">>> Epoch 49 test loss: 4.1750\n",
      ">>> Epoch 50 train loss: 2.6231\n",
      ">>> Epoch 50 test loss: 4.1861\n",
      ">>> Epoch 51 train loss: 2.6190\n",
      ">>> Epoch 51 test loss: 4.1974\n",
      ">>> Epoch 52 train loss: 2.6151\n",
      ">>> Epoch 52 test loss: 4.2088\n",
      ">>> Epoch 53 train loss: 2.6114\n",
      ">>> Epoch 53 test loss: 4.2204\n",
      ">>> Epoch 54 train loss: 2.6079\n",
      ">>> Epoch 54 test loss: 4.2320\n",
      ">>> Epoch 55 train loss: 2.6045\n",
      ">>> Epoch 55 test loss: 4.2437\n",
      ">>> Epoch 56 train loss: 2.6013\n",
      ">>> Epoch 56 test loss: 4.2553\n",
      ">>> Epoch 57 train loss: 2.5983\n",
      ">>> Epoch 57 test loss: 4.2670\n",
      ">>> Epoch 58 train loss: 2.5954\n",
      ">>> Epoch 58 test loss: 4.2785\n",
      ">>> Epoch 59 train loss: 2.5926\n",
      ">>> Epoch 59 test loss: 4.2901\n",
      ">>> Epoch 60 train loss: 2.5899\n",
      ">>> Epoch 60 test loss: 4.3016\n",
      ">>> Epoch 61 train loss: 2.5874\n",
      ">>> Epoch 61 test loss: 4.3132\n",
      ">>> Epoch 62 train loss: 2.5850\n",
      ">>> Epoch 62 test loss: 4.3248\n",
      ">>> Epoch 63 train loss: 2.5827\n",
      ">>> Epoch 63 test loss: 4.3365\n",
      ">>> Epoch 64 train loss: 2.5806\n",
      ">>> Epoch 64 test loss: 4.3482\n",
      ">>> Epoch 65 train loss: 2.5785\n",
      ">>> Epoch 65 test loss: 4.3599\n",
      ">>> Epoch 66 train loss: 2.5765\n",
      ">>> Epoch 66 test loss: 4.3717\n",
      ">>> Epoch 67 train loss: 2.5746\n",
      ">>> Epoch 67 test loss: 4.3834\n",
      ">>> Epoch 68 train loss: 2.5728\n",
      ">>> Epoch 68 test loss: 4.3950\n",
      ">>> Epoch 69 train loss: 2.5710\n",
      ">>> Epoch 69 test loss: 4.4065\n",
      ">>> Epoch 70 train loss: 2.5694\n",
      ">>> Epoch 70 test loss: 4.4180\n",
      ">>> Epoch 71 train loss: 2.5678\n",
      ">>> Epoch 71 test loss: 4.4293\n",
      ">>> Epoch 72 train loss: 2.5663\n",
      ">>> Epoch 72 test loss: 4.4406\n",
      ">>> Epoch 73 train loss: 2.5648\n",
      ">>> Epoch 73 test loss: 4.4519\n",
      ">>> Epoch 74 train loss: 2.5634\n",
      ">>> Epoch 74 test loss: 4.4631\n",
      ">>> Epoch 75 train loss: 2.5621\n",
      ">>> Epoch 75 test loss: 4.4744\n",
      ">>> Epoch 76 train loss: 2.5608\n",
      ">>> Epoch 76 test loss: 4.4857\n",
      ">>> Epoch 77 train loss: 2.5596\n",
      ">>> Epoch 77 test loss: 4.4971\n",
      ">>> Epoch 78 train loss: 2.5584\n",
      ">>> Epoch 78 test loss: 4.5085\n",
      ">>> Epoch 79 train loss: 2.5573\n",
      ">>> Epoch 79 test loss: 4.5199\n",
      ">>> Epoch 80 train loss: 2.5562\n",
      ">>> Epoch 80 test loss: 4.5312\n",
      ">>> Epoch 81 train loss: 2.5551\n",
      ">>> Epoch 81 test loss: 4.5423\n",
      ">>> Epoch 82 train loss: 2.5541\n",
      ">>> Epoch 82 test loss: 4.5533\n",
      ">>> Epoch 83 train loss: 2.5531\n",
      ">>> Epoch 83 test loss: 4.5640\n",
      ">>> Epoch 84 train loss: 2.5522\n",
      ">>> Epoch 84 test loss: 4.5744\n",
      ">>> Epoch 85 train loss: 2.5513\n",
      ">>> Epoch 85 test loss: 4.5845\n",
      ">>> Epoch 86 train loss: 2.5504\n",
      ">>> Epoch 86 test loss: 4.5944\n",
      ">>> Epoch 87 train loss: 2.5496\n",
      ">>> Epoch 87 test loss: 4.6041\n",
      ">>> Epoch 88 train loss: 2.5488\n",
      ">>> Epoch 88 test loss: 4.6136\n",
      ">>> Epoch 89 train loss: 2.5480\n",
      ">>> Epoch 89 test loss: 4.6229\n",
      ">>> Epoch 90 train loss: 2.5473\n",
      ">>> Epoch 90 test loss: 4.6322\n",
      ">>> Epoch 91 train loss: 2.5465\n",
      ">>> Epoch 91 test loss: 4.6414\n",
      ">>> Epoch 92 train loss: 2.5458\n",
      ">>> Epoch 92 test loss: 4.6505\n",
      ">>> Epoch 93 train loss: 2.5451\n",
      ">>> Epoch 93 test loss: 4.6596\n",
      ">>> Epoch 94 train loss: 2.5445\n",
      ">>> Epoch 94 test loss: 4.6687\n",
      ">>> Epoch 95 train loss: 2.5439\n",
      ">>> Epoch 95 test loss: 4.6777\n",
      ">>> Epoch 96 train loss: 2.5432\n",
      ">>> Epoch 96 test loss: 4.6867\n",
      ">>> Epoch 97 train loss: 2.5426\n",
      ">>> Epoch 97 test loss: 4.6956\n",
      ">>> Epoch 98 train loss: 2.5421\n",
      ">>> Epoch 98 test loss: 4.7044\n",
      ">>> Epoch 99 train loss: 2.5415\n",
      ">>> Epoch 99 test loss: 4.7132\n",
      ">>> Epoch 100 train loss: 2.5410\n",
      ">>> Epoch 100 test loss: 4.7219\n",
      ">>> Epoch 101 train loss: 2.5404\n",
      ">>> Epoch 101 test loss: 4.7305\n",
      ">>> Epoch 102 train loss: 2.5399\n",
      ">>> Epoch 102 test loss: 4.7391\n",
      ">>> Epoch 103 train loss: 2.5394\n",
      ">>> Epoch 103 test loss: 4.7477\n",
      ">>> Epoch 104 train loss: 2.5389\n",
      ">>> Epoch 104 test loss: 4.7561\n",
      ">>> Epoch 105 train loss: 2.5385\n",
      ">>> Epoch 105 test loss: 4.7645\n",
      ">>> Epoch 106 train loss: 2.5380\n",
      ">>> Epoch 106 test loss: 4.7728\n",
      ">>> Epoch 107 train loss: 2.5376\n",
      ">>> Epoch 107 test loss: 4.7809\n",
      ">>> Epoch 108 train loss: 2.5372\n",
      ">>> Epoch 108 test loss: 4.7890\n",
      ">>> Epoch 109 train loss: 2.5367\n",
      ">>> Epoch 109 test loss: 4.7970\n",
      ">>> Epoch 110 train loss: 2.5363\n",
      ">>> Epoch 110 test loss: 4.8049\n",
      ">>> Epoch 111 train loss: 2.5359\n",
      ">>> Epoch 111 test loss: 4.8127\n",
      ">>> Epoch 112 train loss: 2.5355\n",
      ">>> Epoch 112 test loss: 4.8205\n",
      ">>> Epoch 113 train loss: 2.5352\n",
      ">>> Epoch 113 test loss: 4.8282\n",
      ">>> Epoch 114 train loss: 2.5348\n",
      ">>> Epoch 114 test loss: 4.8359\n",
      ">>> Epoch 115 train loss: 2.5344\n",
      ">>> Epoch 115 test loss: 4.8436\n",
      ">>> Epoch 116 train loss: 2.5341\n",
      ">>> Epoch 116 test loss: 4.8512\n",
      ">>> Epoch 117 train loss: 2.5338\n",
      ">>> Epoch 117 test loss: 4.8587\n",
      ">>> Epoch 118 train loss: 2.5334\n",
      ">>> Epoch 118 test loss: 4.8662\n",
      ">>> Epoch 119 train loss: 2.5331\n",
      ">>> Epoch 119 test loss: 4.8736\n",
      ">>> Epoch 120 train loss: 2.5328\n",
      ">>> Epoch 120 test loss: 4.8810\n",
      ">>> Epoch 121 train loss: 2.5325\n",
      ">>> Epoch 121 test loss: 4.8882\n",
      ">>> Epoch 122 train loss: 2.5322\n",
      ">>> Epoch 122 test loss: 4.8954\n",
      ">>> Epoch 123 train loss: 2.5319\n",
      ">>> Epoch 123 test loss: 4.9026\n",
      ">>> Epoch 124 train loss: 2.5316\n",
      ">>> Epoch 124 test loss: 4.9097\n",
      ">>> Epoch 125 train loss: 2.5313\n",
      ">>> Epoch 125 test loss: 4.9167\n",
      ">>> Epoch 126 train loss: 2.5311\n",
      ">>> Epoch 126 test loss: 4.9237\n",
      ">>> Epoch 127 train loss: 2.5308\n",
      ">>> Epoch 127 test loss: 4.9306\n",
      ">>> Epoch 128 train loss: 2.5305\n",
      ">>> Epoch 128 test loss: 4.9375\n",
      ">>> Epoch 129 train loss: 2.5303\n",
      ">>> Epoch 129 test loss: 4.9444\n",
      ">>> Epoch 130 train loss: 2.5300\n",
      ">>> Epoch 130 test loss: 4.9512\n",
      ">>> Epoch 131 train loss: 2.5298\n",
      ">>> Epoch 131 test loss: 4.9579\n",
      ">>> Epoch 132 train loss: 2.5296\n",
      ">>> Epoch 132 test loss: 4.9646\n",
      ">>> Epoch 133 train loss: 2.5293\n",
      ">>> Epoch 133 test loss: 4.9712\n",
      ">>> Epoch 134 train loss: 2.5291\n",
      ">>> Epoch 134 test loss: 4.9778\n",
      ">>> Epoch 135 train loss: 2.5289\n",
      ">>> Epoch 135 test loss: 4.9843\n",
      ">>> Epoch 136 train loss: 2.5287\n",
      ">>> Epoch 136 test loss: 4.9908\n",
      ">>> Epoch 137 train loss: 2.5284\n",
      ">>> Epoch 137 test loss: 4.9973\n",
      ">>> Epoch 138 train loss: 2.5282\n",
      ">>> Epoch 138 test loss: 5.0037\n",
      ">>> Epoch 139 train loss: 2.5280\n",
      ">>> Epoch 139 test loss: 5.0100\n",
      ">>> Epoch 140 train loss: 2.5278\n",
      ">>> Epoch 140 test loss: 5.0163\n",
      ">>> Epoch 141 train loss: 2.5276\n",
      ">>> Epoch 141 test loss: 5.0226\n",
      ">>> Epoch 142 train loss: 2.5274\n",
      ">>> Epoch 142 test loss: 5.0288\n",
      ">>> Epoch 143 train loss: 2.5272\n",
      ">>> Epoch 143 test loss: 5.0350\n",
      ">>> Epoch 144 train loss: 2.5271\n",
      ">>> Epoch 144 test loss: 5.0411\n",
      ">>> Epoch 145 train loss: 2.5269\n",
      ">>> Epoch 145 test loss: 5.0472\n",
      ">>> Epoch 146 train loss: 2.5267\n",
      ">>> Epoch 146 test loss: 5.0533\n",
      ">>> Epoch 147 train loss: 2.5265\n",
      ">>> Epoch 147 test loss: 5.0593\n",
      ">>> Epoch 148 train loss: 2.5264\n",
      ">>> Epoch 148 test loss: 5.0653\n",
      ">>> Epoch 149 train loss: 2.5262\n",
      ">>> Epoch 149 test loss: 5.0713\n",
      ">>> Epoch 150 train loss: 2.5260\n",
      ">>> Epoch 150 test loss: 5.0772\n",
      ">>> Epoch 151 train loss: 2.5259\n",
      ">>> Epoch 151 test loss: 5.0831\n",
      ">>> Epoch 152 train loss: 2.5257\n",
      ">>> Epoch 152 test loss: 5.0889\n",
      ">>> Epoch 153 train loss: 2.5255\n",
      ">>> Epoch 153 test loss: 5.0947\n",
      ">>> Epoch 154 train loss: 2.5254\n",
      ">>> Epoch 154 test loss: 5.1005\n",
      ">>> Epoch 155 train loss: 2.5252\n",
      ">>> Epoch 155 test loss: 5.1062\n",
      ">>> Epoch 156 train loss: 2.5251\n",
      ">>> Epoch 156 test loss: 5.1119\n",
      ">>> Epoch 157 train loss: 2.5250\n",
      ">>> Epoch 157 test loss: 5.1176\n",
      ">>> Epoch 158 train loss: 2.5248\n",
      ">>> Epoch 158 test loss: 5.1232\n",
      ">>> Epoch 159 train loss: 2.5247\n",
      ">>> Epoch 159 test loss: 5.1288\n",
      ">>> Epoch 160 train loss: 2.5245\n",
      ">>> Epoch 160 test loss: 5.1344\n",
      ">>> Epoch 161 train loss: 2.5244\n",
      ">>> Epoch 161 test loss: 5.1399\n",
      ">>> Epoch 162 train loss: 2.5243\n",
      ">>> Epoch 162 test loss: 5.1454\n",
      ">>> Epoch 163 train loss: 2.5241\n",
      ">>> Epoch 163 test loss: 5.1508\n",
      ">>> Epoch 164 train loss: 2.5240\n",
      ">>> Epoch 164 test loss: 5.1563\n",
      ">>> Epoch 165 train loss: 2.5239\n",
      ">>> Epoch 165 test loss: 5.1617\n",
      ">>> Epoch 166 train loss: 2.5238\n",
      ">>> Epoch 166 test loss: 5.1670\n",
      ">>> Epoch 167 train loss: 2.5236\n",
      ">>> Epoch 167 test loss: 5.1724\n",
      ">>> Epoch 168 train loss: 2.5235\n",
      ">>> Epoch 168 test loss: 5.1777\n",
      ">>> Epoch 169 train loss: 2.5234\n",
      ">>> Epoch 169 test loss: 5.1830\n",
      ">>> Epoch 170 train loss: 2.5233\n",
      ">>> Epoch 170 test loss: 5.1882\n",
      ">>> Epoch 171 train loss: 2.5232\n",
      ">>> Epoch 171 test loss: 5.1934\n",
      ">>> Epoch 172 train loss: 2.5231\n",
      ">>> Epoch 172 test loss: 5.1986\n",
      ">>> Epoch 173 train loss: 2.5229\n",
      ">>> Epoch 173 test loss: 5.2038\n",
      ">>> Epoch 174 train loss: 2.5228\n",
      ">>> Epoch 174 test loss: 5.2089\n",
      ">>> Epoch 175 train loss: 2.5227\n",
      ">>> Epoch 175 test loss: 5.2140\n",
      ">>> Epoch 176 train loss: 2.5226\n",
      ">>> Epoch 176 test loss: 5.2191\n",
      ">>> Epoch 177 train loss: 2.5225\n",
      ">>> Epoch 177 test loss: 5.2242\n",
      ">>> Epoch 178 train loss: 2.5224\n",
      ">>> Epoch 178 test loss: 5.2292\n",
      ">>> Epoch 179 train loss: 2.5223\n",
      ">>> Epoch 179 test loss: 5.2342\n",
      ">>> Epoch 180 train loss: 2.5222\n",
      ">>> Epoch 180 test loss: 5.2392\n",
      ">>> Epoch 181 train loss: 2.5221\n",
      ">>> Epoch 181 test loss: 5.2441\n",
      ">>> Epoch 182 train loss: 2.5220\n",
      ">>> Epoch 182 test loss: 5.2490\n",
      ">>> Epoch 183 train loss: 2.5219\n",
      ">>> Epoch 183 test loss: 5.2539\n",
      ">>> Epoch 184 train loss: 2.5219\n",
      ">>> Epoch 184 test loss: 5.2588\n",
      ">>> Epoch 185 train loss: 2.5218\n",
      ">>> Epoch 185 test loss: 5.2637\n",
      ">>> Epoch 186 train loss: 2.5217\n",
      ">>> Epoch 186 test loss: 5.2685\n",
      ">>> Epoch 187 train loss: 2.5216\n",
      ">>> Epoch 187 test loss: 5.2733\n",
      ">>> Epoch 188 train loss: 2.5215\n",
      ">>> Epoch 188 test loss: 5.2781\n",
      ">>> Epoch 189 train loss: 2.5214\n",
      ">>> Epoch 189 test loss: 5.2828\n",
      ">>> Epoch 190 train loss: 2.5213\n",
      ">>> Epoch 190 test loss: 5.2875\n",
      ">>> Epoch 191 train loss: 2.5212\n",
      ">>> Epoch 191 test loss: 5.2923\n",
      ">>> Epoch 192 train loss: 2.5212\n",
      ">>> Epoch 192 test loss: 5.2969\n",
      ">>> Epoch 193 train loss: 2.5211\n",
      ">>> Epoch 193 test loss: 5.3016\n",
      ">>> Epoch 194 train loss: 2.5210\n",
      ">>> Epoch 194 test loss: 5.3062\n",
      ">>> Epoch 195 train loss: 2.5209\n",
      ">>> Epoch 195 test loss: 5.3109\n",
      ">>> Epoch 196 train loss: 2.5208\n",
      ">>> Epoch 196 test loss: 5.3154\n",
      ">>> Epoch 197 train loss: 2.5208\n",
      ">>> Epoch 197 test loss: 5.3200\n",
      ">>> Epoch 198 train loss: 2.5207\n",
      ">>> Epoch 198 test loss: 5.3246\n",
      ">>> Epoch 199 train loss: 2.5206\n",
      ">>> Epoch 199 test loss: 5.3291\n",
      ">>> Epoch 200 train loss: 2.5205\n",
      ">>> Epoch 200 test loss: 5.3336\n",
      ">>> Epoch 201 train loss: 2.5205\n",
      ">>> Epoch 201 test loss: 5.3381\n",
      ">>> Epoch 202 train loss: 2.5204\n",
      ">>> Epoch 202 test loss: 5.3425\n",
      ">>> Epoch 203 train loss: 2.5203\n",
      ">>> Epoch 203 test loss: 5.3470\n",
      ">>> Epoch 204 train loss: 2.5203\n",
      ">>> Epoch 204 test loss: 5.3514\n",
      ">>> Epoch 205 train loss: 2.5202\n",
      ">>> Epoch 205 test loss: 5.3558\n",
      ">>> Epoch 206 train loss: 2.5201\n",
      ">>> Epoch 206 test loss: 5.3602\n",
      ">>> Epoch 207 train loss: 2.5201\n",
      ">>> Epoch 207 test loss: 5.3646\n",
      ">>> Epoch 208 train loss: 2.5200\n",
      ">>> Epoch 208 test loss: 5.3689\n",
      ">>> Epoch 209 train loss: 2.5199\n",
      ">>> Epoch 209 test loss: 5.3732\n",
      ">>> Epoch 210 train loss: 2.5199\n",
      ">>> Epoch 210 test loss: 5.3775\n",
      ">>> Epoch 211 train loss: 2.5198\n",
      ">>> Epoch 211 test loss: 5.3818\n",
      ">>> Epoch 212 train loss: 2.5197\n",
      ">>> Epoch 212 test loss: 5.3861\n",
      ">>> Epoch 213 train loss: 2.5197\n",
      ">>> Epoch 213 test loss: 5.3903\n",
      ">>> Epoch 214 train loss: 2.5196\n",
      ">>> Epoch 214 test loss: 5.3945\n",
      ">>> Epoch 215 train loss: 2.5196\n",
      ">>> Epoch 215 test loss: 5.3988\n",
      ">>> Epoch 216 train loss: 2.5195\n",
      ">>> Epoch 216 test loss: 5.4029\n",
      ">>> Epoch 217 train loss: 2.5195\n",
      ">>> Epoch 217 test loss: 5.4071\n",
      ">>> Epoch 218 train loss: 2.5194\n",
      ">>> Epoch 218 test loss: 5.4113\n",
      ">>> Epoch 219 train loss: 2.5193\n",
      ">>> Epoch 219 test loss: 5.4154\n",
      ">>> Epoch 220 train loss: 2.5193\n",
      ">>> Epoch 220 test loss: 5.4195\n",
      ">>> Epoch 221 train loss: 2.5192\n",
      ">>> Epoch 221 test loss: 5.4236\n",
      ">>> Epoch 222 train loss: 2.5192\n",
      ">>> Epoch 222 test loss: 5.4277\n",
      ">>> Epoch 223 train loss: 2.5191\n",
      ">>> Epoch 223 test loss: 5.4318\n",
      ">>> Epoch 224 train loss: 2.5191\n",
      ">>> Epoch 224 test loss: 5.4358\n",
      ">>> Epoch 225 train loss: 2.5190\n",
      ">>> Epoch 225 test loss: 5.4399\n",
      ">>> Epoch 226 train loss: 2.5190\n",
      ">>> Epoch 226 test loss: 5.4439\n",
      ">>> Epoch 227 train loss: 2.5189\n",
      ">>> Epoch 227 test loss: 5.4479\n",
      ">>> Epoch 228 train loss: 2.5189\n",
      ">>> Epoch 228 test loss: 5.4519\n",
      ">>> Epoch 229 train loss: 2.5188\n",
      ">>> Epoch 229 test loss: 5.4558\n",
      ">>> Epoch 230 train loss: 2.5188\n",
      ">>> Epoch 230 test loss: 5.4598\n",
      ">>> Epoch 231 train loss: 2.5187\n",
      ">>> Epoch 231 test loss: 5.4637\n",
      ">>> Epoch 232 train loss: 2.5187\n",
      ">>> Epoch 232 test loss: 5.4676\n",
      ">>> Epoch 233 train loss: 2.5186\n",
      ">>> Epoch 233 test loss: 5.4715\n",
      ">>> Epoch 234 train loss: 2.5186\n",
      ">>> Epoch 234 test loss: 5.4754\n",
      ">>> Epoch 235 train loss: 2.5185\n",
      ">>> Epoch 235 test loss: 5.4793\n",
      ">>> Epoch 236 train loss: 2.5185\n",
      ">>> Epoch 236 test loss: 5.4831\n",
      ">>> Epoch 237 train loss: 2.5184\n",
      ">>> Epoch 237 test loss: 5.4870\n",
      ">>> Epoch 238 train loss: 2.5184\n",
      ">>> Epoch 238 test loss: 5.4908\n",
      ">>> Epoch 239 train loss: 2.5183\n",
      ">>> Epoch 239 test loss: 5.4946\n",
      ">>> Epoch 240 train loss: 2.5183\n",
      ">>> Epoch 240 test loss: 5.4984\n",
      ">>> Epoch 241 train loss: 2.5183\n",
      ">>> Epoch 241 test loss: 5.5022\n",
      ">>> Epoch 242 train loss: 2.5182\n",
      ">>> Epoch 242 test loss: 5.5060\n",
      ">>> Epoch 243 train loss: 2.5182\n",
      ">>> Epoch 243 test loss: 5.5097\n",
      ">>> Epoch 244 train loss: 2.5181\n",
      ">>> Epoch 244 test loss: 5.5135\n",
      ">>> Epoch 245 train loss: 2.5181\n",
      ">>> Epoch 245 test loss: 5.5172\n",
      ">>> Epoch 246 train loss: 2.5181\n",
      ">>> Epoch 246 test loss: 5.5209\n",
      ">>> Epoch 247 train loss: 2.5180\n",
      ">>> Epoch 247 test loss: 5.5246\n",
      ">>> Epoch 248 train loss: 2.5180\n",
      ">>> Epoch 248 test loss: 5.5283\n",
      ">>> Epoch 249 train loss: 2.5179\n",
      ">>> Epoch 249 test loss: 5.5319\n",
      ">>> Epoch 250 train loss: 2.5179\n",
      ">>> Epoch 250 test loss: 5.5356\n",
      ">>> Epoch 251 train loss: 2.5179\n",
      ">>> Epoch 251 test loss: 5.5392\n",
      ">>> Epoch 252 train loss: 2.5178\n",
      ">>> Epoch 252 test loss: 5.5428\n",
      ">>> Epoch 253 train loss: 2.5178\n",
      ">>> Epoch 253 test loss: 5.5464\n",
      ">>> Epoch 254 train loss: 2.5177\n",
      ">>> Epoch 254 test loss: 5.5500\n",
      ">>> Epoch 255 train loss: 2.5177\n",
      ">>> Epoch 255 test loss: 5.5536\n",
      ">>> Epoch 256 train loss: 2.5177\n",
      ">>> Epoch 256 test loss: 5.5572\n",
      ">>> Epoch 257 train loss: 2.5176\n",
      ">>> Epoch 257 test loss: 5.5608\n",
      ">>> Epoch 258 train loss: 2.5176\n",
      ">>> Epoch 258 test loss: 5.5643\n",
      ">>> Epoch 259 train loss: 2.5176\n",
      ">>> Epoch 259 test loss: 5.5678\n",
      ">>> Epoch 260 train loss: 2.5175\n",
      ">>> Epoch 260 test loss: 5.5713\n",
      ">>> Epoch 261 train loss: 2.5175\n",
      ">>> Epoch 261 test loss: 5.5749\n",
      ">>> Epoch 262 train loss: 2.5175\n",
      ">>> Epoch 262 test loss: 5.5783\n",
      ">>> Epoch 263 train loss: 2.5174\n",
      ">>> Epoch 263 test loss: 5.5818\n",
      ">>> Epoch 264 train loss: 2.5174\n",
      ">>> Epoch 264 test loss: 5.5853\n",
      ">>> Epoch 265 train loss: 2.5174\n",
      ">>> Epoch 265 test loss: 5.5888\n",
      ">>> Epoch 266 train loss: 2.5173\n",
      ">>> Epoch 266 test loss: 5.5922\n",
      ">>> Epoch 267 train loss: 2.5173\n",
      ">>> Epoch 267 test loss: 5.5956\n",
      ">>> Epoch 268 train loss: 2.5173\n",
      ">>> Epoch 268 test loss: 5.5990\n",
      ">>> Epoch 269 train loss: 2.5172\n",
      ">>> Epoch 269 test loss: 5.6025\n",
      ">>> Epoch 270 train loss: 2.5172\n",
      ">>> Epoch 270 test loss: 5.6059\n",
      ">>> Epoch 271 train loss: 2.5172\n",
      ">>> Epoch 271 test loss: 5.6092\n",
      ">>> Epoch 272 train loss: 2.5171\n",
      ">>> Epoch 272 test loss: 5.6126\n",
      ">>> Epoch 273 train loss: 2.5171\n",
      ">>> Epoch 273 test loss: 5.6160\n",
      ">>> Epoch 274 train loss: 2.5171\n",
      ">>> Epoch 274 test loss: 5.6193\n",
      ">>> Epoch 275 train loss: 2.5170\n",
      ">>> Epoch 275 test loss: 5.6227\n",
      ">>> Epoch 276 train loss: 2.5170\n",
      ">>> Epoch 276 test loss: 5.6260\n",
      ">>> Epoch 277 train loss: 2.5170\n",
      ">>> Epoch 277 test loss: 5.6293\n",
      ">>> Epoch 278 train loss: 2.5170\n",
      ">>> Epoch 278 test loss: 5.6326\n",
      ">>> Epoch 279 train loss: 2.5169\n",
      ">>> Epoch 279 test loss: 5.6359\n",
      ">>> Epoch 280 train loss: 2.5169\n",
      ">>> Epoch 280 test loss: 5.6392\n",
      ">>> Epoch 281 train loss: 2.5169\n",
      ">>> Epoch 281 test loss: 5.6424\n",
      ">>> Epoch 282 train loss: 2.5168\n",
      ">>> Epoch 282 test loss: 5.6457\n",
      ">>> Epoch 283 train loss: 2.5168\n",
      ">>> Epoch 283 test loss: 5.6489\n",
      ">>> Epoch 284 train loss: 2.5168\n",
      ">>> Epoch 284 test loss: 5.6522\n",
      ">>> Epoch 285 train loss: 2.5168\n",
      ">>> Epoch 285 test loss: 5.6554\n",
      ">>> Epoch 286 train loss: 2.5167\n",
      ">>> Epoch 286 test loss: 5.6586\n",
      ">>> Epoch 287 train loss: 2.5167\n",
      ">>> Epoch 287 test loss: 5.6618\n",
      ">>> Epoch 288 train loss: 2.5167\n",
      ">>> Epoch 288 test loss: 5.6650\n",
      ">>> Epoch 289 train loss: 2.5167\n",
      ">>> Epoch 289 test loss: 5.6682\n",
      ">>> Epoch 290 train loss: 2.5166\n",
      ">>> Epoch 290 test loss: 5.6714\n",
      ">>> Epoch 291 train loss: 2.5166\n",
      ">>> Epoch 291 test loss: 5.6745\n",
      ">>> Epoch 292 train loss: 2.5166\n",
      ">>> Epoch 292 test loss: 5.6777\n",
      ">>> Epoch 293 train loss: 2.5166\n",
      ">>> Epoch 293 test loss: 5.6808\n",
      ">>> Epoch 294 train loss: 2.5165\n",
      ">>> Epoch 294 test loss: 5.6840\n",
      ">>> Epoch 295 train loss: 2.5165\n",
      ">>> Epoch 295 test loss: 5.6871\n",
      ">>> Epoch 296 train loss: 2.5165\n",
      ">>> Epoch 296 test loss: 5.6902\n",
      ">>> Epoch 297 train loss: 2.5165\n",
      ">>> Epoch 297 test loss: 5.6933\n",
      ">>> Epoch 298 train loss: 2.5164\n",
      ">>> Epoch 298 test loss: 5.6964\n",
      ">>> Epoch 299 train loss: 2.5164\n",
      ">>> Epoch 299 test loss: 5.6995\n",
      ">>> Epoch 300 train loss: 2.5164\n",
      ">>> Epoch 300 test loss: 5.7026\n",
      ">>> Epoch 301 train loss: 2.5164\n",
      ">>> Epoch 301 test loss: 5.7056\n",
      ">>> Epoch 302 train loss: 2.5163\n",
      ">>> Epoch 302 test loss: 5.7087\n",
      ">>> Epoch 303 train loss: 2.5163\n",
      ">>> Epoch 303 test loss: 5.7117\n",
      ">>> Epoch 304 train loss: 2.5163\n",
      ">>> Epoch 304 test loss: 5.7148\n",
      ">>> Epoch 305 train loss: 2.5163\n",
      ">>> Epoch 305 test loss: 5.7178\n",
      ">>> Epoch 306 train loss: 2.5163\n",
      ">>> Epoch 306 test loss: 5.7208\n",
      ">>> Epoch 307 train loss: 2.5162\n",
      ">>> Epoch 307 test loss: 5.7238\n",
      ">>> Epoch 308 train loss: 2.5162\n",
      ">>> Epoch 308 test loss: 5.7268\n",
      ">>> Epoch 309 train loss: 2.5162\n",
      ">>> Epoch 309 test loss: 5.7298\n",
      ">>> Epoch 310 train loss: 2.5162\n",
      ">>> Epoch 310 test loss: 5.7328\n",
      ">>> Epoch 311 train loss: 2.5162\n",
      ">>> Epoch 311 test loss: 5.7358\n",
      ">>> Epoch 312 train loss: 2.5161\n",
      ">>> Epoch 312 test loss: 5.7387\n",
      ">>> Epoch 313 train loss: 2.5161\n",
      ">>> Epoch 313 test loss: 5.7417\n",
      ">>> Epoch 314 train loss: 2.5161\n",
      ">>> Epoch 314 test loss: 5.7446\n",
      ">>> Epoch 315 train loss: 2.5161\n",
      ">>> Epoch 315 test loss: 5.7476\n",
      ">>> Epoch 316 train loss: 2.5161\n",
      ">>> Epoch 316 test loss: 5.7505\n",
      ">>> Epoch 317 train loss: 2.5160\n",
      ">>> Epoch 317 test loss: 5.7534\n",
      ">>> Epoch 318 train loss: 2.5160\n",
      ">>> Epoch 318 test loss: 5.7563\n",
      ">>> Epoch 319 train loss: 2.5160\n",
      ">>> Epoch 319 test loss: 5.7592\n",
      ">>> Epoch 320 train loss: 2.5160\n",
      ">>> Epoch 320 test loss: 5.7621\n",
      ">>> Epoch 321 train loss: 2.5160\n",
      ">>> Epoch 321 test loss: 5.7650\n",
      ">>> Epoch 322 train loss: 2.5159\n",
      ">>> Epoch 322 test loss: 5.7679\n",
      ">>> Epoch 323 train loss: 2.5159\n",
      ">>> Epoch 323 test loss: 5.7707\n",
      ">>> Epoch 324 train loss: 2.5159\n",
      ">>> Epoch 324 test loss: 5.7736\n",
      ">>> Epoch 325 train loss: 2.5159\n",
      ">>> Epoch 325 test loss: 5.7764\n",
      ">>> Epoch 326 train loss: 2.5159\n",
      ">>> Epoch 326 test loss: 5.7793\n",
      ">>> Epoch 327 train loss: 2.5158\n",
      ">>> Epoch 327 test loss: 5.7821\n",
      ">>> Epoch 328 train loss: 2.5158\n",
      ">>> Epoch 328 test loss: 5.7849\n",
      ">>> Epoch 329 train loss: 2.5158\n",
      ">>> Epoch 329 test loss: 5.7877\n",
      ">>> Epoch 330 train loss: 2.5158\n",
      ">>> Epoch 330 test loss: 5.7906\n",
      ">>> Epoch 331 train loss: 2.5158\n",
      ">>> Epoch 331 test loss: 5.7934\n",
      ">>> Epoch 332 train loss: 2.5158\n",
      ">>> Epoch 332 test loss: 5.7962\n",
      ">>> Epoch 333 train loss: 2.5157\n",
      ">>> Epoch 333 test loss: 5.7989\n",
      ">>> Epoch 334 train loss: 2.5157\n",
      ">>> Epoch 334 test loss: 5.8017\n",
      ">>> Epoch 335 train loss: 2.5157\n",
      ">>> Epoch 335 test loss: 5.8045\n",
      ">>> Epoch 336 train loss: 2.5157\n",
      ">>> Epoch 336 test loss: 5.8072\n",
      ">>> Epoch 337 train loss: 2.5157\n",
      ">>> Epoch 337 test loss: 5.8100\n",
      ">>> Epoch 338 train loss: 2.5157\n",
      ">>> Epoch 338 test loss: 5.8127\n",
      ">>> Epoch 339 train loss: 2.5156\n",
      ">>> Epoch 339 test loss: 5.8155\n",
      ">>> Epoch 340 train loss: 2.5156\n",
      ">>> Epoch 340 test loss: 5.8182\n",
      ">>> Epoch 341 train loss: 2.5156\n",
      ">>> Epoch 341 test loss: 5.8209\n",
      ">>> Epoch 342 train loss: 2.5156\n",
      ">>> Epoch 342 test loss: 5.8237\n",
      ">>> Epoch 343 train loss: 2.5156\n",
      ">>> Epoch 343 test loss: 5.8264\n",
      ">>> Epoch 344 train loss: 2.5156\n",
      ">>> Epoch 344 test loss: 5.8291\n",
      ">>> Epoch 345 train loss: 2.5155\n",
      ">>> Epoch 345 test loss: 5.8318\n",
      ">>> Epoch 346 train loss: 2.5155\n",
      ">>> Epoch 346 test loss: 5.8344\n",
      ">>> Epoch 347 train loss: 2.5155\n",
      ">>> Epoch 347 test loss: 5.8371\n",
      ">>> Epoch 348 train loss: 2.5155\n",
      ">>> Epoch 348 test loss: 5.8398\n",
      ">>> Epoch 349 train loss: 2.5155\n",
      ">>> Epoch 349 test loss: 5.8425\n",
      ">>> Epoch 350 train loss: 2.5155\n",
      ">>> Epoch 350 test loss: 5.8451\n",
      ">>> Epoch 351 train loss: 2.5154\n",
      ">>> Epoch 351 test loss: 5.8478\n",
      ">>> Epoch 352 train loss: 2.5154\n",
      ">>> Epoch 352 test loss: 5.8504\n",
      ">>> Epoch 353 train loss: 2.5154\n",
      ">>> Epoch 353 test loss: 5.8530\n",
      ">>> Epoch 354 train loss: 2.5154\n",
      ">>> Epoch 354 test loss: 5.8557\n",
      ">>> Epoch 355 train loss: 2.5154\n",
      ">>> Epoch 355 test loss: 5.8583\n",
      ">>> Epoch 356 train loss: 2.5154\n",
      ">>> Epoch 356 test loss: 5.8609\n",
      ">>> Epoch 357 train loss: 2.5154\n",
      ">>> Epoch 357 test loss: 5.8635\n",
      ">>> Epoch 358 train loss: 2.5153\n",
      ">>> Epoch 358 test loss: 5.8661\n",
      ">>> Epoch 359 train loss: 2.5153\n",
      ">>> Epoch 359 test loss: 5.8687\n",
      ">>> Epoch 360 train loss: 2.5153\n",
      ">>> Epoch 360 test loss: 5.8713\n",
      ">>> Epoch 361 train loss: 2.5153\n",
      ">>> Epoch 361 test loss: 5.8739\n",
      ">>> Epoch 362 train loss: 2.5153\n",
      ">>> Epoch 362 test loss: 5.8765\n",
      ">>> Epoch 363 train loss: 2.5153\n",
      ">>> Epoch 363 test loss: 5.8790\n",
      ">>> Epoch 364 train loss: 2.5153\n",
      ">>> Epoch 364 test loss: 5.8816\n",
      ">>> Epoch 365 train loss: 2.5153\n",
      ">>> Epoch 365 test loss: 5.8841\n",
      ">>> Epoch 366 train loss: 2.5152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# emb_dim from 64 -> 32\n",
    "# lr 1e-2 -> 1e-3\n",
    "# train loss stabilizes around 2.6, up from 2.3\n",
    "vt.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "353227c7-a667-49fe-8135-843ac53451fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Epoch 0 train loss: 3.6958\n",
      ">>> Epoch 0 test loss: 3.5385\n",
      "\n",
      ">>> Epoch 20 train loss: 3.4084\n",
      ">>> Epoch 20 test loss: 3.5384\n",
      "\n",
      ">>> Epoch 40 train loss: 3.4018\n",
      ">>> Epoch 40 test loss: 3.4523\n",
      "\n",
      ">>> Epoch 60 train loss: 3.4017\n",
      ">>> Epoch 60 test loss: 3.5359\n",
      "\n",
      ">>> Epoch 80 train loss: 3.4000\n",
      ">>> Epoch 80 test loss: 3.4880\n",
      "\n",
      ">>> Epoch 100 train loss: 3.3980\n",
      ">>> Epoch 100 test loss: 3.5478\n",
      "\n",
      ">>> Epoch 120 train loss: 3.4010\n",
      ">>> Epoch 120 test loss: 3.6549\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# emb_dim from 64 -> 32\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# lr 1e-2 -> 0.001\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# train loss stabilizes around 2.6, up from 2.3\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[43mvt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/vision-transformer/vision_transformer_v1.py:209\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    207\u001b[0m epoch_losses \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    208\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m--> 209\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, (inputs, labels) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_dataloader):\n\u001b[1;32m    210\u001b[0m     inputs, labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    211\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/vit/lib/python3.10/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/vit/lib/python3.10/site-packages/torch/utils/data/dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/vit/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:50\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[0;32m---> 50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/vit/lib/python3.10/site-packages/torch/utils/data/dataset.py:418\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitems__\u001b[39m(\u001b[38;5;28mself\u001b[39m, indices: List[\u001b[38;5;28mint\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[_T_co]:\n\u001b[1;32m    415\u001b[0m     \u001b[38;5;66;03m# add batched sampling support when parent dataset supports it.\u001b[39;00m\n\u001b[1;32m    416\u001b[0m     \u001b[38;5;66;03m# see torch.utils.data._utils.fetch._MapDatasetFetcher\u001b[39;00m\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)):\n\u001b[0;32m--> 418\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    419\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    420\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx]] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/vit/lib/python3.10/site-packages/torch/utils/data/dataset.py:420\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx]] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/vit/lib/python3.10/site-packages/torch/utils/data/dataset.py:420\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/vit/lib/python3.10/site-packages/torchvision/datasets/oxford_iiit_pet.py:116\u001b[0m, in \u001b[0;36mOxfordIIITPet.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    113\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(target)\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m--> 116\u001b[0m     image, target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransforms\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m image, target\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/vit/lib/python3.10/site-packages/torchvision/datasets/vision.py:95\u001b[0m, in \u001b[0;36mStandardTransform.__call__\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Any, target: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Any, Any]:\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 95\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     97\u001b[0m         target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/vit/lib/python3.10/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/vit/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/vit/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/vit/lib/python3.10/site-packages/torchvision/transforms/transforms.py:354\u001b[0m, in \u001b[0;36mResize.forward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m    347\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    349\u001b[0m \u001b[38;5;124;03m        img (PIL Image or Tensor): Image to be scaled.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;124;03m        PIL Image or Tensor: Rescaled image.\u001b[39;00m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 354\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mantialias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/vit/lib/python3.10/site-packages/torchvision/transforms/functional.py:477\u001b[0m, in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation, max_size, antialias)\u001b[0m\n\u001b[1;32m    475\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnti-alias option is always applied for PIL Image input. Argument antialias is ignored.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    476\u001b[0m     pil_interpolation \u001b[38;5;241m=\u001b[39m pil_modes_mapping[interpolation]\n\u001b[0;32m--> 477\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_pil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpil_interpolation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F_t\u001b[38;5;241m.\u001b[39mresize(img, size\u001b[38;5;241m=\u001b[39moutput_size, interpolation\u001b[38;5;241m=\u001b[39minterpolation\u001b[38;5;241m.\u001b[39mvalue, antialias\u001b[38;5;241m=\u001b[39mantialias)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/vit/lib/python3.10/site-packages/torchvision/transforms/_functional_pil.py:250\u001b[0m, in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(size, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(size) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m):\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot inappropriate size arg: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 250\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/vit/lib/python3.10/site-packages/PIL/Image.py:2356\u001b[0m, in \u001b[0;36mImage.resize\u001b[0;34m(self, size, resample, box, reducing_gap)\u001b[0m\n\u001b[1;32m   2344\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2345\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduce(factor, box\u001b[38;5;241m=\u001b[39mreduce_box)\n\u001b[1;32m   2346\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduce)\n\u001b[1;32m   2347\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m Image\u001b[38;5;241m.\u001b[39mreduce(\u001b[38;5;28mself\u001b[39m, factor, box\u001b[38;5;241m=\u001b[39mreduce_box)\n\u001b[1;32m   2348\u001b[0m         )\n\u001b[1;32m   2349\u001b[0m         box \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2350\u001b[0m             (box[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_x,\n\u001b[1;32m   2351\u001b[0m             (box[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_y,\n\u001b[1;32m   2352\u001b[0m             (box[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_x,\n\u001b[1;32m   2353\u001b[0m             (box[\u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_y,\n\u001b[1;32m   2354\u001b[0m         )\n\u001b[0;32m-> 2356\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbox\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <bound method AutoreloadMagics.post_execute_hook of <IPython.extensions.autoreload.AutoreloadMagics object at 0x10649ccd0>> (for post_execute), with arguments args (),kwargs {}:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# emb_dim from 64 -> 32\n",
    "# lr 1e-2 -> 0.001\n",
    "# train loss stabilizes around 2.6, up from 2.3\n",
    "vt.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be8f47c-2a8e-4b8f-a0f7-70e2ad9180fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "train size 5000\n",
      "0\n",
      ">>> Epoch 0 train loss: 2.3918\n",
      ">>> Epoch 0 test loss: 2.3242\n",
      "\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      ">>> Epoch 5 train loss: 2.3132\n"
     ]
    }
   ],
   "source": [
    "# CIFAR\n",
    "# emb_dim from 64 -> 32\n",
    "# lr 1e-2 -> 0.001\n",
    "# train loss stabilizes around 2.6, up from 2.3\n",
    "vt.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed0f61c-bc8c-4a87-841c-b6753c3093d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
